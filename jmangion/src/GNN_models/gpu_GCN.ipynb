{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the Colab (for GPU utilization) implementation of GCN, GAT, and GIN\n",
        "============================\n",
        "Usage Instructions:\n",
        "  * Ensure Runtime is set to a GPU\n",
        "  * As colab does not accept command line arguments, the following cell will allow user to set the most common preferences. For less commonly changed preferences, the user must modify them directly inside of the cell labled 'arguments.py File'\n",
        "  * __GAN__ is more complex than GCN and some hyperparameters must be changed based on the dataset. However, the following are in common for all datasets:\n",
        "    -  __two layers__ are used\n",
        "    - Layer 1 has 8 attention heads with 8 features each, and its nonlinearity is the __ELU__ function\n",
        "    - Dropout is applied to both layers with coefficient p = 0.6\n",
        "  * GAN hyperparameters & architectural details that vary based on the dataset are as follows:\n",
        "    - __Cora & CiteSeer__:  Layer 2 has 1 attention head with C (# of classes) features. L2 regularization is applied with a weighting coefficient of 0.0005\n",
        "    - __PubMed__: Layer 2 has 8 attention heads with  C (# of classes) features. L2 regularization is applied with a weighting coefficient of 0.001\n"
      ],
      "metadata": {
        "id": "FydpzENMFoOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preferences cell\n",
        "user_specified_dataset = \"Cora\" # choose between Cora, PubMed, and CiteSeer\n",
        "user_specified_model = \"GCN\" # choose between GCN, GAN\n"
      ],
      "metadata": {
        "id": "44UavQzZGrzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-wY1r0GBultF"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "GyJdorM9usM_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### arguments.py File ###\n",
        "### Modified for colab since colab does not seem to like argparse ###\n",
        "\n",
        "# Defined custom class to hold arguments\n",
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.root_dir = \"/content\"\n",
        "    self.data_dir = \"/content/data\"\n",
        "    self.epochs = 300\n",
        "    self.runs = 5\n",
        "    self.droput = 0.4\n",
        "    self.lr = 0.001\n",
        "    self.wd = 0.001\n",
        "    self.num_layers = 2\n",
        "    self.num_hidden = 256\n",
        "    self.num_features = 0 # placeholder\n",
        "    self.num_classes = 0 # placeholder\n",
        "\n",
        "def add_data_features(args, data):\n",
        "  args.num_features = data.x.shape[1]\n",
        "  args.num_classes = data.y.shape[0]\n",
        "  return args"
      ],
      "metadata": {
        "id": "PoFVwJW_vEML"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### data.py File ###\n",
        "\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "args = Args()\n",
        "dataset = Planetoid(root=args.root_dir, name=user_specified_dataset)"
      ],
      "metadata": {
        "id": "hYtRO0D_u4MI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### model.py File ###\n",
        "\n",
        "def make_layers(self):\n",
        "    layers = []\n",
        "    # initialize layers in a loop that uses conditionals to determine the input and output dimensions of the feature vectors\n",
        "    for i in range(self.num_layers):\n",
        "        if i == 0:  # first layer\n",
        "            # dimensions in = input data size\n",
        "            # dimensions out = hidden layer size\n",
        "            if\n",
        "            layer = GCNConv(self.num_features, self.num_hidden)\n",
        "\n",
        "        elif i < self.num_layers - 1: # hidden layer(s)\n",
        "            # dimensions in = hidden layer size\n",
        "            # dimensions out = hidden layer size\n",
        "            layer = GCNConv(self.num_hidden, self.num_hidden)\n",
        "\n",
        "        else:  # output layer\n",
        "            # dimensions in = hidden layer size\n",
        "            # dimensions out = output size\n",
        "            layer = GCNConv(self.num_hidden, self.num_classes)\n",
        "\n",
        "        layers.append(layer)\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "class GCN_model(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super().__init__()\n",
        "        self.num_features = args.num_features\n",
        "        self.num_layers = args.num_layers\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.num_classes = args.num_classes\n",
        "        self.wd = args.wd\n",
        "        self.lr = args.lr\n",
        "        self.layers = make_layers(self)\n",
        "\n",
        "    def forward(self, x, edge_idx):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # apply the convolutional layer\n",
        "            x = layer(x, edge_idx)\n",
        "\n",
        "            # Since I did not apply the activation function in the Layers array, I apply it using conditionals (to decide relu or softmax) here\n",
        "            if i != len(self.layers) - 1:\n",
        "                x = F.relu(x)\n",
        "            else:\n",
        "                x = F.log_softmax(x, dim = 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class GAN_model(nn.Module):\n",
        "  def__init__(self, args):\n",
        "    super().__init()\n",
        "    self.num_features = args.num_features\n",
        "      self.num_layers = args.num_layers\n",
        "      self.num_hidden = args.num_hidden\n",
        "      self.num_classes = args.num_classes\n",
        "      self.wd = args.wd\n",
        "      self.lr = args.lr\n",
        "      self.layers = make_layers(self)\n"
      ],
      "metadata": {
        "id": "KyIrKRFfvT6C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py File ###\n",
        "\n",
        "def train(model, X, Y, data):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = model.lr, weight_decay = model.wd)\n",
        "    optimizer.zero_grad()\n",
        "    activations = model(X, data.edge_index)\n",
        "\n",
        "    # only calculate loss on train labels!!\n",
        "    loss = F.nll_loss(activations[data.train_mask], Y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def get_masked_acc(activations, y_true, mask):\n",
        "    length = activations[mask].shape[0]\n",
        "    correct = 0\n",
        "    for yhat, y in zip(activations[mask], y_true[mask]):\n",
        "        if torch.argmax(yhat) == y:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / length\n",
        "\n",
        "def get_accuracy(activations, y_true, data):\n",
        "    train_acc = get_masked_acc(activations, y_true, data.train_mask)\n",
        "    test_acc = get_masked_acc(activations, y_true, data.test_mask)\n",
        "    val_acc = get_masked_acc(activations, y_true, data.val_mask)\n",
        "    return train_acc, test_acc, val_acc\n",
        "\n",
        "def main():\n",
        "    # use gpu if possible (works most of the time here on colab)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # get data\n",
        "    data = dataset[0].to(device)\n",
        "    x = data.x\n",
        "    y = data.y\n",
        "\n",
        "    # get preferences\n",
        "    args = Args()\n",
        "    args = add_data_features(args, data)\n",
        "\n",
        "\n",
        "    for run in range(args.runs):\n",
        "        # initialize model\n",
        "        model = GCN_model(args).to(device)\n",
        "        print(\"\\n------------ new model ------------\\n\")\n",
        "        for epoch in range(args.epochs):\n",
        "          # log loss every 50 steps\n",
        "            if epoch % 50 == 0 or epoch == args.epochs - 1:\n",
        "                model.eval()\n",
        "                activations = model(x, data.edge_index)\n",
        "                loss = F.nll_loss(activations, y)\n",
        "                train_acc, test_acc, val_acc = get_accuracy(activations, y, data)\n",
        "                print(f\" Epoch: {epoch} | Total Loss: {loss} | Train Accuracy: {train_acc} | Test Accuracy: {test_acc} | Val Accuracy: {val_acc}\")\n",
        "\n",
        "            # backprop & update\n",
        "            train(model, x, y, data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "7c8qFCbQuntx",
        "outputId": "d36732f1-794a-4e6c-a30a-2ddcdd2762c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 9.885893821716309 | Train Accuracy: 0.18333333333333332 | Test Accuracy: 0.165 | Val Accuracy: 0.136\n",
            " Epoch: 50 | Total Loss: 8.278214454650879 | Train Accuracy: 0.6333333333333333 | Test Accuracy: 0.461 | Val Accuracy: 0.476\n",
            " Epoch: 100 | Total Loss: 3.8787031173706055 | Train Accuracy: 0.6 | Test Accuracy: 0.516 | Val Accuracy: 0.538\n",
            " Epoch: 150 | Total Loss: 1.4334903955459595 | Train Accuracy: 0.65 | Test Accuracy: 0.486 | Val Accuracy: 0.516\n",
            " Epoch: 200 | Total Loss: 1.1925625801086426 | Train Accuracy: 0.43333333333333335 | Test Accuracy: 0.312 | Val Accuracy: 0.33\n",
            " Epoch: 250 | Total Loss: 0.9940095543861389 | Train Accuracy: 0.9333333333333333 | Test Accuracy: 0.734 | Val Accuracy: 0.746\n",
            " Epoch: 299 | Total Loss: 0.7798689603805542 | Train Accuracy: 0.95 | Test Accuracy: 0.769 | Val Accuracy: 0.776\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 9.88580322265625 | Train Accuracy: 0.2833333333333333 | Test Accuracy: 0.233 | Val Accuracy: 0.236\n",
            " Epoch: 50 | Total Loss: 8.290199279785156 | Train Accuracy: 0.3333333333333333 | Test Accuracy: 0.18 | Val Accuracy: 0.196\n",
            " Epoch: 100 | Total Loss: 3.8969554901123047 | Train Accuracy: 0.3333333333333333 | Test Accuracy: 0.18 | Val Accuracy: 0.196\n",
            " Epoch: 150 | Total Loss: 1.4415624141693115 | Train Accuracy: 0.3333333333333333 | Test Accuracy: 0.18 | Val Accuracy: 0.196\n",
            " Epoch: 200 | Total Loss: 1.1937639713287354 | Train Accuracy: 0.4166666666666667 | Test Accuracy: 0.261 | Val Accuracy: 0.284\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-6952577de923>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-6952577de923>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# backprop & update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# log loss every 50 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6952577de923>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, Y, data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# only calculate loss on train labels!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1eeee0c7ce6e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# apply the convolutional layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Since I did not apply the activation function in the Layers array, I apply it using conditionals (to decide relu or softmax) here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    242\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0madd_self_loops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         edge_index, edge_weight = add_remaining_self_loops(\n\u001b[0m\u001b[1;32m    100\u001b[0m             edge_index, edge_weight, fill_value, num_nodes)\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/loop.py\u001b[0m in \u001b[0;36madd_remaining_self_loops\u001b[0;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEdgeIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_undirected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_undirected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m     r\"\"\"\n\u001b[1;32m   1122\u001b[0m     \u001b[0mFunction\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcompilation\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
