{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GAT\n",
        "- currently able to run on homogeneous cora datasets\n",
        "- need to run this on heterogeneous AMiner, DBLP"
      ],
      "metadata": {
        "id": "Lnw-GJ26nSQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# choose which dataset you want to run on. Default: AMiner\n",
        "user_dataset = \"Planetoid\"\n",
        "# For heterogeneous: choose which label you want to compute loss against. Default: Author\n",
        "label = \"author\""
      ],
      "metadata": {
        "id": "m6tbqNztn4o8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DZjNoiQynK88"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GATConv\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "x4EKwbkSnQw6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### arguments.py File ###\n",
        "### Modified for colab since colab does not seem to like argparse ###\n",
        "\n",
        "# Defined custom class to hold arguments\n",
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.root_dir = \"/content\"\n",
        "    self.data_dir = \"/content/data\"\n",
        "    self.epochs = 300\n",
        "    self.runs = 5\n",
        "    self.droput = 0.4\n",
        "    self.lr = 0.001\n",
        "    self.wd = 0.001\n",
        "    self.num_layers = 2\n",
        "    self.num_hidden = 8\n",
        "    self.num_features = 0 # placeholder\n",
        "    self.num_classes = 0 # placeholder\n",
        "\n",
        "def add_data_features(args, data):\n",
        "  args.num_features = data.x.shape[1]\n",
        "  args.num_classes = data.y.shape[0]\n",
        "  return args"
      ],
      "metadata": {
        "id": "Pqbi6huLnf1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### data.py File ###\n",
        "\n",
        "args = Args()\n",
        "\n",
        "if user_dataset == \"AMiner\":\n",
        "  from torch_geometric.datasets import AMiner\n",
        "  dataset = AMiner(root=args.root_dir)\n",
        "elif user_dataset == \"DBLP\":\n",
        "  from torch_geometric.datasets import DBLP\n",
        "  dataset = DBLP(root=args.root_dir)\n",
        "elif user_dataset == \"Planetoid\":\n",
        "  from torch_geometric.datasets import Planetoid\n",
        "  dataset = Planetoid(root=args.root_dir, name=\"Cora\")\n",
        "else:\n",
        "  print(f\"ERROR! Your dataset {user_dataset} was not found.\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmLRBOAqnjIP",
        "outputId": "dd339949-d738-47fa-ff5b-f04967d4cd8e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### model.py File ###\n",
        "\n",
        "def make_layers(self):\n",
        "    layers = []\n",
        "    # initialize layers in a loop that uses conditionals to determine the input and output dimensions of the feature vectors\n",
        "    for i in range(self.num_layers):\n",
        "        if i == 0:  # first layer\n",
        "            # dimensions in = input data size\n",
        "            # dimensions out = hidden layer size\n",
        "            layer = GATConv(dataset.num_features, self.num_hidden, heads=self.in_head, dropout=0.6)\n",
        "\n",
        "        elif i < self.num_layers - 1: # hidden layer(s)\n",
        "            # dimensions in = hidden layer size\n",
        "            # dimensions out = hidden layer size\n",
        "            # shouldn't hit this case for only two layers anyways\n",
        "            layer = GATConv(self.num_hidden, self.num_hidden)\n",
        "\n",
        "\n",
        "        else:  # output layer\n",
        "            # dimensions in = hidden layer size\n",
        "            # dimensions out = output size\n",
        "            layer = GATConv(self.num_hidden*self.in_head, self.num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "        layers.append(layer)\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "class GAT_model(torch.nn.Module):\n",
        "    def __init__(self, args):\n",
        "        #super(GAT_model, self).__init__()\n",
        "        super().__init__()\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "        self.num_features = dataset.num_features\n",
        "        self.num_layers = args.num_layers\n",
        "        self.num_classes = dataset.num_classes\n",
        "        self.layers = make_layers(self)\n",
        "        self.lr = args.lr\n",
        "\n",
        "    def forward(self, x, edge_idx):\n",
        "      for i, layer in enumerate(self.layers):\n",
        "\n",
        "        # apply the Attention layer\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = layer(x, edge_idx)\n",
        "\n",
        "        # Since I did not apply the activation function in the Layers array, I apply it using conditionals (to decide relu or softmax) here\n",
        "        if i != len(self.layers) - 1:\n",
        "          x = F.elu(x)\n",
        "        else:\n",
        "          x = F.log_softmax(x, dim = 1)\n",
        "\n",
        "      return x\n",
        "\n"
      ],
      "metadata": {
        "id": "Suss9nWCvYey"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py File ###\n",
        "\n",
        "def train(model, X, Y, data):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = model.lr)\n",
        "    optimizer.zero_grad()\n",
        "    activations = model(X, data.edge_index)\n",
        "\n",
        "    # only calculate loss on train labels!!\n",
        "    loss = F.nll_loss(activations[data.train_mask], Y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def get_masked_acc(activations, y_true, mask):\n",
        "    length = activations[mask].shape[0]\n",
        "    correct = 0\n",
        "    for yhat, y in zip(activations[mask], y_true[mask]):\n",
        "        if torch.argmax(yhat) == y:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / length\n",
        "\n",
        "def get_accuracy(activations, y_true, data):\n",
        "    train_acc = get_masked_acc(activations, y_true, data.train_mask)\n",
        "    test_acc = get_masked_acc(activations, y_true, data.test_mask)\n",
        "    val_acc = get_masked_acc(activations, y_true, data.val_mask)\n",
        "    return train_acc, test_acc, val_acc\n",
        "\n",
        "def main():\n",
        "    # use gpu if possible (works most of the time here on colab)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # get data\n",
        "    data = dataset[0].to(device)\n",
        "    x = data.x\n",
        "    y = data.y\n",
        "\n",
        "    # get preferences\n",
        "    args = Args()\n",
        "    args = add_data_features(args, data)\n",
        "\n",
        "\n",
        "    for run in range(args.runs):\n",
        "        # initialize model\n",
        "        model = GAT_model(args).to(device)\n",
        "        print(\"\\n------------ new model ------------\\n\")\n",
        "        for epoch in range(args.epochs):\n",
        "          # log loss every 50 steps\n",
        "            if epoch % 50 == 0 or epoch == args.epochs - 1:\n",
        "                model.eval()\n",
        "                activations = model(x, data.edge_index)\n",
        "                loss = F.nll_loss(activations, y)\n",
        "                train_acc, test_acc, val_acc = get_accuracy(activations, y, data)\n",
        "                print(f\" Epoch: {epoch} | Total Loss: {loss} | Train Accuracy: {train_acc} | Test Accuracy: {test_acc} | Val Accuracy: {val_acc}\")\n",
        "\n",
        "            # backprop & update\n",
        "            train(model, x, y, data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pznSWxDEzEIe",
        "outputId": "e83ce67a-28d8-4132-8cfc-82762c4688de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 1.9295308589935303 | Train Accuracy: 0.1357142857142857 | Test Accuracy: 0.216 | Val Accuracy: 0.236\n",
            " Epoch: 50 | Total Loss: 1.3065060377120972 | Train Accuracy: 0.9428571428571428 | Test Accuracy: 0.774 | Val Accuracy: 0.782\n",
            " Epoch: 100 | Total Loss: 0.8845387101173401 | Train Accuracy: 0.9642857142857143 | Test Accuracy: 0.802 | Val Accuracy: 0.794\n",
            " Epoch: 150 | Total Loss: 0.6898713707923889 | Train Accuracy: 0.9785714285714285 | Test Accuracy: 0.81 | Val Accuracy: 0.8\n",
            " Epoch: 200 | Total Loss: 0.6129773259162903 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.81 | Val Accuracy: 0.796\n",
            " Epoch: 250 | Total Loss: 0.5817471146583557 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.813 | Val Accuracy: 0.794\n",
            " Epoch: 299 | Total Loss: 0.5772804021835327 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.811 | Val Accuracy: 0.792\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 1.9376516342163086 | Train Accuracy: 0.18571428571428572 | Test Accuracy: 0.167 | Val Accuracy: 0.186\n",
            " Epoch: 50 | Total Loss: 1.3365418910980225 | Train Accuracy: 0.9357142857142857 | Test Accuracy: 0.786 | Val Accuracy: 0.772\n",
            " Epoch: 100 | Total Loss: 0.9473889470100403 | Train Accuracy: 0.9642857142857143 | Test Accuracy: 0.801 | Val Accuracy: 0.772\n",
            " Epoch: 150 | Total Loss: 0.7526307106018066 | Train Accuracy: 0.9785714285714285 | Test Accuracy: 0.801 | Val Accuracy: 0.782\n",
            " Epoch: 200 | Total Loss: 0.6569034457206726 | Train Accuracy: 0.9857142857142858 | Test Accuracy: 0.805 | Val Accuracy: 0.786\n",
            " Epoch: 250 | Total Loss: 0.6279401183128357 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.802 | Val Accuracy: 0.782\n",
            " Epoch: 299 | Total Loss: 0.6115110516548157 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.8 | Val Accuracy: 0.774\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 1.9354287385940552 | Train Accuracy: 0.17857142857142858 | Test Accuracy: 0.129 | Val Accuracy: 0.142\n",
            " Epoch: 50 | Total Loss: 1.2505316734313965 | Train Accuracy: 0.9428571428571428 | Test Accuracy: 0.813 | Val Accuracy: 0.8\n",
            " Epoch: 100 | Total Loss: 0.8262131810188293 | Train Accuracy: 0.9642857142857143 | Test Accuracy: 0.822 | Val Accuracy: 0.812\n",
            " Epoch: 150 | Total Loss: 0.6598498821258545 | Train Accuracy: 0.9785714285714285 | Test Accuracy: 0.827 | Val Accuracy: 0.804\n",
            " Epoch: 200 | Total Loss: 0.5934327244758606 | Train Accuracy: 0.9857142857142858 | Test Accuracy: 0.824 | Val Accuracy: 0.792\n",
            " Epoch: 250 | Total Loss: 0.5700283646583557 | Train Accuracy: 1.0 | Test Accuracy: 0.823 | Val Accuracy: 0.78\n",
            " Epoch: 299 | Total Loss: 0.5710608959197998 | Train Accuracy: 1.0 | Test Accuracy: 0.816 | Val Accuracy: 0.772\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 1.9477474689483643 | Train Accuracy: 0.12857142857142856 | Test Accuracy: 0.132 | Val Accuracy: 0.156\n",
            " Epoch: 50 | Total Loss: 1.3448805809020996 | Train Accuracy: 0.9285714285714286 | Test Accuracy: 0.782 | Val Accuracy: 0.77\n",
            " Epoch: 100 | Total Loss: 0.9689669013023376 | Train Accuracy: 0.9642857142857143 | Test Accuracy: 0.801 | Val Accuracy: 0.79\n",
            " Epoch: 150 | Total Loss: 0.7835783362388611 | Train Accuracy: 0.9714285714285714 | Test Accuracy: 0.794 | Val Accuracy: 0.788\n",
            " Epoch: 200 | Total Loss: 0.6752982139587402 | Train Accuracy: 0.9857142857142858 | Test Accuracy: 0.797 | Val Accuracy: 0.796\n",
            " Epoch: 250 | Total Loss: 0.62162184715271 | Train Accuracy: 0.9857142857142858 | Test Accuracy: 0.803 | Val Accuracy: 0.792\n",
            " Epoch: 299 | Total Loss: 0.6022016406059265 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.8 | Val Accuracy: 0.786\n",
            "\n",
            "------------ new model ------------\n",
            "\n",
            " Epoch: 0 | Total Loss: 1.9749476909637451 | Train Accuracy: 0.03571428571428571 | Test Accuracy: 0.069 | Val Accuracy: 0.072\n",
            " Epoch: 50 | Total Loss: 1.3200843334197998 | Train Accuracy: 0.9428571428571428 | Test Accuracy: 0.814 | Val Accuracy: 0.778\n",
            " Epoch: 100 | Total Loss: 0.9046663045883179 | Train Accuracy: 0.9714285714285714 | Test Accuracy: 0.817 | Val Accuracy: 0.798\n",
            " Epoch: 150 | Total Loss: 0.7086116671562195 | Train Accuracy: 0.9714285714285714 | Test Accuracy: 0.817 | Val Accuracy: 0.8\n",
            " Epoch: 200 | Total Loss: 0.628502607345581 | Train Accuracy: 0.9857142857142858 | Test Accuracy: 0.818 | Val Accuracy: 0.796\n",
            " Epoch: 250 | Total Loss: 0.5926771759986877 | Train Accuracy: 0.9928571428571429 | Test Accuracy: 0.823 | Val Accuracy: 0.8\n",
            " Epoch: 299 | Total Loss: 0.5758970379829407 | Train Accuracy: 1.0 | Test Accuracy: 0.824 | Val Accuracy: 0.808\n"
          ]
        }
      ]
    }
  ]
}