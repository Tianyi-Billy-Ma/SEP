{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GIN\n",
        "- Incomplete, needs more work even for planetoid dataset\n",
        "- need to run this on heterogeneous AMiner, DBLP"
      ],
      "metadata": {
        "id": "kwW0cPDPGxyS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpokWmo--kA3"
      },
      "outputs": [],
      "source": [
        "# choose which dataset you want to run on. Default: AMiner\n",
        "user_dataset = \"Planetoid\"\n",
        "# For heterogeneous: choose which label you want to compute loss against. Default: Author\n",
        "label = \"author\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric > /dev/null"
      ],
      "metadata": {
        "id": "84sp4VHp-5IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import GINConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MLP"
      ],
      "metadata": {
        "id": "svja5y-W-6s0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### arguments.py File ###\n",
        "### Modified for colab since colab does not seem to like argparse ###\n",
        "\n",
        "# Defined custom class to hold arguments\n",
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.root_dir = \"/content\"\n",
        "    self.data_dir = \"/content/data\"\n",
        "    self.epochs = 300\n",
        "    self.runs = 5\n",
        "    self.droput = 0.4\n",
        "    self.lr = 0.001\n",
        "    self.wd = 0.001\n",
        "    self.num_layers = 5\n",
        "    self.num_hidden = 8\n",
        "    self.num_features = 0 # placeholder\n",
        "    self.num_classes = 0 # placeholder\n",
        "\n",
        "def add_data_features(args, data):\n",
        "  args.num_features = data.x.shape[1]\n",
        "  args.num_classes = data.y.shape[0]\n",
        "  return args"
      ],
      "metadata": {
        "id": "RVIrV1iv-9c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### data.py File ###\n",
        "\n",
        "args = Args()\n",
        "\n",
        "if user_dataset == \"AMiner\":\n",
        "  from torch_geometric.datasets import AMiner\n",
        "  dataset = AMiner(root=args.root_dir)\n",
        "elif user_dataset == \"DBLP\":\n",
        "  from torch_geometric.datasets import DBLP\n",
        "  dataset = DBLP(root=args.root_dir)\n",
        "elif user_dataset == \"Planetoid\":\n",
        "  from torch_geometric.datasets import Planetoid\n",
        "  dataset = Planetoid(root=args.root_dir, name=\"Cora\")\n",
        "else:\n",
        "  print(f\"ERROR! Your dataset {user_dataset} was not found.\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "EVSVy50V--DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### model.py File ###\n",
        "\n",
        "def make_layers(self):\n",
        "    layers = []\n",
        "    # initialize layers in a loop that uses conditionals to determine the input and output dimensions of the feature vectors\n",
        "    for i in range(self.num_layers):\n",
        "        if i == 0:  # first layer\n",
        "            # dimensions in = input data size\n",
        "            # dimensions out = hidden layer size\n",
        "            layer = GATConv(dataset.num_features, self.num_hidden, heads=self.in_head, dropout=0.6)\n",
        "            mlp = MLP([dataset.num_features, hidden_channels, hidden_channels])\n",
        "            layer = GINConv(nn=mlp, train_eps=False)\n",
        "\n",
        "        elif i < self.num_layers - 1: # hidden layer(s)\n",
        "            # dimensions in = hidden layer size\n",
        "            # dimensions out = hidden layer size\n",
        "            mlp = MLP([hidden_channels, hidden_channels, hidden_channels])\n",
        "            layer = GINConv(nn=mlp, train_eps=False)\n",
        "\n",
        "\n",
        "        else:  # output layer\n",
        "            # dimensions in = hidden layer size\n",
        "            # dimensions out = output size\n",
        "            layer = GATConv(self.num_hidden*self.in_head, self.num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "        layers.append(layer)\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "class GAT_model(torch.nn.Module):\n",
        "    def __init__(self, args):\n",
        "        #super(GAT_model, self).__init__()\n",
        "        super().__init__()\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "        self.num_features = dataset.num_features\n",
        "        self.num_layers = args.num_layers\n",
        "        self.num_classes = dataset.num_classes\n",
        "        self.layers = make_layers(self)\n",
        "        self.lr = args.lr\n",
        "\n",
        "        self.mlp = TwoLayerMLP([num_hidden, num_hidden, out_channels],\n",
        "                       norm=None, dropout=0.5)\n",
        "\n",
        "    def forward(self, x, edge_idx):\n",
        "      for i, layer in enumerate(self.layers):\n",
        "\n",
        "        # apply the Attention layer\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = layer(x, edge_idx)\n",
        "\n",
        "        # Since I did not apply the activation function in the Layers array, I apply it using conditionals (to decide relu or softmax) here\n",
        "        if i != len(self.layers) - 1:\n",
        "          x = F.elu(x)\n",
        "        else:\n",
        "          x = F.log_softmax(x, dim = 1)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A0VzyXLp_AEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### main.py File ###\n",
        "\n",
        "def train(model, X, Y, data):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = model.lr)\n",
        "    optimizer.zero_grad()\n",
        "    activations = model(X, data.edge_index)\n",
        "\n",
        "    # only calculate loss on train labels!!\n",
        "    loss = F.nll_loss(activations[data.train_mask], Y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def get_masked_acc(activations, y_true, mask):\n",
        "    length = activations[mask].shape[0]\n",
        "    correct = 0\n",
        "    for yhat, y in zip(activations[mask], y_true[mask]):\n",
        "        if torch.argmax(yhat) == y:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / length\n",
        "\n",
        "def get_accuracy(activations, y_true, data):\n",
        "    train_acc = get_masked_acc(activations, y_true, data.train_mask)\n",
        "    test_acc = get_masked_acc(activations, y_true, data.test_mask)\n",
        "    val_acc = get_masked_acc(activations, y_true, data.val_mask)\n",
        "    return train_acc, test_acc, val_acc\n",
        "\n",
        "def main():\n",
        "    # use gpu if possible (works most of the time here on colab)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # get data\n",
        "    data = dataset[0].to(device)\n",
        "    x = data.x\n",
        "    y = data.y\n",
        "\n",
        "    # get preferences\n",
        "    args = Args()\n",
        "    args = add_data_features(args, data)\n",
        "\n",
        "\n",
        "    for run in range(args.runs):\n",
        "        # initialize model\n",
        "\n",
        "        model = GIN(\n",
        "                in_channels=args.num_features,\n",
        "                num_hidden=32,\n",
        "                 out_channels=args.num_classes,\n",
        "                num_layers=5,\n",
        "                ).to(device)\n",
        "        print(\"\\n------------ new model ------------\\n\")\n",
        "        for epoch in range(args.epochs):\n",
        "          # log loss every 50 steps\n",
        "            if epoch % 50 == 0 or epoch == args.epochs - 1:\n",
        "                model.eval()\n",
        "                activations = model(x, data.edge_index)\n",
        "                loss = F.nll_loss(activations, y)\n",
        "                train_acc, test_acc, val_acc = get_accuracy(activations, y, data)\n",
        "                print(f\" Epoch: {epoch} | Total Loss: {loss} | Train Accuracy: {train_acc} | Test Accuracy: {test_acc} | Val Accuracy: {val_acc}\")\n",
        "\n",
        "            # backprop & update\n",
        "            train(model, x, y, data)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "nHxAvwYSCqA5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}