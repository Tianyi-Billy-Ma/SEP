{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tweepy' has no attribute 'OAuth1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consumer_key, consumer_secret, access_token, access_token_secret\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# potential library for websraping  \"playwright.sync_api\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# use cases are web scraping and broswer automation, offers the\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# flexibility to interact with any web content, and is not limited\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# to API\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOAuth1\u001b[49m(\n\u001b[0;32m     14\u001b[0m     consumer_key\u001b[38;5;241m=\u001b[39mconsumer_key,\n\u001b[0;32m     15\u001b[0m     consumer_secret\u001b[38;5;241m=\u001b[39mconsumer_secret,\n\u001b[0;32m     16\u001b[0m     access_token\u001b[38;5;241m=\u001b[39maccess_token,\n\u001b[0;32m     17\u001b[0m     access_token_secret\u001b[38;5;241m=\u001b[39maccess_token_secret\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create API object\u001b[39;00m\n\u001b[0;32m     21\u001b[0m api \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mAPI(auth)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tweepy' has no attribute 'OAuth1'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tweepy \n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from config import consumer_key, consumer_secret, access_token, access_token_secret\n",
    "# potential library for websraping  \"playwright.sync_api\"\n",
    "# use cases are web scraping and broswer automation, offers the\n",
    "# flexibility to interact with any web content, and is not limited\n",
    "# to API\n",
    "\n",
    "\n",
    "auth = tweepy.OAuth1(\n",
    "    consumer_key=consumer_key,\n",
    "    consumer_secret=consumer_secret,\n",
    "    access_token=access_token,\n",
    "    access_token_secret=access_token_secret\n",
    ")\n",
    "\n",
    "# Create API object\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "\n",
    "url = 'https://allmysportsteamssuck.com/ncaa-division-i-football-and-basketball-twitter-hashtags-and-handles/'\n",
    "\n",
    "def scrape_team_twitter_info():\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200: #okay\n",
    "        soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "        # find ranks table \n",
    "        table = soup.find(\"table\",attrs={\"id\":\"rankingstable\"})\n",
    "\n",
    "        # parse the header\n",
    "        thread = table.find(\"thead\")\n",
    "\n",
    "        # grab all the column names from the header\n",
    "        ths = thread.find_all(\"th\")\n",
    "        col_names = [th.get_text() for th in ths]\n",
    "\n",
    "        # parse the tbdoy, a table (2d List) of all the rows in the body\n",
    "        tbody = table.find('tbody')\n",
    "        trs = tbody.find_all('tr')\n",
    "        rows = []\n",
    "        for tr in trs:\n",
    "            row = []\n",
    "            tds = tr.find_all('td')\n",
    "            for td in tds:\n",
    "                row.append(td.get_text())\n",
    "            rows.append(row)\n",
    "\n",
    "        # make into data frame\n",
    "        df = pd.DataFrame(rows,columns = col_names)\n",
    "        df = df.set_index(\"School\")\n",
    "        return df\n",
    "    return None # TODO: should do better error handling \n",
    "\n",
    "\n",
    "def fetch_user_account_info(client,username):\n",
    "    response = client.get_user(username=username, user_fields=[\"created_at\",\"description\",\"public_metrics\"])\n",
    "    user = response.data\n",
    "    print(user.keys())\n",
    "\n",
    "df = scrape_team_twitter_info()\n",
    "print(df.loc[\"Gonzaga\"])\n",
    "\n",
    "# open the file that contains the bearer token\n",
    "with open(\"twitter_keys.json\") as infile:\n",
    "\n",
    "    # opne file and save as json obj and pull the token \n",
    "    json_obj =json.load(infile)\n",
    "    token = json_obj[\"bearer_token\"]\n",
    "\n",
    "    # authenticating with the twitter API\n",
    "    client = tweepy.Client(bearer_token=token)\n",
    "zag_username = df.loc[\"Gonzaga\",\"Menâ€™s Basketball Team\"][1:]\n",
    "print(zag_username)\n",
    "user_ser = fetch_user_account_info(client,zag_username)\n",
    "print(user_ser)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
