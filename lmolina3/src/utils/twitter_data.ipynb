{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client, TooManyRequests\n",
    "from account_info import USERNAME, EMAIL, PASSWORD\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# client = Client('en-US')\n",
    "# client.login( \n",
    "#     auth_info_1='god_comput74080' ,\n",
    "#     auth_info_2='computerscienceenrichment@outlook.com',\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "\n",
    "client = Client('en-US')\n",
    "\n",
    "client.login( \n",
    "    auth_info_1='TonyJ21128' ,\n",
    "    auth_info_2='notredamelogin@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "client2 = Client('en-US')\n",
    "\n",
    "client2.login( \n",
    "    auth_info_1='turo343994' ,\n",
    "    auth_info_2='summerenrichmentprogramcs@outlook.com',\n",
    "    password='Molina.2005'\n",
    ")\n",
    "\n",
    "# client3 = Client('en-US')\n",
    "\n",
    "# client3.login( \n",
    "#     auth_info_1='tony00551251172' ,\n",
    "#     auth_info_2='tonystarkwon@outlook.com',\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "\n",
    "# save cookies in order to pull data without getting banned\n",
    "# client.save_cookies('cookies.json')\n",
    "client.save_cookies('cookies1.json')\n",
    "client2.save_cookies('cookies2.json')\n",
    "# client3.save_cookies('cookies3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client5 = Client('en-US')\n",
    "\n",
    "# client5.login( \n",
    "#     auth_info_1='LebronJack27791',\n",
    "#     auth_info_2='project2forser@outlook.com',\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "client4 = Client('en-US')\n",
    "\n",
    "client4.login( \n",
    "    auth_info_1='StarkMiche4868' ,\n",
    "    auth_info_2='project1forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "\n",
    "client6 = Client('en-US')\n",
    "\n",
    "client6.login( \n",
    "    auth_info_1='sanches50767' ,\n",
    "    auth_info_2='project3forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "\n",
    "client7 = Client('en-US')\n",
    "\n",
    "client7.login( \n",
    "    auth_info_1='tonymolnar83096' ,\n",
    "    auth_info_2='project4forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "client8 = Client('en-US')\n",
    "client8.login( \n",
    "    auth_info_1='michealjon77794' ,\n",
    "    auth_info_2='project5forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "client4.save_cookies('cookies4.json')\n",
    "client5.save_cookies('cookies5.json')\n",
    "client6.save_cookies('cookies6.json')\n",
    "client7.save_cookies('cookies7.json')\n",
    "client8.save_cookies('cookies8.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to excel sheet\n",
    "file_path_to_excel = r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(URL, save_as):\n",
    "    '''Function uses image URL and saves the image onto the desired path and file type'''\n",
    "    urllib.request.urlretrieve(URL,save_as)\n",
    "\n",
    "\n",
    "def delete_jpg(file):\n",
    "    '''This file takes in the file path and deletes it.'''\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "\n",
    "# this saves the json file on the described directory \n",
    "def save_json(file, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(file, f,indent=4)\n",
    "\n",
    "\n",
    "def open_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "\n",
    "\n",
    "def retry_on_rate_limit_error(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TooManyRequests as e:\n",
    "            print(\"Rate limit exceed trying again in 60 sec\")\n",
    "            time.sleep(60)\n",
    "            retries += 1\n",
    "    raise Exception(\"Max tries reached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def keywords_to_dict(file_path =r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\" ):\n",
    "    '''This function takes in the excel sheet that has keywords and its ids split up into columns.\n",
    "    Then it creates two dictionaries, keyname and keyname_id. The values are numbered 0 to the \n",
    "    length of the columns. Once created they return these dictionaries.'''\n",
    "\n",
    "    # reads in excel sheet into pandas data frame\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # this turns the street name and its code name into list \n",
    "    street_name_list = df['keyname'].astype(str).values.tolist()\n",
    "    street_name_code_list = df['keyname_id'].astype(str).values.tolist()\n",
    "\n",
    "    # this dictionary contains the keyname in the keys and keyname id in values\n",
    "    keyname = {}\n",
    "    keyname_id = {}\n",
    "    for i in range(33):\n",
    "        keyname[i] = street_name_list[i]\n",
    "        keyname_id[i] = street_name_code_list[i]\n",
    "        \n",
    "    return keyname, keyname_id\n",
    "\n",
    "keyname, keyname_id = keywords_to_dict()\n",
    "print(keyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keyword_search_in_users(keyname_to_search, list_of_users, list_to_append, keywords_dict, iteration):\n",
    "    \"\"\"This function takes in a keyname to search a list of users. It will search the users name,\n",
    "    keyname, description and tweets for the keyname. If it finds it on the tweets it appends\n",
    "    to the list_to_append\"\"\"\n",
    "\n",
    "    keynames, keyname_ids = keywords_to_dict()\n",
    "    keyword_id = keyname_ids[iteration]\n",
    "    # gets a user from the list_of_users and goes through their name, screen name \n",
    "    # and description to find keywords. If they find one they append the user to the list]\n",
    "    \n",
    "    for user in list_of_users:\n",
    "        if len(list_to_append) == 20:\n",
    "            break\n",
    "        else: \n",
    "            if keyname_to_search in user.name:\n",
    "                keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                list_to_append.append('user_' +  user.id)\n",
    "\n",
    "            elif keyname_to_search in user.screen_name:\n",
    "                keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                list_to_append.append('user_' + user.id)\n",
    "\n",
    "            elif keyname_to_search in user.description:\n",
    "                keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                list_to_append.append('user_' + user.id)\n",
    "\n",
    "    return list_to_append, keywords_dict\n",
    "\n",
    "\n",
    "def keyword_search_in_tweets(tweet):\n",
    "    \"\"\"This function takes in tweets, keyname, and keyname ids. Then \n",
    "    it goes thourgh the keynames and trys to find it in the text of the \n",
    "    tweet. If it finds it appends it to a list and at the end it returns this\n",
    "    list.\"\"\"\n",
    "\n",
    "    keyname, keyname_id = keywords_to_dict()\n",
    "    keyname_found_in_tweet = []\n",
    "\n",
    "    # find keyword in tweet text, if so append to list\n",
    "    for keys, keyword in keyname.items():\n",
    "        if keyword in tweet.full_text:\n",
    "            keyname_found_in_tweet.append(keyname_id[keys])\n",
    "    \n",
    "    return keyname_found_in_tweet\n",
    "\n",
    "\n",
    "def relations_in_comments(tweet, reply, relations):\n",
    "            \n",
    "            Relation_Structure = {\n",
    "                \"src_id\": 'tweet_' + str(reply.id),\n",
    "                \"relation\": 'comment-under-post',\n",
    "                \"dest_id\": 'tweet_' + str(tweet.id)\n",
    "            }\n",
    "\n",
    "            reply_user = reply.user\n",
    "            relations.append(Relation_Structure)\n",
    "\n",
    "            Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(reply_user.id),\n",
    "                \"relation\": 'user-make-comment',\n",
    "                \"dest_id\": 'tweet_' + str(reply.id)\n",
    "            }\n",
    "\n",
    "            relations.append(Relation_Structure)\n",
    "\n",
    "            keywords, keywords_ids = keywords_to_dict()\n",
    "\n",
    "            for key, keyname in keywords.items():\n",
    "                if keyname in reply.full_text:\n",
    "                    Relation_Structure = {\n",
    "                        \"src_id\": 'tweet_' + str(reply.id),\n",
    "                        \"relation\": 'comment-contain-keyword',\n",
    "                        \"dest_id\": keywords_ids[key]\n",
    "                    }\n",
    "\n",
    "                    relations.append(Relation_Structure)\n",
    "            return relations\n",
    "\n",
    "\n",
    "def remove_duplicates_from_list(list):\n",
    "    \"\"\"This functions removes duplicates from list\"\"\"\n",
    "    result = []\n",
    "    for item in list:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "\n",
    "def tweet_mentions_user(user, tweet, relations):\n",
    "    \"\"\"This function is for when a user mentions another user in a tweet. Finds \n",
    "    mentioned users user_name and adds the relation_structure to relations.\"\"\"\n",
    "\n",
    "    mentions = re.findall(r'@(\\S+)', tweet.full_text)\n",
    "    mentions = remove_duplicates_from_list(mentions)\n",
    "    for mention in mentions:\n",
    "        try:\n",
    "            mentioned_user = client.get_user_by_screen_name(mention)\n",
    "            Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(user.id),\n",
    "                \"relation\": 'user-mention/tag-user',\n",
    "                \"dest_id\": 'user_' + str(mentioned_user.id)\n",
    "            }\n",
    "\n",
    "            relations.append(Relation_Structure)\n",
    "        except:\n",
    "            pass\n",
    "    return relations \n",
    "\n",
    "\n",
    "def extract_tweet_picture_structure(user, tweet, pic_id_starter, relations):\n",
    "    \"\"\"This function takes in the type user and tweet. Then goes down the post\n",
    "    structure sorting out the data. Finally it returns the post_structure\"\"\"\n",
    "    \n",
    "    if '@' in tweet.full_text:\n",
    "        relations = tweet_mentions_user(user, tweet, relations)\n",
    "\n",
    "    # retrieve keywords found in tweet\n",
    "    keyname_found = keyword_search_in_tweets(tweet)\n",
    "    # construct post structure\n",
    "    favoriters = tweet.get_favoriters(20)\n",
    "\n",
    "    Relation_Structure = {\n",
    "            \"src_id\": 'user_' + str(user.id),\n",
    "            \"relation\": 'user-publish-post',\n",
    "            \"dest_id\": 'tweet_' + str(tweet.id)\n",
    "        }\n",
    "\n",
    "    relations.append(Relation_Structure)\n",
    "\n",
    "    Post_Structure = {\n",
    "        \"user_id\": 'user_' + str(user.id),\n",
    "        \"post_id\": 'tweet_' + str(tweet.id),\n",
    "        \"user_comment\": tweet.full_text,\n",
    "        \"pic_id\": \"\",\n",
    "        \"liked_users\": [ 'user_' + str(favoriter.id) for favoriter in favoriters],\n",
    "        \"comments\": '',\n",
    "        \"keywords\": keyname_found,\n",
    "    }\n",
    "    if keyname_found:\n",
    "        tweet = client.get_tweet_by_id(tweet.id)\n",
    "        replies = tweet.replies\n",
    "        if replies:\n",
    "            Post_Structure[\"comments\"] = [f'tweet_{reply.id}' for reply in replies]\n",
    "            \n",
    "            for reply in replies:\n",
    "                relations = relations_in_comments(tweet, reply, relations)\n",
    "    elif tweet.replies:\n",
    "        replies = tweet.replies\n",
    "        if replies:\n",
    "            Post_Structure[\"comments\"] = [f'tweet_{reply.id}' for reply in replies]\n",
    "            \n",
    "            for reply in replies:\n",
    "                relations = relations_in_comments(tweet, reply, relations)\n",
    "            \n",
    "    for favoriter in favoriters:\n",
    "        Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(favoriter.id),\n",
    "                \"relation\": 'user-like-post',\n",
    "                \"dest_id\": 'tweet_' + str(tweet.id)\n",
    "            }\n",
    "        \n",
    "        relations.append(Relation_Structure)\n",
    "         \n",
    "    media_data = tweet.media\n",
    "    if media_data:\n",
    "        Post_Structure[\"pic_id\"] = 'pic_' + str(pic_id_starter)\n",
    "\n",
    "        Picture_Structure = {\n",
    "            \"pic_id\": 'pic_' + str(pic_id_starter),\n",
    "            \"post_id\": 'tweet_' + str(tweet.id),\n",
    "            \"url\": media_data[0].get('media_url_https')\n",
    "        }\n",
    "\n",
    "        Relation_Structure = {\n",
    "                \"src_id\": 'tweet_' + str(tweet.id),\n",
    "                \"relation\": 'post-has-picture',\n",
    "                \"dest_id\": 'post_' + str(pic_id_starter)\n",
    "            }\n",
    "        \n",
    "        relations.append(Relation_Structure)\n",
    "        pic_id_starter += 1\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter, relations\n",
    "    \n",
    "    else:\n",
    "        Picture_Structure = {}\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter, relations\n",
    "\n",
    "\n",
    "def get_relations_followers_following(relations, followers_list, user):\n",
    "    '''This function gets the realtions between a user and it's\n",
    "    following/follower list'''\n",
    "\n",
    "    user_id = user.id\n",
    "    for follower in followers_list:\n",
    "        Relation_Structure = {\n",
    "                \"src_id\": follower,\n",
    "                \"relation\": 'user-follow/followed-user',\n",
    "                \"dest_id\": 'user_' + str(user_id)\n",
    "            }\n",
    "                \n",
    "        relations.append(Relation_Structure)\n",
    "    return relations\n",
    "    \n",
    "\n",
    "def realtion_in_profile(user, keyword_id, relations):\n",
    "    '''Finds the relations profile of users. If a keyword is found \n",
    "    in the profile it creates a relation structure for it.'''\n",
    "    Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(user.id),\n",
    "                \"relation\": 'user-profile-keyword',\n",
    "                \"dest_id\": keyword_id\n",
    "            }\n",
    "    relations.append(Relation_Structure)\n",
    "    return relations\n",
    "\n",
    "\n",
    "def realtion_keyword_in_tweet(tweet, keyword_id, relations):\n",
    "    '''Adds relation structure between post and keyword'''\n",
    "    Relation_Structure = {\n",
    "                \"src_id\": 'tweet_' + str(tweet.id),\n",
    "                \"relation\": 'post-include-keyword',\n",
    "                \"dest_id\": keyword_id,\n",
    "            }\n",
    "    relations.append(Relation_Structure)\n",
    "    return relations\n",
    "\n",
    "\n",
    "def add_followers_or_following(followers_list, followers):\n",
    "    if len(followers_list) != 10:\n",
    "        for follower in followers:\n",
    "            if len(followers_list) == 10:\n",
    "                break\n",
    "            elif follower not in followers_list:\n",
    "                followers_list.append('user_' + follower.id)\n",
    "    return followers_list\n",
    "\n",
    "def user_get_followers(user):\n",
    "    followers = user.get_followers(10)\n",
    "    return followers\n",
    "\n",
    "\n",
    "def user_get_following(user):\n",
    "    following = user.get_following(10)\n",
    "    return following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_user_following(user):\n",
    "    try:\n",
    "        followers = user.get_followers(10)\n",
    "        following = user.get_following(10)\n",
    "    except:\n",
    "        followers = []\n",
    "        following = []\n",
    "    if len(followers) == 0:\n",
    "        try:\n",
    "            followers = client.get_user_followers(user.id,10)\n",
    "            following = client.get_user_following(user.id,10)\n",
    "        except:\n",
    "            followers = []\n",
    "            following = []\n",
    "    if len(followers) == 0:\n",
    "        try:\n",
    "            followers = user_get_followers(user)\n",
    "            following = user_get_following(user)\n",
    "        except:\n",
    "            followers = []\n",
    "            following = []\n",
    "    return followers, following\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "    return contents \n",
    "\n",
    "\n",
    "def write_txt_file(file_path, new_contents):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(new_contents)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_client_for_users(counter):\n",
    "    if counter > 4:\n",
    "        counter = 1\n",
    "    if counter == 1:\n",
    "        client.load_cookies('cookies7.json')\n",
    "    elif counter == 2:\n",
    "        client.load_cookies('cookies8.json')\n",
    "    elif counter == 3:\n",
    "        client.load_cookies('cookies7.json')\n",
    "    elif counter == 4:\n",
    "        client.load_cookies('cookies8.json')\n",
    "\n",
    "    return counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_structures_for_twitter(user_keyword, pic_id_num_starter, save_path_list):\n",
    "\n",
    "    account_num = 1\n",
    "    time_counter = 1\n",
    "    counter_client= 1\n",
    "    for i, user in enumerate(user_keyword):\n",
    "        pic_id_num_starter = int(read_txt_file(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\picture_id_num.txt'))\n",
    "        users = open_json(save_path_list[0])\n",
    "        posts = open_json(save_path_list[1])\n",
    "        pictures = open_json(save_path_list[2])\n",
    "        keywords_dict = open_json(save_path_list[3])\n",
    "        relations = open_json(save_path_list[4])\n",
    "        counter_client = change_client_for_users(counter_client)\n",
    "\n",
    "        if f'user_{user.id}' in users:\n",
    "            account_num += 1\n",
    "            continue\n",
    "        print(f'entering {account_num}, {user.screen_name}')\n",
    "        try: \n",
    "\n",
    "            keywords = []\n",
    "\n",
    "            # keyname contains the keynames and keyname_id contains their ids\n",
    "            keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "            # get the users tweets\n",
    "            tweets = user.get_tweets('tweets',10)\n",
    "\n",
    "            followers, following = try_get_user_following(user)\n",
    "            \n",
    "            \n",
    "            followers_list = []\n",
    "            following_list = []\n",
    "            found_in_user_description = False\n",
    "            # add the user or their tweets to keyword_structure if a keyword is found\n",
    "            for i in range(len(keyname)):\n",
    "                keyword = keyname[i]\n",
    "                keyword_id = keyname_id[i]\n",
    "                if keyword in user.name:\n",
    "                    keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                    relations = realtion_in_profile(user, keyword_id, relations)\n",
    "                    keywords.append(keyword_id)\n",
    "                    found_in_user_description = True\n",
    "                elif keyword in user.screen_name:\n",
    "                    keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                    relations = realtion_in_profile(user, keyword_id, relations)\n",
    "                    keywords.append(keyword_id)\n",
    "                    found_in_user_description = True\n",
    "                elif keyword in user.description:\n",
    "                    keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                    relations = realtion_in_profile(user, keyword_id, relations)\n",
    "                    keywords.append(keyword_id)\n",
    "                    found_in_user_description = True\n",
    "                for tweet in tweets:\n",
    "                    if keyword in tweet.full_text:\n",
    "                        keywords_dict[keyword_id][\"ids\"].append('tweet_'+ str(tweet.id))\n",
    "                        relations = realtion_keyword_in_tweet(tweet, keyword_id, relations)\n",
    "                        if found_in_user_description:\n",
    "                            pass\n",
    "                        else:\n",
    "                            keywords.append(keyword_id)\n",
    "                            found_in_user_description = True\n",
    "            if followers:\n",
    "                # go through the users tweets, description, name to find keywords\n",
    "                counter = 0\n",
    "                for keys, keyname in keyname.items():\n",
    "                    # go through the users followers description, tweet, and names to \n",
    "                    # see if it finds any keywords. If it does it adds them to the follower_list\n",
    "                    followers_list, keywords_dict = keyword_search_in_users(keyname, followers, followers_list, keywords_dict, counter)\n",
    "\n",
    "                    # Goes thorugh the users following description, tweet, and names to \n",
    "                    # see if it finds any keywords. If it does it adds them to the following_list\n",
    "                    following_list, keywords_dict= keyword_search_in_users(keyname, following, following_list, keywords_dict, counter)\n",
    "                    counter += 1 \n",
    "\n",
    "                # ensure that if not followers or following where found through the keywords\n",
    "                # to add however many followers are left. \n",
    "                followers_list = add_followers_or_following(followers_list, followers)\n",
    "                following_list = add_followers_or_following(following_list, following)\n",
    "\n",
    "                relations = get_relations_followers_following(relations, followers_list, user)\n",
    "                relations = get_relations_followers_following(relations,following_list, user)\n",
    "\n",
    "            # Structure to store the data\n",
    "            User_Structure = {\n",
    "                \"username\": user.screen_name,\n",
    "                \"user_id\": 'user_' + str(user.id), \n",
    "                \"followers\": followers_list,\n",
    "                \"followees\": following_list, #['user_' + str(followees_id) for followees_id in client.get_friends_ids(user.id,user.screen_name,30)]\n",
    "                \"profile_pic\": user.profile_image_url,\n",
    "                \"profile_text\": user.description,\n",
    "                \"posts\": ['tweet_' + tweet.id for tweet in tweets]  ,\n",
    "                \"keywords\": keywords\n",
    "            }\n",
    "            \n",
    "            # go through each tweet in users tweet \n",
    "            for tweet in tweets:\n",
    "                tweet_id_name = 'tweet_' + str(tweet.id)\n",
    "\n",
    "                # check to see if tweet id not in post_strucutre if not create one for the tweet\n",
    "                if tweet_id_name not in posts:\n",
    "\n",
    "                    # extract the post structure form the tweet and comments \n",
    "                    original_pic_id_num = pic_id_num_starter\n",
    "                    post_structure, picture_structure, pic_id_num_starter, relations = extract_tweet_picture_structure(user, tweet, pic_id_num_starter, relations)\n",
    "                    posts[tweet_id_name] = post_structure\n",
    "                    if picture_structure:\n",
    "                        pictures['pic_' + str(original_pic_id_num)] = picture_structure\n",
    "\n",
    "                replies_in_post = post_structure['comments']\n",
    "                replies_in_post = [reply.replace('tweet_','') for reply in replies_in_post]\n",
    "                if replies_in_post:\n",
    "\n",
    "                    for reply in replies_in_post:\n",
    "                        reply = client.get_tweet_by_id(reply)\n",
    "                        reply_id_name = 'tweet_' +str(reply.id)\n",
    "\n",
    "                        if reply_id_name not in posts:\n",
    "                            original_pic_id_num = pic_id_num_starter\n",
    "                            post_structure, picture_structure, pic_id_num_starter, relations = extract_tweet_picture_structure(reply.user, reply, pic_id_num_starter, relations)\n",
    "                            posts[reply_id_name] = post_structure\n",
    "                            \n",
    "                            if picture_structure:\n",
    "                                pictures['pic_' + str(original_pic_id_num)] = picture_structure\n",
    "\n",
    "                \n",
    "            # save the user to the dictionary\n",
    "            user_id_name = 'user_'+ user.id\n",
    "            users[user_id_name] = User_Structure\n",
    "            write_txt_file(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\picture_id_num.txt', str(pic_id_num_starter))\n",
    "\n",
    "            # list that contains all data structures \n",
    "            # data_structure_for_twitter = [users, posts, pictures, keywords, relations]\n",
    "            save_json(users, save_path_list[0])\n",
    "            save_json(posts, save_path_list[1])\n",
    "            save_json(pictures, save_path_list[2])\n",
    "            save_json(keywords_dict, save_path_list[3])\n",
    "            save_json(relations, save_path_list[4])\n",
    "            print(f'success with {account_num}')\n",
    "\n",
    "            account_num += 1 \n",
    "            # print('30 sec')\n",
    "            # time.sleep(20)\n",
    "            # print('continue')\n",
    "            counter_client += 1\n",
    "        except TooManyRequests as e:\n",
    "            print('Rate limit exceeced')\n",
    "            time_to_wait = 60 * time_counter\n",
    "            time_counter += 1\n",
    "            print(f\"Time to wait {time_to_wait}\")\n",
    "            time.sleep(time_to_wait)\n",
    "            if time_counter == 5:\n",
    "                print('exceeded time rate limits')\n",
    "                break\n",
    "            counter_client += 1\n",
    "            continue \n",
    "\n",
    "\n",
    "    return pic_id_num_starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_structures_to_path(save_path_list):\n",
    "    keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "\n",
    "    # all data structures \n",
    "    users = {}\n",
    "    posts = {}\n",
    "    pictures = {}\n",
    "    keywords_dict = {}\n",
    "    relations = []\n",
    "\n",
    "    for keys, keynames in keyname.items():\n",
    "        keyname_structure = {\n",
    "            \"keyword\": keynames,\n",
    "            \"keyword_id\": keyname_id[keys],\n",
    "            \"ids\": [],\n",
    "        }\n",
    "        keywords_dict[keyname_id[keys]] = keyname_structure\n",
    "\n",
    "    data_structures_to_save = [users ,posts, pictures, keywords_dict, relations]\n",
    "\n",
    "    for i in range(len(save_path_list)):\n",
    "        save_json(data_structures_to_save[i], save_path_list[i])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_client(counter):\n",
    "    if counter == 5:\n",
    "        counter = 0\n",
    "    if counter == 1:\n",
    "        client.load_cookies('cookies4.json')\n",
    "    elif counter == 2:\n",
    "        client.load_cookies('cookies1.json')\n",
    "    elif counter == 3:\n",
    "        client.load_cookies('cookies2.json')\n",
    "    elif counter == 4:\n",
    "        client.load_cookies('cookies1.json')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_user_by_keyword(keyword):\n",
    "    searched_tweets = client.search_tweet(keyword)\n",
    "    user_list = []\n",
    "    for tweet in searched_tweets:\n",
    "        user = tweet.user\n",
    "        if user in user_list:\n",
    "            continue\n",
    "        try: \n",
    "            user_found = client.get_user_by_id(user.id)\n",
    "            user_list.append(user_found)\n",
    "        except:\n",
    "            print('rate limits reached sleep for 2 min')\n",
    "            time.sleep(120)\n",
    "    return user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_list = []\n",
    "keywords, keywords_ids = keywords_to_dict()\n",
    "# all the file paths for all the structures in the end \n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure__2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre__2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure__2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure__2nd_round.json')\n",
    "\n",
    "\n",
    "user_struct = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure__1st round.json')\n",
    "# save_data_structures_to_path(save_path_list)\n",
    "# keyname_list = [keywords[key] for key in range(13)]\n",
    "\n",
    "counter = 1\n",
    "for struct in user_struct.values():\n",
    "    current_struct_work = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_2nd_round.json')\n",
    "\n",
    "    follower_following_list = []\n",
    "    change_client(counter)\n",
    "    try:\n",
    "        follower_following_list.extend( [client.get_user_by_id(follower.replace('user_', '')) for count, follower in enumerate(struct['followers']) if count < 5])\n",
    "        follower_following_list.extend([client.get_user_by_id(followee.replace('user_','')) for count, followee in enumerate(struct['followees']) if count < 5])\n",
    "    except TooManyRequests:\n",
    "        print('sleep for followers ')\n",
    "        time.sleep(120)\n",
    "        print('exit')\n",
    "    # user_list = retry_on_rate_limit_error(client.search_user, keyname, 20)\n",
    "    \n",
    "    pic_id_num_starter = read_txt_file('picture_id_num.txt')\n",
    "\n",
    "    pic_id_num_starter = creating_structures_for_twitter(follower_following_list, int(pic_id_num_starter), save_path_list)\n",
    "    write_txt_file('picture_id_num.txt', str(pic_id_num_starter))\n",
    "    print('')\n",
    "    print('sleep')\n",
    "    time.sleep(60)\n",
    "    print('exit')\n",
    "    counter += 1\n",
    "    if len(current_struct_work) > 150:\n",
    "        print('limits reached')\n",
    "# pic_id_num_starter = 20000000\n",
    "\n",
    "\n",
    "# [0,13] words to search. \n",
    "# list = client.search_user('Amphetamine', 20)\n",
    "# list_20 = list.next()\n",
    "# list_40 = list_20.next()\n",
    "# list_60 = list_40.next()\n",
    "# list_80 = list_60.next()\n",
    "\n",
    "# picture_id_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_list = []\n",
    "keywords, keywords_ids = keywords_to_dict()\n",
    "# all the file paths for all the structures in the end \n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\user_structure_1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\post_structure__1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\picture_strucutre__1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\keyword_structure__1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\relation_structure__1st_round_new_drugs.json')\n",
    "\n",
    "\n",
    "\n",
    "# user_struct = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_1st_round_new_drugs.json')\n",
    "\n",
    "# save_data_structures_to_path(save_path_list)\n",
    "keyname_list = [keywords[key] for key in range(2, 26)]\n",
    "\n",
    "\n",
    "counter = 1\n",
    "for keyname in keyname_list:\n",
    "    print(keyname)\n",
    "    change_client(counter)\n",
    "    tweet1 = client.search_tweet(keyname,'Latest',10)\n",
    "    client_tweets = client.search_tweet(keyname,'Top', 10)\n",
    "    tweet1 = [twt for twt in tweet1]\n",
    "    client_tweets = [twt for twt in client_tweets]\n",
    "    client_tweets.extend(tweet1)\n",
    "    user_list = []\n",
    "    for tweet in client_tweets:\n",
    "        user = tweet.user\n",
    "        user_list.append(user)\n",
    "    # user_list = retry_on_rate_limit_error(client.search_user, keyname, 20)\n",
    "    pic_id_num_starter = read_txt_file('picture_id_num.txt')\n",
    "    pic_id_num_starter = creating_structures_for_twitter(user_list, int(pic_id_num_starter), save_path_list)\n",
    "    write_txt_file('picture_id_num.txt', str(pic_id_num_starter))\n",
    "    print('')\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counter = 0 \n",
    "# for struct in user_struct.values():\n",
    "#     current_struct = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_2nd_round_new_drugs.json')\n",
    "\n",
    "#     print(keywords)\n",
    "#     follower_following_list = []\n",
    "#     change_client(counter)\n",
    "#     try:\n",
    "#         follower_following_list.extend([client.get_user_by_id(follower.replace('user_', '')) for count, follower in enumerate(struct['followers']) if count <=10])\n",
    "#         follower_following_list.extend([client.get_user_by_id(followee.replace('user_','')) for count, followee in enumerate(struct['followees']) if count <=10])\n",
    "#     except TooManyRequests:\n",
    "#         print('sleep')\n",
    "#         time.sleep(120)\n",
    "#         print('exit')\n",
    "\n",
    "    # pic_id_num_starter = read_txt_file('picture_id_num.txt')\n",
    "    # tweet1 = client.search_tweet(keywords,'Latest',20)\n",
    "    # client_tweets = client.search_tweet(keywords,'Top', 20)\n",
    "    # tweet1 = [twt for twt in tweet1]\n",
    "    # client_tweets = [twt for twt in client_tweets]\n",
    "    # client_tweets.extend(tweet1)\n",
    "    # user_list = []\n",
    "    # for tweet in client_tweets:\n",
    "    #     user = tweet.user\n",
    "    #     user_list.append(user)\n",
    "        \n",
    "\n",
    "    # pic_id_num_starter = creating_structures_for_twitter(follower_following_list, int(pic_id_num_starter), save_path_list)\n",
    "    # write_txt_file('picture_id_num.txt', str(pic_id_num_starter))\n",
    "    # print('sleep')\n",
    "    # time.sleep(60)\n",
    "    # print('')\n",
    "    # counter +=1\n",
    "    # if len(current_struct) > 150:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\user_structure_1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\user_structure_1st_round_new_drugs.json']\n",
    "\n",
    "post_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\post_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\post_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\post_structure__1st_round_new_drugs.json']\n",
    "                            \n",
    "picture_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\picture_strucutre__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\picture_strucutre__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\picture_strucutre__1st_round_new_drugs.json']\n",
    "\n",
    "relation_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\relation_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\relation_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\relation_structure__1st_round_new_drugs.json']\n",
    "\n",
    "keyword_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure_1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\keyword_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\keyword_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\keyword_structure__1st_round_new_drugs.json']\n",
    "\n",
    "all_paths = [user_struct_1st_round, post_struct_1st_round, picture_struct_1st_round, relation_struct_1st_round, keyword_struct_1st_round]\n",
    "for i,path_list in enumerate(all_paths):\n",
    "    for path in path_list:\n",
    "        if os.path.exists(path):\n",
    "            pass\n",
    "        else:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_struct_2nd_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\user_structure_2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_2nd_round_new_drugs.json']\n",
    "\n",
    "post_struct_2nd_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\post_structure__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\post_structure__2nd_round_new_drugs.json']\n",
    "\n",
    "picture_struct_2nd_round =  [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\picture_strucutre__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\picture_strucutre__2nd_round_new_drugs.json']\n",
    "\n",
    "relation_struct_2nd_round =  [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\relation_structure__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\relation_structure__2nd_round_new_drugs.json']\n",
    "\n",
    "keyword_struct_2nd_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\keyword_structure__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\keyword_structure__2nd_round_new_drugs.json']\n",
    "\n",
    "all_2nd_paths = [user_struct_2nd_round, post_struct_2nd_round, picture_struct_2nd_round, relation_struct_2nd_round, keywrod_struct_2nd_round]\n",
    "\n",
    "for path_list in all_2nd_paths:\n",
    "    for path in path_list:\n",
    "        if os.path.exists(path):\n",
    "            pass\n",
    "        else:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_struct(path_list):\n",
    "    struct_list = []\n",
    "    for path in path_list:\n",
    "        struct = open_json(path)\n",
    "        struct_list.append(struct)\n",
    "    \n",
    "    final_struct = {}\n",
    "\n",
    "    for struct in struct_list:\n",
    "        for user in struct:\n",
    "            if user in final_struct.keys():\n",
    "                continue \n",
    "            final_struct[user] = struct[user]\n",
    "    return final_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_final_struct(path_list):\n",
    "    list_struct = []\n",
    "    for path in path_list:\n",
    "        struct = open_json(path)\n",
    "        list_struct.append(struct)\n",
    "    \n",
    "    final_list = []\n",
    "    for struct in list_struct:\n",
    "        for relation in struct:\n",
    "            if any(relation == info for info in relation):\n",
    "                continue \n",
    "            final_list.append(relation)\n",
    "\n",
    "    return final_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_final_struct(path_list):\n",
    "    struct_list = []\n",
    "    for path in path_list:\n",
    "        struct = open_json(path)\n",
    "        struct_list.append(struct)\n",
    "    \n",
    "    final_struct = {}\n",
    "\n",
    "    # go into the first list, then go into the dictionary and then each key \n",
    "    for struct in struct_list:\n",
    "        for key in struct:\n",
    "\n",
    "            # if the key in the final structure add ids if they are not in there \n",
    "            if key in final_struct.keys():\n",
    "                ids_list = final_struct[key]['ids']\n",
    "                current_ids_list = struct[key]['ids']\n",
    "\n",
    "                for cur_ids in current_ids_list:\n",
    "                    if cur_ids in ids_list:\n",
    "                        continue\n",
    "        \n",
    "                    ids_list.append(cur_ids)\n",
    "                final_struct[key]['ids'] = ids_list\n",
    "                \n",
    "                continue \n",
    "\n",
    "            # if not in dictionary add it and get rid of all ids that are duplicates. \n",
    "            final_ids = set()\n",
    "            for ids in struct[key]['ids']:\n",
    "                if ids in final_ids:\n",
    "                    continue\n",
    "                final_ids.add(ids)\n",
    "\n",
    "            struct[key]['ids'] = list(final_ids)\n",
    "            final_struct[key] = struct[key]\n",
    "    return final_struct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_list = [final_struct(user_struct_1st_round),\n",
    " final_struct(user_struct_2nd_round),\n",
    "final_struct(post_struct_1st_round),\n",
    "final_struct(post_struct_2nd_round),\n",
    "final_struct(picture_struct_1st_round),\n",
    " final_struct(picture_struct_2nd_round),\n",
    " relation_final_struct(relation_struct_1st_round),\n",
    " relation_final_struct(relation_struct_2nd_round),\n",
    " keyword_final_struct(keyword_struct_1st_round),\n",
    "keyword_final_struct(keyword_struct_2nd_round)]\n",
    "final_struct_list = ['user_1st_round_final_struct', 'user_2nd_round_final_struct', 'post_1st_round_final_struct', 'post_2nd_round_final_struct',\n",
    "                      'picture_1st_round_final_struct', 'picture_2nd_round_final_struct', 'relation_1st_round_final_struct', 'relation_2nd_round_final_struct', \n",
    "                     'keyword_1st_round_final_struct', 'keyword_2nd_round_final_struct']\n",
    "\n",
    "for i, struct in enumerate(final_data_list):\n",
    "    path = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\final_data_structures'\n",
    "    path_file = final_struct_list[i] + '.json'\n",
    "    final_path = os.path.join(path,path_file)\n",
    "    save_json(struct, final_path)\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
