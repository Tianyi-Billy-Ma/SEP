{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client, TooManyRequests\n",
    "from account_info import USERNAME, EMAIL, PASSWORD\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = Client('en-US')\n",
    "\n",
    "client.login(\n",
    "    auth_info_1=USERNAME ,\n",
    "    auth_info_2=EMAIL,\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "# save cookies in order to pull data without getting banned\n",
    "client.save_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the cookies that were saved\n",
    "client.load_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(URL, save_as):\n",
    "    '''Function uses image URL and saves the image onto the desired path and file type'''\n",
    "    urllib.request.urlretrieve(URL,save_as)\n",
    "\n",
    "def delete_jpg(file):\n",
    "    '''This file takes in the file path and deletes it.'''\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "# this saves the json file on the described directory \n",
    "def save_json(file, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(file, f,indent=4)\n",
    "\n",
    "\n",
    "def retry_on_rate_limit_error(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TooManyRequests as e:\n",
    "            print(\"Rate limit exceed trying again in 60 sec\")\n",
    "            time.sleep(60)\n",
    "            retries += 1\n",
    "    raise Exception(\"Max tries reached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'amphetamine', 1: 'Cocaine ', 2: 'crack cocaine', 3: 'fentanyl and fentanyl derivatives', 4: 'GHB', 5: 'Heroin', 6: 'Hydrocodone', 7: 'Klonopin', 8: 'LSDtrip', 9: 'Marijuana', 10: 'Marijuana Concentrates', 11: 'MDMA', 12: 'Mescaline'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# file path to excel sheet\n",
    "file_path = r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\"\n",
    "\n",
    "def keywords_to_dict(file_path =r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\" ):\n",
    "    '''This function takes in the excel sheet that has keywords and its ids split up into columns.\n",
    "    Then it creates two dictionaries, keyname and keyname_id. The values are numbered 0 to the \n",
    "    length of the columns. Once created they return these dictionaries.'''\n",
    "\n",
    "    # reads in excel sheet into pandas data frame\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # this turns the street name and its code name into list \n",
    "    street_name_list = df['keyname'].astype(str).values.tolist()\n",
    "    street_name_code_list = df['keyname_id'].astype(str).values.tolist()\n",
    "\n",
    "    # this dictionary contains the keyname in the keys and keyname id in values\n",
    "    keyname = {}\n",
    "    keyname_id = {}\n",
    "    for i in range(0,13):\n",
    "        keyname[i] = street_name_list[i]\n",
    "        keyname_id[i] = street_name_code_list[i]\n",
    "        \n",
    "    return keyname, keyname_id\n",
    "\n",
    "# print(json.dumps(keyname, indent =4))\n",
    "keyname, keyname_id = keywords_to_dict()\n",
    "print(keyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_list = []\n",
    "\n",
    "# all the file paths for all the structures in the end \n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\comment_structure.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search_in_users(keyname_to_search, list_of_users, list_to_append):\n",
    "    \"\"\"This function takes in a keyname to search a list of users. It will search the users name,\n",
    "    keyname, description and tweets for the keyname. If it finds it on the tweets it appends\n",
    "    to the list_to_append\"\"\"\n",
    "\n",
    "\n",
    "    # gets a user from the list_of_users and goes through their name, screen name \n",
    "    # and description to find keywords. If they find one they append the user to the list\n",
    "    for user in list_of_users:\n",
    "        if len(list_to_append) == 30:\n",
    "            break\n",
    "        elif ('user_' + user.id) not in list_to_append:\n",
    "            if keyname_to_search in user.name:\n",
    "                list_to_append.append('user_' +  user.id)\n",
    "            elif keyname_to_search in user.screen_name:\n",
    "                list_to_append.append('user_' + user.id)\n",
    "            elif keyname_to_search in user.description:\n",
    "                list_to_append.append('user_' + user.id)\n",
    "            # else:\n",
    "            #     # get first 5 tweets and compare to keyname \n",
    "            #     tweets = user.get_tweets('tweets', 5)\n",
    "            #     for tweet in tweets:\n",
    "            #         if keyname_to_search in tweet.text:\n",
    "            #             list_to_append.append('user_' + user.id)\n",
    "    return list_to_append\n",
    "\n",
    "def keyword_search_in_tweets(tweet):\n",
    "    \"\"\"This function takes in tweets, keyname, and keyname ids. Then \n",
    "    it goes thourgh the keynames and trys to find it in the text of the \n",
    "    tweet. If it finds it appends it to a list and at the end it returns this\n",
    "    list.\"\"\"\n",
    "\n",
    "    keyname, keyname_id = keywords_to_dict()\n",
    "    keyname_found_in_tweet = []\n",
    "\n",
    "    # find keyword in tweet text, if so append to list\n",
    "    for keys, keyword in keyname.items():\n",
    "        if keyword in tweet.full_text:\n",
    "            keyname_found_in_tweet.append(keyname_id[keys])\n",
    "    \n",
    "    return keyname_found_in_tweet\n",
    "\n",
    "\n",
    "def extract_tweet_picture_structure(user, tweet, pic_id_starter):\n",
    "    \"\"\"This function takes in the type user and tweet. Then goes down the post\n",
    "    structure sorting out the data. Finally it returns the post_structure\"\"\"\n",
    "\n",
    "    # retrieve keywords found in tweet\n",
    "    keyname_found = keyword_search_in_tweets(tweet)\n",
    "           \n",
    "    # construct post structure\n",
    "    Post_Structure = {\n",
    "        \"user_id\": 'user_' + str(user.id),\n",
    "        \"post_id\": 'tweet_' + str(tweet.id),\n",
    "        \"user_comment\": tweet.full_text,\n",
    "        \"pic_id\": \"pic_id1\",\n",
    "        \"liked_users\": [ 'user_' + str(favoriter.id) for favoriter in tweet.get_favoriters(20)],\n",
    "        \"comments\": '',\n",
    "        \"keywords\": keyname_found,\n",
    "    }\n",
    "    if tweet.has_card:\n",
    "        # image_directoary = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\tweet_images'\n",
    "        # image_file_name = 'tweet_image' + str(tweet.id) + '.jpg'\n",
    "        # image_save_path = os.path.join(image_directoary, image_file_name)\n",
    "        # download_image(tweet.thumbnail_url, image_save_path)\n",
    "\n",
    "        Picture_Structure = {\n",
    "            \"pic_id\": 'pic_' + str(pic_id_starter),\n",
    "            \"post_id\": 'tweet_' + str(tweet.id),\n",
    "            \"url\": tweet.thumbnail_url,\n",
    "        }\n",
    "        pic_id_starter += 1\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter\n",
    "    else:\n",
    "        Picture_Structure = {}\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3084: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n",
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3084: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n",
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3084: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "keyname, keyname_id = keywords_to_dict()\n",
    "search_key = keyname[0]\n",
    "\n",
    "user_keyword = retry_on_rate_limit_error(client.search_user,'Amphetamine', 10)\n",
    "\n",
    "# all data structures \n",
    "users = {}\n",
    "posts = {}\n",
    "comments = {}\n",
    "pictures = {}\n",
    "keywords = {}\n",
    "relations = {}\n",
    "\n",
    "pic_id_num_starter = 20000000000\n",
    "picture_id_dict = {}\n",
    "for user in user_keyword:\n",
    "\n",
    "    try: \n",
    "        # save the user progile pic \n",
    "        image_directoary = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\profile_images'\n",
    "        image_file_name = 'user_profile_pic_' + str(user.id) + '.jpg'\n",
    "        image_save_path = os.path.join(image_directoary, image_file_name)\n",
    "        download_image(user.profile_image_url, image_save_path)\n",
    "\n",
    "        keywords = []\n",
    "\n",
    "        # keyname contains the keynames and keyname_id contains their ids\n",
    "        keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "        # get the users tweets\n",
    "        tweets = user.get_tweets('tweets')\n",
    "\n",
    "        # gets users followers returns a list of users, return user type \n",
    "        followers = user.get_followers(user.id)\n",
    "\n",
    "        # gets users following returns a list of users, return user type\n",
    "        following = user.get_following(user.id)\n",
    "\n",
    "        # empty list for the followers\n",
    "        followers_list = []\n",
    "        following_list = []\n",
    "\n",
    "        # go through the users tweets, description, name to find keywords\n",
    "        for keys, keyname in keyname.items():\n",
    "            if keyname in user.name:\n",
    "                keywords.append(keyname_id[keys])\n",
    "            elif keyname in user.screen_name:\n",
    "                keywords.append(keyname_id[keys])\n",
    "            elif keyname in user.description:\n",
    "                keywords.append(keyname_id[keys])\n",
    "            else:\n",
    "                for tweet in tweets:\n",
    "                    if keyname in tweet.text:\n",
    "                        keywords.append(keyname_id[keys])\n",
    "\n",
    "            # go through the users followers description, tweet, and names to \n",
    "            # see if it finds any keywords. If it does it adds them to the follower_list\n",
    "            followers_list = keyword_search_in_users(keyname, followers, followers_list)\n",
    "\n",
    "            # Goes thorugh the users following description, tweet, and names to \n",
    "            # see if it finds any keywords. If it does it adds them to the following_list\n",
    "            following_list = keyword_search_in_users(keyname, following, following_list)\n",
    "\n",
    "        # Structure to store the data\n",
    "        User_Structure = {\n",
    "            \"username\": user.screen_name,\n",
    "            \"user_id\": 'user_' + str(user.id), \n",
    "            \"followers\": [follower for follower in followers_list],\n",
    "            \"followees\": [following for following in following_list], #['user_' + str(followees_id) for followees_id in client.get_friends_ids(user.id,user.screen_name,30)]\n",
    "            \"profile_pic\": image_save_path,\n",
    "            \"profile_text\": user.description,\n",
    "            \"posts\": ['tweet_' + tweet.id for tweet in tweets]  ,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "        \n",
    "        # go through each tweet in users tweet \n",
    "        for tweet in tweets:\n",
    "            tweet_id_name = 'tweet_' + str(tweet.id)\n",
    "\n",
    "            # check to see if tweet id not in post_strucutre if not create one for the tweet\n",
    "            if tweet_id_name not in posts:\n",
    "\n",
    "                # extract the post structure form the tweet and comments \n",
    "                original_pic_id_num = pic_id_num_starter\n",
    "                post_structure, picture_structure, pic_id_num_starter = extract_tweet_picture_structure(user, tweet, pic_id_num_starter)\n",
    "                posts[tweet_id_name] = post_structure\n",
    "                pictures['pic_' + str(original_pic_id_num)] = picture_structure\n",
    "        # save the user to the dictionary\n",
    "        user_id_name = 'user_'+ user.id\n",
    "        if user_id_name not in users:\n",
    "            users[user_id_name] = User_Structure\n",
    "\n",
    "        # list that contains all data structures \n",
    "        data_structure_for_twitter = [users, posts, comments, pictures, keywords, relations]\n",
    "        save_json(users, save_path_list[0])\n",
    "        save_json(posts, save_path_list[1])\n",
    "        save_json(pictures, save_path_list[3])\n",
    "        \n",
    "    except TooManyRequests as e:\n",
    "        print('Rate limit exceeced')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users)\n",
    "\n",
    "# TODO: Create the rest of the structures\n",
    "# TODO: Create all of the structures under the same for loop and function. Then create the function to loop over every 15 min. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
