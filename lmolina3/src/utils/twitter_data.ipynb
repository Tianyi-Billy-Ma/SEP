{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client, TooManyRequests\n",
    "from account_info import USERNAME, EMAIL, PASSWORD\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = Client('en-US')\n",
    "\n",
    "client.login(\n",
    "    auth_info_1=USERNAME ,\n",
    "    auth_info_2=EMAIL,\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "# save cookies in order to pull data without getting banned\n",
    "client.save_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the cookies that were saved\n",
    "client.load_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(URL, save_as):\n",
    "    '''Function uses image URL and saves the image onto the desired path and file type'''\n",
    "    urllib.request.urlretrieve(URL,save_as)\n",
    "\n",
    "def delete_jpg(file):\n",
    "    '''This file takes in the file path and deletes it.'''\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "# this saves the json file on the described directory \n",
    "def save_json(file, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(file, f,indent=4)\n",
    "\n",
    "\n",
    "def retry_on_rate_limit_error(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TooManyRequests as e:\n",
    "            print(\"Rate limit exceed trying again in 60 sec\")\n",
    "            time.sleep(60)\n",
    "            retries += 1\n",
    "    raise Exception(\"Max tries reached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file path to excel sheet\n",
    "file_path = r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\"\n",
    "\n",
    "def keywords_to_dict(file_path =r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\" ):\n",
    "    '''This function takes in the excel sheet that has keywords and its ids split up into columns.\n",
    "    Then it creates two dictionaries, keyname and keyname_id. The values are numbered 0 to the \n",
    "    length of the columns. Once created they return these dictionaries.'''\n",
    "\n",
    "    # reads in excel sheet into pandas data frame\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # this turns the street name and its code name into list \n",
    "    street_name_list = df['keyname'].astype(str).values.tolist()\n",
    "    street_name_code_list = df['keyname_id'].astype(str).values.tolist()\n",
    "\n",
    "    # this dictionary contains the keyname in the keys and keyname id in values\n",
    "    keyname = {}\n",
    "    keyname_id = {}\n",
    "    for i in range(26):\n",
    "        keyname[i] = street_name_list[i]\n",
    "        keyname_id[i] = street_name_code_list[i]\n",
    "        \n",
    "    return keyname, keyname_id\n",
    "\n",
    "# print(json.dumps(keyname, indent =4))\n",
    "keyname, keyname_id = keywords_to_dict()\n",
    "print(keyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_list = []\n",
    "\n",
    "# all the file paths for all the structures in the end \n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_test.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure_test.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\comment_structure.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre_test.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure_test.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure_test.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search_in_users(keyname_to_search, list_of_users, list_to_append, key_structure_list, iteration):\n",
    "    \"\"\"This function takes in a keyname to search a list of users. It will search the users name,\n",
    "    keyname, description and tweets for the keyname. If it finds it on the tweets it appends\n",
    "    to the list_to_append\"\"\"\n",
    "\n",
    "\n",
    "    # gets a user from the list_of_users and goes through their name, screen name \n",
    "    # and description to find keywords. If they find one they append the user to the list]\n",
    "    \n",
    "    for user in list_of_users:\n",
    "        if len(list_to_append) == 30:\n",
    "            break\n",
    "        else: #('user_' + user.id) not in list_to_append:\n",
    "            if keyname_to_search in user.name:\n",
    "                key_structure_list[iteration][\"ids\"].append('user_' + str(user.id) )\n",
    "                list_to_append.append('user_' +  user.id)\n",
    "            elif keyname_to_search in user.screen_name:\n",
    "                key_structure_list[iteration][\"ids\"].append('user_' + str(user.id) )\n",
    "                list_to_append.append('user_' + user.id)\n",
    "            elif keyname_to_search in user.description:\n",
    "                key_structure_list[iteration][\"ids\"].append('user_' + str(user.id) )\n",
    "                list_to_append.append('user_' + user.id)\n",
    "    return list_to_append, key_structure_list\n",
    "\n",
    "def keyword_search_in_tweets(tweet):\n",
    "    \"\"\"This function takes in tweets, keyname, and keyname ids. Then \n",
    "    it goes thourgh the keynames and trys to find it in the text of the \n",
    "    tweet. If it finds it appends it to a list and at the end it returns this\n",
    "    list.\"\"\"\n",
    "\n",
    "    keyname, keyname_id = keywords_to_dict()\n",
    "    keyname_found_in_tweet = []\n",
    "\n",
    "    # find keyword in tweet text, if so append to list\n",
    "    for keys, keyword in keyname.items():\n",
    "        if keyword in tweet.full_text:\n",
    "            keyname_found_in_tweet.append(keyname_id[keys])\n",
    "    \n",
    "    return keyname_found_in_tweet\n",
    "\n",
    "\n",
    "def extract_tweet_picture_structure(user, tweet, pic_id_starter):\n",
    "    \"\"\"This function takes in the type user and tweet. Then goes down the post\n",
    "    structure sorting out the data. Finally it returns the post_structure\"\"\"\n",
    "\n",
    "    # retrieve keywords found in tweet\n",
    "    keyname_found = keyword_search_in_tweets(tweet)\n",
    "    # construct post structure\n",
    "    Post_Structure = {\n",
    "        \"user_id\": 'user_' + str(user.id),\n",
    "        \"post_id\": 'tweet_' + str(tweet.id),\n",
    "        \"user_comment\": tweet.full_text,\n",
    "        \"pic_id\": \"\",\n",
    "        \"liked_users\": [ 'user_' + str(favoriter.id) for favoriter in tweet.get_favoriters(20)],\n",
    "        \"comments\": '',\n",
    "        \"keywords\": keyname_found,\n",
    "    }\n",
    "\n",
    "           \n",
    "    media_data = tweet.media\n",
    "    if media_data:\n",
    "        Post_Structure[\"pic_id\"] = 'pic_' + str(pic_id_starter)\n",
    "\n",
    "        Picture_Structure = {\n",
    "            \"pic_id\": 'pic_' + str(pic_id_starter),\n",
    "            \"post_id\": 'tweet_' + str(tweet.id),\n",
    "            \"url\": media_data[0].get('media_url_https'),\n",
    "        }\n",
    "        pic_id_starter += 1\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter\n",
    "    else:\n",
    "        Picture_Structure = {}\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyname, keyname_id = keywords_to_dict()\n",
    "search_key = keyname[0]\n",
    "\n",
    "user_keyword = retry_on_rate_limit_error(client.search_user,'Amphetamine', 1)\n",
    "\n",
    "for user in user_keyword:\n",
    "    tweets = user.get_tweets('tweets')\n",
    "    for tweet in tweets:\n",
    "\n",
    "        media_data = tweet.media\n",
    "        print(media_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyname, keyname_id = keywords_to_dict()\n",
    "search_key = keyname[0]\n",
    "# [0,13] words to search. \n",
    "user_keyword = retry_on_rate_limit_error(client.search_user,'Amphetamine', 5)\n",
    "\n",
    "# all data structures \n",
    "users = {}\n",
    "posts = {}\n",
    "comments = {}\n",
    "pictures = {}\n",
    "keywords_dict = {}\n",
    "relations = {}\n",
    "\n",
    "key_structure_list = []\n",
    "for keys, keynames in keyname.items():\n",
    "    keyname_structure = {\n",
    "        \"keyword\": keynames,\n",
    "        \"keyword_id\": keyname_id[keys],\n",
    "        \"ids\": [],\n",
    "    }\n",
    "    key_structure_list.append(keyname_structure)\n",
    "\n",
    "\n",
    "pic_id_num_starter = 20000000000\n",
    "picture_id_dict = {}\n",
    "for user in user_keyword:\n",
    "\n",
    "    try: \n",
    "        # save the user progile pic \n",
    "        image_directoary = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\profile_images'\n",
    "        image_file_name = 'user_profile_pic_' + str(user.id) + '.jpg'\n",
    "        image_save_path = os.path.join(image_directoary, image_file_name)\n",
    "        download_image(user.profile_image_url, image_save_path)\n",
    "\n",
    "        keywords = []\n",
    "\n",
    "        # keyname contains the keynames and keyname_id contains their ids\n",
    "        keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "        # get the users tweets\n",
    "        tweets = user.get_tweets('tweets')\n",
    "\n",
    "        # gets users followers returns a list of users, return user type \n",
    "        followers = user.get_followers(user.id)\n",
    "\n",
    "        # gets users following returns a list of users, return user type\n",
    "        following = user.get_following(user.id)\n",
    "\n",
    "        \n",
    "        # empty list for the followers\n",
    "        followers_list = []\n",
    "        following_list = []\n",
    "\n",
    "        # add the user or their tweets to keyword_structure if a keyword is found\n",
    "        if 'user_id' + str(user.id) not in key_structure_list[0][\"ids\"]:\n",
    "            for i in range(len(keyname)):\n",
    "                keyword = keyname[i]\n",
    "                if keyword in user.name:\n",
    "                    key_structure_list[i][\"ids\"].append('user_' + str(user.id) )\n",
    "                elif keyword in user.screen_name:\n",
    "                    key_structure_list[i][\"ids\"].append('user_' + str(user.id) )\n",
    "                elif keyword in user.description:\n",
    "                    key_structure_list[i][\"ids\"].append('user_' + str(user.id) )\n",
    "                for tweet in tweets:\n",
    "                    if keyword in tweet.text:\n",
    "                        key_structure_list[i][\"ids\"].append('tweet_' + str(tweet.id) )\n",
    "\n",
    "        # go through the users tweets, description, name to find keywords\n",
    "        counter = 0\n",
    "        for keys, keyname in keyname.items():\n",
    "            if keyname in user.name:\n",
    "                keywords.append(keyname_id[keys])\n",
    "            elif keyname in user.screen_name:\n",
    "                keywords.append(keyname_id[keys])\n",
    "            elif keyname in user.description:\n",
    "                keywords.append(keyname_id[keys])\n",
    "            else:\n",
    "                for tweet in tweets:\n",
    "                    if keyname in tweet.text:\n",
    "                        keywords.append(keyname_id[keys])\n",
    "            # go through the users followers description, tweet, and names to \n",
    "            # see if it finds any keywords. If it does it adds them to the follower_list\n",
    "            followers_list, key_structure_list = keyword_search_in_users(keyname, followers, followers_list, key_structure_list, counter)\n",
    "\n",
    "            # Goes thorugh the users following description, tweet, and names to \n",
    "            # see if it finds any keywords. If it does it adds them to the following_list\n",
    "            following_list, key_structure_list = keyword_search_in_users(keyname, following, following_list, key_structure_list, counter)\n",
    "            counter += 1 \n",
    "\n",
    "        # ensure that if not followers or following where found through the keywords\n",
    "        # to add however many followers are left. \n",
    "        if len(followers_list) != 30:\n",
    "            for follower in followers:\n",
    "                if len(followers_list) == 30:\n",
    "                    break\n",
    "                elif follower not in followers_list:\n",
    "                    followers_list.append('user_' + follower.id)\n",
    "        if len(following_list) != 30:\n",
    "            for follow in following:\n",
    "                if len(following_list) == 30:\n",
    "                    break\n",
    "                elif follow not in following_list:\n",
    "                    following_list.append('user_' + follow.id)\n",
    "\n",
    "        # Structure to store the data\n",
    "        User_Structure = {\n",
    "            \"username\": user.screen_name,\n",
    "            \"user_id\": 'user_' + str(user.id), \n",
    "            \"followers\": followers_list,\n",
    "            \"followees\": following_list, #['user_' + str(followees_id) for followees_id in client.get_friends_ids(user.id,user.screen_name,30)]\n",
    "            \"profile_pic\": image_save_path,\n",
    "            \"profile_text\": user.description,\n",
    "            \"posts\": ['tweet_' + tweet.id for tweet in tweets]  ,\n",
    "            \"keywords\": keywords\n",
    "        }\n",
    "        \n",
    "        # go through each tweet in users tweet \n",
    "        for tweet in tweets:\n",
    "            tweet_id_name = 'tweet_' + str(tweet.id)\n",
    "\n",
    "            # check to see if tweet id not in post_strucutre if not create one for the tweet\n",
    "            if tweet_id_name not in posts:\n",
    "\n",
    "                # extract the post structure form the tweet and comments \n",
    "                original_pic_id_num = pic_id_num_starter\n",
    "                post_structure, picture_structure, pic_id_num_starter = extract_tweet_picture_structure(user, tweet, pic_id_num_starter)\n",
    "                posts[tweet_id_name] = post_structure\n",
    "                pictures['pic_' + str(original_pic_id_num)] = picture_structure\n",
    "        # save the user to the dictionary\n",
    "        user_id_name = 'user_'+ user.id\n",
    "        if user_id_name not in users:\n",
    "            users[user_id_name] = User_Structure\n",
    "        for i in range(len(key_structure_list)):\n",
    "            keyword_id = str(key_structure_list[i][\"keyword_id\"])\n",
    "            keywords_dict[keyword_id] = key_structure_list[i]\n",
    "\n",
    "\n",
    "        # list that contains all data structures \n",
    "        data_structure_for_twitter = [users, posts, comments, pictures, keywords, relations]\n",
    "        save_json(users, save_path_list[0])\n",
    "        save_json(posts, save_path_list[1])\n",
    "        save_json(pictures, save_path_list[3])\n",
    "        save_json(keywords_dict, save_path_list[4])\n",
    "        \n",
    "    except TooManyRequests as e:\n",
    "        print('Rate limit exceeced')\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users)\n",
    "# TODO: create a list with all the drug names, to search for. \n",
    "# TODO: Create the rest of the structures\n",
    "# TODO: Create all of the structures under the same for loop and function. Then create the function to loop over every 15 min.\n",
    "# TDDO: Catch all the images for picture_structure\n",
    "# TODO: find the words that are associated with the user and create and list\n",
    "# TODO: Start to create structure to pull relations.\n",
    "# TODO: Test to see how many unsers to pull, guess 10 time.sleep( about 7.5 - 15 min) in order to not exceed time rates.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
