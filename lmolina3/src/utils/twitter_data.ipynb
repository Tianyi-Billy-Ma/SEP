{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client, TooManyRequests\n",
    "from account_info import USERNAME, EMAIL, PASSWORD\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# client = Client('en-US')\n",
    "# client.login( \n",
    "#     auth_info_1='god_comput74080' ,\n",
    "#     auth_info_2='computerscienceenrichment@outlook.com',\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "\n",
    "client = Client('en-US')\n",
    "\n",
    "client.login( \n",
    "    auth_info_1='TonyJ21128' ,\n",
    "    auth_info_2='notredamelogin@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "client2 = Client('en-US')\n",
    "\n",
    "client2.login( \n",
    "    auth_info_1='turo343994' ,\n",
    "    auth_info_2='summerenrichmentprogramcs@outlook.com',\n",
    "    password='Molina.2005'\n",
    ")\n",
    "\n",
    "# client3 = Client('en-US')\n",
    "\n",
    "# client3.login( \n",
    "#     auth_info_1='tony00551251172' ,\n",
    "#     auth_info_2='tonystarkwon@outlook.com',\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "\n",
    "# save cookies in order to pull data without getting banned\n",
    "# client.save_cookies('cookies.json')\n",
    "client.save_cookies('cookies1.json')\n",
    "client2.save_cookies('cookies2.json')\n",
    "# client3.save_cookies('cookies3.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client5 = Client('en-US')\n",
    "\n",
    "# client5.login( \n",
    "#     auth_info_1='LebronJack27791',\n",
    "#     auth_info_2='project2forser@outlook.com',\n",
    "#     password=PASSWORD\n",
    "# )\n",
    "client4 = Client('en-US')\n",
    "\n",
    "client4.login( \n",
    "    auth_info_1='StarkMiche4868' ,\n",
    "    auth_info_2='project1forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "\n",
    "client6 = Client('en-US')\n",
    "\n",
    "client6.login( \n",
    "    auth_info_1='sanches50767' ,\n",
    "    auth_info_2='project3forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "\n",
    "client7 = Client('en-US')\n",
    "\n",
    "client7.login( \n",
    "    auth_info_1='tonymolnar83096' ,\n",
    "    auth_info_2='project4forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "client8 = Client('en-US')\n",
    "client8.login( \n",
    "    auth_info_1='michealjon77794' ,\n",
    "    auth_info_2='project5forser@outlook.com',\n",
    "    password=PASSWORD\n",
    ")\n",
    "client4.save_cookies('cookies4.json')\n",
    "client5.save_cookies('cookies5.json')\n",
    "client6.save_cookies('cookies6.json')\n",
    "client7.save_cookies('cookies7.json')\n",
    "client8.save_cookies('cookies8.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path to excel sheet\n",
    "file_path_to_excel = r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(URL, save_as):\n",
    "    '''Function uses image URL and saves the image onto the desired path and file type'''\n",
    "    urllib.request.urlretrieve(URL,save_as)\n",
    "\n",
    "\n",
    "def delete_jpg(file):\n",
    "    '''This file takes in the file path and deletes it.'''\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "\n",
    "# this saves the json file on the described directory \n",
    "def save_json(file, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(file, f,indent=4)\n",
    "\n",
    "\n",
    "def open_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "\n",
    "\n",
    "def retry_on_rate_limit_error(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TooManyRequests as e:\n",
    "            print(\"Rate limit exceed trying again in 60 sec\")\n",
    "            time.sleep(60)\n",
    "            retries += 1\n",
    "    raise Exception(\"Max tries reached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'dexies', 1: 'ketshards', 2: 'coketwt', 3: 'breath mint fent ', 4: 'oxy 60s', 5: 'ghb ', 6: '#heroin', 7: 'hyrocodone', 8: 'ket bars', 9: 'lsd pills ', 10: 'kush sale ', 11: 'doseeds', 12: 'mdma pills ', 13: 'mescaline plug', 14: 'meth plug', 15: 'shrooms', 16: 'opium', 17: 'oxycodone', 18: 'PCP', 19: 'm30s', 20: 'Peyote', 21: 'Promethazine with Codeine', 22: 'psychedilics plug', 23: 'pineapple express', 24: 'flakka', 25: 'xanax plug', 26: 'coke', 27: 'mdma', 28: 'pills', 29: 'weed', 30: 'plug', 31: 'sale', 32: 'dmt'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def keywords_to_dict(file_path =r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\" ):\n",
    "    '''This function takes in the excel sheet that has keywords and its ids split up into columns.\n",
    "    Then it creates two dictionaries, keyname and keyname_id. The values are numbered 0 to the \n",
    "    length of the columns. Once created they return these dictionaries.'''\n",
    "\n",
    "    # reads in excel sheet into pandas data frame\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # this turns the street name and its code name into list \n",
    "    street_name_list = df['keyname'].astype(str).values.tolist()\n",
    "    street_name_code_list = df['keyname_id'].astype(str).values.tolist()\n",
    "\n",
    "    # this dictionary contains the keyname in the keys and keyname id in values\n",
    "    keyname = {}\n",
    "    keyname_id = {}\n",
    "    for i in range(33):\n",
    "        keyname[i] = street_name_list[i]\n",
    "        keyname_id[i] = street_name_code_list[i]\n",
    "        \n",
    "    return keyname, keyname_id\n",
    "\n",
    "keyname, keyname_id = keywords_to_dict()\n",
    "print(keyname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def keyword_search_in_users(keyname_to_search, list_of_users, list_to_append, keywords_dict, iteration):\n",
    "    \"\"\"This function takes in a keyname to search a list of users. It will search the users name,\n",
    "    keyname, description and tweets for the keyname. If it finds it on the tweets it appends\n",
    "    to the list_to_append\"\"\"\n",
    "\n",
    "    keynames, keyname_ids = keywords_to_dict()\n",
    "    keyword_id = keyname_ids[iteration]\n",
    "    # gets a user from the list_of_users and goes through their name, screen name \n",
    "    # and description to find keywords. If they find one they append the user to the list]\n",
    "    \n",
    "    for user in list_of_users:\n",
    "        if len(list_to_append) == 20:\n",
    "            break\n",
    "        else: \n",
    "            if keyname_to_search in user.name:\n",
    "                keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                list_to_append.append('user_' +  user.id)\n",
    "\n",
    "            elif keyname_to_search in user.screen_name:\n",
    "                keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                list_to_append.append('user_' + user.id)\n",
    "\n",
    "            elif keyname_to_search in user.description:\n",
    "                keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                list_to_append.append('user_' + user.id)\n",
    "\n",
    "    return list_to_append, keywords_dict\n",
    "\n",
    "\n",
    "def keyword_search_in_tweets(tweet):\n",
    "    \"\"\"This function takes in tweets, keyname, and keyname ids. Then \n",
    "    it goes thourgh the keynames and trys to find it in the text of the \n",
    "    tweet. If it finds it appends it to a list and at the end it returns this\n",
    "    list.\"\"\"\n",
    "\n",
    "    keyname, keyname_id = keywords_to_dict()\n",
    "    keyname_found_in_tweet = []\n",
    "\n",
    "    # find keyword in tweet text, if so append to list\n",
    "    for keys, keyword in keyname.items():\n",
    "        if keyword in tweet.full_text:\n",
    "            keyname_found_in_tweet.append(keyname_id[keys])\n",
    "    \n",
    "    return keyname_found_in_tweet\n",
    "\n",
    "\n",
    "def relations_in_comments(tweet, reply, relations):\n",
    "            \n",
    "            Relation_Structure = {\n",
    "                \"src_id\": 'tweet_' + str(reply.id),\n",
    "                \"relation\": 'comment-under-post',\n",
    "                \"dest_id\": 'tweet_' + str(tweet.id)\n",
    "            }\n",
    "\n",
    "            reply_user = reply.user\n",
    "            relations.append(Relation_Structure)\n",
    "\n",
    "            Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(reply_user.id),\n",
    "                \"relation\": 'user-make-comment',\n",
    "                \"dest_id\": 'tweet_' + str(reply.id)\n",
    "            }\n",
    "\n",
    "            relations.append(Relation_Structure)\n",
    "\n",
    "            keywords, keywords_ids = keywords_to_dict()\n",
    "\n",
    "            for key, keyname in keywords.items():\n",
    "                if keyname in reply.full_text:\n",
    "                    Relation_Structure = {\n",
    "                        \"src_id\": 'tweet_' + str(reply.id),\n",
    "                        \"relation\": 'comment-contain-keyword',\n",
    "                        \"dest_id\": keywords_ids[key]\n",
    "                    }\n",
    "\n",
    "                    relations.append(Relation_Structure)\n",
    "            return relations\n",
    "\n",
    "\n",
    "def remove_duplicates_from_list(list):\n",
    "    \"\"\"This functions removes duplicates from list\"\"\"\n",
    "    result = []\n",
    "    for item in list:\n",
    "        if item not in result:\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "\n",
    "def tweet_mentions_user(user, tweet, relations):\n",
    "    \"\"\"This function is for when a user mentions another user in a tweet. Finds \n",
    "    mentioned users user_name and adds the relation_structure to relations.\"\"\"\n",
    "\n",
    "    mentions = re.findall(r'@(\\S+)', tweet.full_text)\n",
    "    mentions = remove_duplicates_from_list(mentions)\n",
    "    for mention in mentions:\n",
    "        try:\n",
    "            mentioned_user = client.get_user_by_screen_name(mention)\n",
    "            Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(user.id),\n",
    "                \"relation\": 'user-mention/tag-user',\n",
    "                \"dest_id\": 'user_' + str(mentioned_user.id)\n",
    "            }\n",
    "\n",
    "            relations.append(Relation_Structure)\n",
    "        except:\n",
    "            pass\n",
    "    return relations \n",
    "\n",
    "\n",
    "def extract_tweet_picture_structure(user, tweet, pic_id_starter, relations):\n",
    "    \"\"\"This function takes in the type user and tweet. Then goes down the post\n",
    "    structure sorting out the data. Finally it returns the post_structure\"\"\"\n",
    "    \n",
    "    if '@' in tweet.full_text:\n",
    "        relations = tweet_mentions_user(user, tweet, relations)\n",
    "\n",
    "    # retrieve keywords found in tweet\n",
    "    keyname_found = keyword_search_in_tweets(tweet)\n",
    "    # construct post structure\n",
    "    favoriters = tweet.get_favoriters(20)\n",
    "\n",
    "    Relation_Structure = {\n",
    "            \"src_id\": 'user_' + str(user.id),\n",
    "            \"relation\": 'user-publish-post',\n",
    "            \"dest_id\": 'tweet_' + str(tweet.id)\n",
    "        }\n",
    "\n",
    "    relations.append(Relation_Structure)\n",
    "\n",
    "    Post_Structure = {\n",
    "        \"user_id\": 'user_' + str(user.id),\n",
    "        \"post_id\": 'tweet_' + str(tweet.id),\n",
    "        \"user_comment\": tweet.full_text,\n",
    "        \"pic_id\": \"\",\n",
    "        \"liked_users\": [ 'user_' + str(favoriter.id) for favoriter in favoriters],\n",
    "        \"comments\": '',\n",
    "        \"keywords\": keyname_found,\n",
    "    }\n",
    "    if keyname_found:\n",
    "        tweet = client.get_tweet_by_id(tweet.id)\n",
    "        replies = tweet.replies\n",
    "        if replies:\n",
    "            Post_Structure[\"comments\"] = [f'tweet_{reply.id}' for reply in replies]\n",
    "            \n",
    "            for reply in replies:\n",
    "                relations = relations_in_comments(tweet, reply, relations)\n",
    "    elif tweet.replies:\n",
    "        replies = tweet.replies\n",
    "        if replies:\n",
    "            Post_Structure[\"comments\"] = [f'tweet_{reply.id}' for reply in replies]\n",
    "            \n",
    "            for reply in replies:\n",
    "                relations = relations_in_comments(tweet, reply, relations)\n",
    "            \n",
    "    for favoriter in favoriters:\n",
    "        Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(favoriter.id),\n",
    "                \"relation\": 'user-like-post',\n",
    "                \"dest_id\": 'tweet_' + str(tweet.id)\n",
    "            }\n",
    "        \n",
    "        relations.append(Relation_Structure)\n",
    "         \n",
    "    media_data = tweet.media\n",
    "    if media_data:\n",
    "        Post_Structure[\"pic_id\"] = 'pic_' + str(pic_id_starter)\n",
    "\n",
    "        Picture_Structure = {\n",
    "            \"pic_id\": 'pic_' + str(pic_id_starter),\n",
    "            \"post_id\": 'tweet_' + str(tweet.id),\n",
    "            \"url\": media_data[0].get('media_url_https')\n",
    "        }\n",
    "\n",
    "        Relation_Structure = {\n",
    "                \"src_id\": 'tweet_' + str(tweet.id),\n",
    "                \"relation\": 'post-has-picture',\n",
    "                \"dest_id\": 'post_' + str(pic_id_starter)\n",
    "            }\n",
    "        \n",
    "        relations.append(Relation_Structure)\n",
    "        pic_id_starter += 1\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter, relations\n",
    "    \n",
    "    else:\n",
    "        Picture_Structure = {}\n",
    "        return Post_Structure, Picture_Structure, pic_id_starter, relations\n",
    "\n",
    "\n",
    "def get_relations_followers_following(relations, followers_list, user):\n",
    "    '''This function gets the realtions between a user and it's\n",
    "    following/follower list'''\n",
    "\n",
    "    user_id = user.id\n",
    "    for follower in followers_list:\n",
    "        Relation_Structure = {\n",
    "                \"src_id\": follower,\n",
    "                \"relation\": 'user-follow/followed-user',\n",
    "                \"dest_id\": 'user_' + str(user_id)\n",
    "            }\n",
    "                \n",
    "        relations.append(Relation_Structure)\n",
    "    return relations\n",
    "    \n",
    "\n",
    "def realtion_in_profile(user, keyword_id, relations):\n",
    "    '''Finds the relations profile of users. If a keyword is found \n",
    "    in the profile it creates a relation structure for it.'''\n",
    "    Relation_Structure = {\n",
    "                \"src_id\": 'user_' + str(user.id),\n",
    "                \"relation\": 'user-profile-keyword',\n",
    "                \"dest_id\": keyword_id\n",
    "            }\n",
    "    relations.append(Relation_Structure)\n",
    "    return relations\n",
    "\n",
    "\n",
    "def realtion_keyword_in_tweet(tweet, keyword_id, relations):\n",
    "    '''Adds relation structure between post and keyword'''\n",
    "    Relation_Structure = {\n",
    "                \"src_id\": 'tweet_' + str(tweet.id),\n",
    "                \"relation\": 'post-include-keyword',\n",
    "                \"dest_id\": keyword_id,\n",
    "            }\n",
    "    relations.append(Relation_Structure)\n",
    "    return relations\n",
    "\n",
    "\n",
    "def add_followers_or_following(followers_list, followers):\n",
    "    if len(followers_list) != 10:\n",
    "        for follower in followers:\n",
    "            if len(followers_list) == 10:\n",
    "                break\n",
    "            elif follower not in followers_list:\n",
    "                followers_list.append('user_' + follower.id)\n",
    "    return followers_list\n",
    "\n",
    "def user_get_followers(user):\n",
    "    followers = user.get_followers(10)\n",
    "    return followers\n",
    "\n",
    "\n",
    "def user_get_following(user):\n",
    "    following = user.get_following(10)\n",
    "    return following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_user_following(user):\n",
    "    try:\n",
    "        followers = user.get_followers(10)\n",
    "        following = user.get_following(10)\n",
    "    except:\n",
    "        followers = []\n",
    "        following = []\n",
    "    if len(followers) == 0:\n",
    "        try:\n",
    "            followers = client.get_user_followers(user.id,10)\n",
    "            following = client.get_user_following(user.id,10)\n",
    "        except:\n",
    "            followers = []\n",
    "            following = []\n",
    "    if len(followers) == 0:\n",
    "        try:\n",
    "            followers = user_get_followers(user)\n",
    "            following = user_get_following(user)\n",
    "        except:\n",
    "            followers = []\n",
    "            following = []\n",
    "    return followers, following\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        contents = file.read()\n",
    "    return contents \n",
    "\n",
    "\n",
    "def write_txt_file(file_path, new_contents):\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(new_contents)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_client_for_users(counter):\n",
    "    if counter > 4:\n",
    "        counter = 1\n",
    "    if counter == 1:\n",
    "        client.load_cookies('cookies7.json')\n",
    "    elif counter == 2:\n",
    "        client.load_cookies('cookies8.json')\n",
    "    elif counter == 3:\n",
    "        client.load_cookies('cookies7.json')\n",
    "    elif counter == 4:\n",
    "        client.load_cookies('cookies8.json')\n",
    "\n",
    "    return counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_structures_for_twitter(user_keyword, pic_id_num_starter, save_path_list):\n",
    "\n",
    "    account_num = 1\n",
    "    time_counter = 1\n",
    "    counter_client= 1\n",
    "    for i, user in enumerate(user_keyword):\n",
    "        pic_id_num_starter = int(read_txt_file(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\picture_id_num.txt'))\n",
    "        users = open_json(save_path_list[0])\n",
    "        posts = open_json(save_path_list[1])\n",
    "        pictures = open_json(save_path_list[2])\n",
    "        keywords_dict = open_json(save_path_list[3])\n",
    "        relations = open_json(save_path_list[4])\n",
    "        counter_client = change_client_for_users(counter_client)\n",
    "\n",
    "        if f'user_{user.id}' in users:\n",
    "            account_num += 1\n",
    "            continue\n",
    "        print(f'entering {account_num}, {user.screen_name}')\n",
    "        try: \n",
    "\n",
    "            keywords = []\n",
    "\n",
    "            # keyname contains the keynames and keyname_id contains their ids\n",
    "            keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "            # get the users tweets\n",
    "            tweets = user.get_tweets('tweets',10)\n",
    "\n",
    "            followers, following = try_get_user_following(user)\n",
    "            \n",
    "            \n",
    "            followers_list = []\n",
    "            following_list = []\n",
    "            found_in_user_description = False\n",
    "            # add the user or their tweets to keyword_structure if a keyword is found\n",
    "            for i in range(len(keyname)):\n",
    "                keyword = keyname[i]\n",
    "                keyword_id = keyname_id[i]\n",
    "                if keyword in user.name:\n",
    "                    keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                    relations = realtion_in_profile(user, keyword_id, relations)\n",
    "                    keywords.append(keyword_id)\n",
    "                    found_in_user_description = True\n",
    "                elif keyword in user.screen_name:\n",
    "                    keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                    relations = realtion_in_profile(user, keyword_id, relations)\n",
    "                    keywords.append(keyword_id)\n",
    "                    found_in_user_description = True\n",
    "                elif keyword in user.description:\n",
    "                    keywords_dict[keyword_id][\"ids\"].append('user_'+ str(user.id))\n",
    "                    relations = realtion_in_profile(user, keyword_id, relations)\n",
    "                    keywords.append(keyword_id)\n",
    "                    found_in_user_description = True\n",
    "                for tweet in tweets:\n",
    "                    if keyword in tweet.full_text:\n",
    "                        keywords_dict[keyword_id][\"ids\"].append('tweet_'+ str(tweet.id))\n",
    "                        relations = realtion_keyword_in_tweet(tweet, keyword_id, relations)\n",
    "                        if found_in_user_description:\n",
    "                            pass\n",
    "                        else:\n",
    "                            keywords.append(keyword_id)\n",
    "                            found_in_user_description = True\n",
    "            if followers:\n",
    "                # go through the users tweets, description, name to find keywords\n",
    "                counter = 0\n",
    "                for keys, keyname in keyname.items():\n",
    "                    # go through the users followers description, tweet, and names to \n",
    "                    # see if it finds any keywords. If it does it adds them to the follower_list\n",
    "                    followers_list, keywords_dict = keyword_search_in_users(keyname, followers, followers_list, keywords_dict, counter)\n",
    "\n",
    "                    # Goes thorugh the users following description, tweet, and names to \n",
    "                    # see if it finds any keywords. If it does it adds them to the following_list\n",
    "                    following_list, keywords_dict= keyword_search_in_users(keyname, following, following_list, keywords_dict, counter)\n",
    "                    counter += 1 \n",
    "\n",
    "                # ensure that if not followers or following where found through the keywords\n",
    "                # to add however many followers are left. \n",
    "                followers_list = add_followers_or_following(followers_list, followers)\n",
    "                following_list = add_followers_or_following(following_list, following)\n",
    "\n",
    "                relations = get_relations_followers_following(relations, followers_list, user)\n",
    "                relations = get_relations_followers_following(relations,following_list, user)\n",
    "\n",
    "            # Structure to store the data\n",
    "            User_Structure = {\n",
    "                \"username\": user.screen_name,\n",
    "                \"user_id\": 'user_' + str(user.id), \n",
    "                \"followers\": followers_list,\n",
    "                \"followees\": following_list, #['user_' + str(followees_id) for followees_id in client.get_friends_ids(user.id,user.screen_name,30)]\n",
    "                \"profile_pic\": user.profile_image_url,\n",
    "                \"profile_text\": user.description,\n",
    "                \"posts\": ['tweet_' + tweet.id for tweet in tweets]  ,\n",
    "                \"keywords\": keywords\n",
    "            }\n",
    "            \n",
    "            # go through each tweet in users tweet \n",
    "            for tweet in tweets:\n",
    "                tweet_id_name = 'tweet_' + str(tweet.id)\n",
    "\n",
    "                # check to see if tweet id not in post_strucutre if not create one for the tweet\n",
    "                if tweet_id_name not in posts:\n",
    "\n",
    "                    # extract the post structure form the tweet and comments \n",
    "                    original_pic_id_num = pic_id_num_starter\n",
    "                    post_structure, picture_structure, pic_id_num_starter, relations = extract_tweet_picture_structure(user, tweet, pic_id_num_starter, relations)\n",
    "                    posts[tweet_id_name] = post_structure\n",
    "                    if picture_structure:\n",
    "                        pictures['pic_' + str(original_pic_id_num)] = picture_structure\n",
    "\n",
    "                replies_in_post = post_structure['comments']\n",
    "                replies_in_post = [reply.replace('tweet_','') for reply in replies_in_post]\n",
    "                if replies_in_post:\n",
    "\n",
    "                    for reply in replies_in_post:\n",
    "                        reply = client.get_tweet_by_id(reply)\n",
    "                        reply_id_name = 'tweet_' +str(reply.id)\n",
    "\n",
    "                        if reply_id_name not in posts:\n",
    "                            original_pic_id_num = pic_id_num_starter\n",
    "                            post_structure, picture_structure, pic_id_num_starter, relations = extract_tweet_picture_structure(reply.user, reply, pic_id_num_starter, relations)\n",
    "                            posts[reply_id_name] = post_structure\n",
    "                            \n",
    "                            if picture_structure:\n",
    "                                pictures['pic_' + str(original_pic_id_num)] = picture_structure\n",
    "\n",
    "                \n",
    "            # save the user to the dictionary\n",
    "            user_id_name = 'user_'+ user.id\n",
    "            users[user_id_name] = User_Structure\n",
    "            write_txt_file(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\picture_id_num.txt', str(pic_id_num_starter))\n",
    "\n",
    "            # list that contains all data structures \n",
    "            # data_structure_for_twitter = [users, posts, pictures, keywords, relations]\n",
    "            save_json(users, save_path_list[0])\n",
    "            save_json(posts, save_path_list[1])\n",
    "            save_json(pictures, save_path_list[2])\n",
    "            save_json(keywords_dict, save_path_list[3])\n",
    "            save_json(relations, save_path_list[4])\n",
    "            print(f'success with {account_num}')\n",
    "\n",
    "            account_num += 1 \n",
    "            # print('30 sec')\n",
    "            # time.sleep(20)\n",
    "            # print('continue')\n",
    "            counter_client += 1\n",
    "        except TooManyRequests as e:\n",
    "            print('Rate limit exceeced')\n",
    "            time_to_wait = 60 * time_counter\n",
    "            time_counter += 1\n",
    "            print(f\"Time to wait {time_to_wait}\")\n",
    "            time.sleep(time_to_wait)\n",
    "            if time_counter == 5:\n",
    "                print('exceeded time rate limits')\n",
    "                break\n",
    "            counter_client += 1\n",
    "            continue \n",
    "\n",
    "\n",
    "    return pic_id_num_starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_structures_to_path(save_path_list):\n",
    "    keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "\n",
    "    # all data structures \n",
    "    users = {}\n",
    "    posts = {}\n",
    "    pictures = {}\n",
    "    keywords_dict = {}\n",
    "    relations = []\n",
    "\n",
    "    for keys, keynames in keyname.items():\n",
    "        keyname_structure = {\n",
    "            \"keyword\": keynames,\n",
    "            \"keyword_id\": keyname_id[keys],\n",
    "            \"ids\": [],\n",
    "        }\n",
    "        keywords_dict[keyname_id[keys]] = keyname_structure\n",
    "\n",
    "    data_structures_to_save = [users ,posts, pictures, keywords_dict, relations]\n",
    "\n",
    "    for i in range(len(save_path_list)):\n",
    "        save_json(data_structures_to_save[i], save_path_list[i])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_client(counter):\n",
    "    if counter == 5:\n",
    "        counter = 0\n",
    "    if counter == 1:\n",
    "        client.load_cookies('cookies4.json')\n",
    "    elif counter == 2:\n",
    "        client.load_cookies('cookies1.json')\n",
    "    elif counter == 3:\n",
    "        client.load_cookies('cookies2.json')\n",
    "    elif counter == 4:\n",
    "        client.load_cookies('cookies1.json')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_user_by_keyword(keyword):\n",
    "    searched_tweets = client.search_tweet(keyword)\n",
    "    user_list = []\n",
    "    for tweet in searched_tweets:\n",
    "        user = tweet.user\n",
    "        if user in user_list:\n",
    "            continue\n",
    "        try: \n",
    "            user_found = client.get_user_by_id(user.id)\n",
    "            user_list.append(user_found)\n",
    "        except:\n",
    "            print('rate limits reached sleep for 2 min')\n",
    "            time.sleep(120)\n",
    "    return user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_list = []\n",
    "keywords, keywords_ids = keywords_to_dict()\n",
    "# all the file paths for all the structures in the end \n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure__2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre__2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure__2nd_round.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure__2nd_round.json')\n",
    "\n",
    "\n",
    "user_struct = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure__1st round.json')\n",
    "# save_data_structures_to_path(save_path_list)\n",
    "# keyname_list = [keywords[key] for key in range(13)]\n",
    "\n",
    "counter = 1\n",
    "for struct in user_struct.values():\n",
    "    current_struct_work = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_2nd_round.json')\n",
    "\n",
    "    follower_following_list = []\n",
    "    change_client(counter)\n",
    "    try:\n",
    "        follower_following_list.extend( [client.get_user_by_id(follower.replace('user_', '')) for count, follower in enumerate(struct['followers']) if count < 5])\n",
    "        follower_following_list.extend([client.get_user_by_id(followee.replace('user_','')) for count, followee in enumerate(struct['followees']) if count < 5])\n",
    "    except TooManyRequests:\n",
    "        print('sleep for followers ')\n",
    "        time.sleep(120)\n",
    "        print('exit')\n",
    "    # user_list = retry_on_rate_limit_error(client.search_user, keyname, 20)\n",
    "    \n",
    "    pic_id_num_starter = read_txt_file('picture_id_num.txt')\n",
    "\n",
    "    pic_id_num_starter = creating_structures_for_twitter(follower_following_list, int(pic_id_num_starter), save_path_list)\n",
    "    write_txt_file('picture_id_num.txt', str(pic_id_num_starter))\n",
    "    print('')\n",
    "    print('sleep')\n",
    "    time.sleep(60)\n",
    "    print('exit')\n",
    "    counter += 1\n",
    "    if len(current_struct_work) > 150:\n",
    "        print('limits reached')\n",
    "# pic_id_num_starter = 20000000\n",
    "\n",
    "\n",
    "# [0,13] words to search. \n",
    "# list = client.search_user('Amphetamine', 20)\n",
    "# list_20 = list.next()\n",
    "# list_40 = list_20.next()\n",
    "# list_60 = list_40.next()\n",
    "# list_80 = list_60.next()\n",
    "\n",
    "# picture_id_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coketwt\n",
      "entering 1, TDispensarry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 1\n",
      "entering 2, ZoneTrippy\n",
      "success with 2\n",
      "entering 3, Jumblemanideals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 3\n",
      "entering 5, max21heal\n",
      "success with 5\n",
      "\n",
      "breath mint fent \n",
      "entering 1, Snack_Memories\n",
      "success with 1\n",
      "entering 2, HIGH_TIMES_Mag\n",
      "success with 2\n",
      "entering 3, didysemata\n",
      "success with 3\n",
      "entering 4, ThreeEyedLamb\n",
      "success with 4\n",
      "entering 5, MsAshBash420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 5\n",
      "entering 6, Modanya\n",
      "success with 6\n",
      "entering 7, cuntyrak\n",
      "success with 7\n",
      "entering 8, knivessama\n",
      "success with 8\n",
      "entering 9, MiddleageM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 9\n",
      "entering 10, AndyAitcheson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 10\n",
      "\n",
      "oxy 60s\n",
      "entering 1, clinton_ebako\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 1\n",
      "entering 2, jg_trading\n",
      "success with 2\n",
      "entering 3, DayDreamingFUND\n",
      "success with 3\n",
      "entering 4, OxyMLZ\n",
      "success with 4\n",
      "entering 5, k41_ni\n",
      "success with 5\n",
      "entering 6, Dannytrz\n",
      "success with 6\n",
      "entering 7, jacobs_cellars\n",
      "success with 7\n",
      "entering 8, sar_eldar\n",
      "success with 8\n",
      "entering 9, xtyfo\n",
      "success with 9\n",
      "entering 10, curator_oxalis\n",
      "success with 10\n",
      "entering 11, linesonchart\n",
      "success with 11\n",
      "entering 12, JamesRuport\n",
      "success with 12\n",
      "entering 15, 9Pharmas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 15\n",
      "\n",
      "ghb \n",
      "entering 1, EmilyHerna75874\n",
      "success with 1\n",
      "entering 2, DebbieS90236850\n",
      "success with 2\n",
      "entering 3, B03GHB4L1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 3\n",
      "entering 4, monica_coo42989\n",
      "success with 4\n",
      "entering 5, MichelleDa8999\n",
      "success with 5\n",
      "entering 6, JForselles45247\n",
      "success with 6\n",
      "entering 7, SoniaRodri60408\n",
      "success with 7\n",
      "entering 9, AprilSimps61991\n",
      "success with 9\n",
      "entering 10, lavergne84821\n",
      "success with 10\n",
      "entering 12, SalazarCan80428\n",
      "success with 12\n",
      "entering 14, sarah_davi87945\n",
      "success with 14\n",
      "\n",
      "#heroin\n",
      "entering 1, somas_somu98324\n",
      "success with 1\n",
      "entering 2, BrianGPowell\n",
      "success with 2\n",
      "entering 3, GolaghatPolice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 3\n",
      "entering 4, FritzMigue39801\n",
      "success with 4\n",
      "entering 5, ifiware\n",
      "success with 5\n",
      "entering 6, officerfart\n",
      "success with 6\n",
      "entering 7, onemevinod\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 7\n",
      "entering 8, heypstump\n",
      "success with 8\n",
      "entering 9, sandra_bharati\n",
      "success with 9\n",
      "entering 10, BSF_Rajasthan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 10\n",
      "\n",
      "hyrocodone\n",
      "entering 1, 662x3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 1\n",
      "entering 2, RobertPearreOne\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 2\n",
      "entering 3, AIC_of_Spades\n",
      "success with 3\n",
      "entering 4, barocko\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 4\n",
      "entering 5, MetalFaceVal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 5\n",
      "entering 6, SmartassChef\n",
      "success with 6\n",
      "entering 8, B_A_Con\n",
      "success with 8\n",
      "entering 9, pray4bricks\n",
      "success with 9\n",
      "entering 10, _itsbiru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 10\n",
      "entering 11, DicusJoe\n",
      "success with 11\n",
      "entering 12, lhourigan5\n",
      "success with 12\n",
      "entering 13, looloolikes\n",
      "success with 13\n",
      "entering 14, ARCAMMON\n",
      "Rate limit exceeced\n",
      "Time to wait 60\n",
      "entering 14, CooperW30674474\n",
      "success with 14\n",
      "\n",
      "ket bars\n",
      "entering 1, ket4m1ne7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limit exceeced\n",
      "Time to wait 60\n",
      "entering 1, _a_kitty_ket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 1\n",
      "entering 2, TomBombPodcast\n",
      "Rate limit exceeced\n",
      "Time to wait 120\n",
      "entering 2, girlmeat5557\n",
      "success with 2\n",
      "entering 3, kylelewjah\n",
      "Rate limit exceeced\n",
      "Time to wait 180\n",
      "entering 3, furblee\n",
      "success with 3\n",
      "entering 4, BamFromDatSouf\n",
      "success with 4\n",
      "entering 5, renskcal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 5\n",
      "entering 6, Mars_bars_back\n",
      "success with 6\n",
      "entering 7, sirkolakola\n",
      "success with 7\n",
      "entering 9, crackajack_00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 9\n",
      "entering 10, nalocatk\n",
      "success with 10\n",
      "\n",
      "lsd pills \n",
      "entering 3, KenTerry12849\n",
      "success with 3\n",
      "\n",
      "kush sale \n",
      "entering 1, Akame424\n",
      "success with 1\n",
      "entering 2, whatever_weed\n",
      "success with 2\n",
      "entering 3, l9krona\n",
      "success with 3\n",
      "entering 4, DirtyBirdSeeds\n",
      "success with 4\n",
      "entering 5, Terpfi3nd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 5\n",
      "entering 6, kush07_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 6\n",
      "entering 7, DavdTheRockstar\n",
      "success with 7\n",
      "entering 9, Florecita_kush\n",
      "success with 9\n",
      "entering 10, barbi77239259\n",
      "success with 10\n",
      "entering 11, Prince87393650\n",
      "success with 11\n",
      "\n",
      "doseeds\n",
      "entering 1, cana_kush\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 1\n",
      "entering 2, opbtt1\n",
      "success with 2\n",
      "\n",
      "mdma pills \n",
      "entering 1, edmontonjournal\n",
      "success with 1\n",
      "entering 2, MaxwellSny7168\n",
      "success with 2\n",
      "entering 4, lolacantola\n",
      "success with 4\n",
      "entering 5, WeAreTheLoopUK\n",
      "success with 5\n",
      "\n",
      "mescaline plug\n",
      "entering 1, Fam4769317\n",
      "success with 1\n",
      "entering 2, MainPlug49863\n",
      "success with 2\n",
      "entering 3, Ann1347825\n",
      "success with 3\n",
      "entering 4, get_ecstasy\n",
      "success with 4\n",
      "entering 8, kifmanGunz\n",
      "success with 8\n",
      "entering 9, trippyzone22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 9\n",
      "entering 10, psychtrip4\n",
      "success with 10\n",
      "\n",
      "meth plug\n",
      "entering 1, KINGHOPP_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 1\n",
      "entering 2, PlugPulse\n",
      "success with 2\n",
      "entering 3, drdave1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 3\n",
      "entering 4, NicoleEuph56326\n",
      "success with 4\n",
      "\n",
      "shrooms\n",
      "entering 1, welldonebiscuit\n",
      "success with 1\n",
      "entering 2, riverrsxn\n",
      "success with 2\n",
      "entering 3, ZDRAGON396\n",
      "success with 3\n",
      "entering 4, Karlara95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 4\n",
      "entering 5, cindysbush\n",
      "success with 5\n",
      "entering 6, MediaSensei3rd\n",
      "success with 6\n",
      "entering 7, belsbean\n",
      "success with 7\n",
      "entering 8, TGODxHIPPIE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 8\n",
      "entering 9, leenicoletx\n",
      "success with 9\n",
      "entering 10, AmosFaola\n",
      "success with 10\n",
      "entering 14, than_truth\n",
      "success with 14\n",
      "entering 15, fazblastfrights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 15\n",
      "\n",
      "opium\n",
      "entering 1, sadrxckstar\n",
      "Rate limit exceeced\n",
      "Time to wait 60\n",
      "entering 1, opium_39\n",
      "success with 1\n",
      "entering 2, OpiumLATAM\n",
      "Rate limit exceeced\n",
      "Time to wait 120\n",
      "entering 3, wendysbbqsauce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 3\n",
      "entering 5, OPIUM2900\n",
      "Rate limit exceeced\n",
      "Time to wait 180\n",
      "entering 5, PopoolaAyomiku2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 5\n",
      "entering 7, PatMichaelMulle\n",
      "Rate limit exceeced\n",
      "Time to wait 240\n",
      "exceeded time rate limits\n",
      "\n",
      "oxycodone\n",
      "entering 1, Krisiki420\n",
      "success with 1\n",
      "entering 2, BlackySpeakz\n",
      "success with 2\n",
      "entering 3, BabyOxycodone\n",
      "success with 3\n",
      "entering 4, Danbowaga\n",
      "success with 4\n",
      "entering 5, Psychomedicine3\n",
      "success with 5\n",
      "entering 7, Lawso2717Bruno\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 7\n",
      "entering 8, CallMeOxycodone\n",
      "success with 8\n",
      "entering 9, CorsicaCaz\n",
      "success with 9\n",
      "entering 11, middlea75746336\n",
      "success with 11\n",
      "entering 12, pigshitsonballs\n",
      "success with 12\n",
      "entering 13, DrSarahEaton\n",
      "success with 13\n",
      "entering 14, lovenature_unit\n",
      "success with 14\n",
      "\n",
      "PCP\n",
      "entering 1, RRcampo1\n",
      "success with 1\n",
      "entering 2, vocaltest\n",
      "success with 2\n",
      "entering 3, val_1010pcp\n",
      "success with 3\n",
      "entering 4, Peritectic26630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 4\n",
      "entering 6, PCPFoxes\n",
      "success with 6\n",
      "entering 7, condjocond\n",
      "success with 7\n",
      "entering 10, josefern77\n",
      "success with 10\n",
      "entering 11, XLRCSB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 11\n",
      "entering 12, Renato_Seara\n",
      "success with 12\n",
      "entering 13, MeiaLua22\n",
      "success with 13\n",
      "entering 14, M00nChildAna\n",
      "success with 14\n",
      "entering 15, Carlos_Guedes\n",
      "success with 15\n",
      "entering 17, JMcNutty23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 17\n",
      "\n",
      "m30s\n",
      "entering 1, RightWingCope\n",
      "success with 1\n",
      "entering 2, munique172\n",
      "success with 2\n",
      "entering 3, Waif2000\n",
      "success with 3\n",
      "entering 5, Growerparadise1\n",
      "success with 5\n",
      "entering 6, WeedHouston\n",
      "success with 6\n",
      "entering 7, warnympkh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 7\n",
      "entering 8, ShroomsStore0\n",
      "success with 8\n",
      "entering 9, freakyzone499\n",
      "success with 9\n",
      "entering 10, carts_thc_vape\n",
      "success with 10\n",
      "\n",
      "Peyote\n",
      "entering 1, seemseema1\n",
      "success with 1\n",
      "entering 2, m3sca1\n",
      "success with 2\n",
      "entering 3, GiuIianoDMedici\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 3\n",
      "entering 4, peyote_18\n",
      "success with 4\n",
      "entering 5, janwe\n",
      "success with 5\n",
      "entering 6, IY3WNdbxph4mSEu\n",
      "success with 6\n",
      "entering 7, cannabishub\n",
      "success with 7\n",
      "entering 8, Peyoteotaku\n",
      "success with 8\n",
      "entering 9, lopezobrador_\n",
      "success with 9\n",
      "entering 11, PeyotePyro\n",
      "success with 11\n",
      "entering 12, Peyotimus\n",
      "success with 12\n",
      "entering 15, Carlosa98CARLOS\n",
      "success with 15\n",
      "\n",
      "Promethazine with Codeine\n",
      "entering 1, Psilocybin29142\n",
      "success with 1\n",
      "entering 2, TallgeeseW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:3110: UserWarning: Some followers are excluded because \"Quality Filter\" is enabled. To get all followers, turn off it in the Twitter settings.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success with 2\n",
      "entering 3, Shez_1Of1\n",
      "Rate limit exceeced\n",
      "Time to wait 60\n",
      "entering 3, DallowMend53121\n",
      "success with 3\n",
      "entering 4, chiweethedog\n",
      "Rate limit exceeced\n",
      "Time to wait 120\n",
      "entering 4, Drmichael_clark\n",
      "success with 4\n",
      "entering 5, Codeine_Rx\n"
     ]
    },
    {
     "ename": "Unauthorized",
     "evalue": "status: 401, message: \"{\"errors\":[{\"message\":\"Could not authenticate you\",\"code\":32}]}\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# user_list = retry_on_rate_limit_error(client.search_user, keyname, 20)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m pic_id_num_starter \u001b[38;5;241m=\u001b[39m read_txt_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpicture_id_num.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m pic_id_num_starter \u001b[38;5;241m=\u001b[39m \u001b[43mcreating_structures_for_twitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic_id_num_starter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m write_txt_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpicture_id_num.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(pic_id_num_starter))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m, in \u001b[0;36mcreating_structures_for_twitter\u001b[1;34m(user_keyword, pic_id_num_starter, save_path_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m keyname, keyname_id \u001b[38;5;241m=\u001b[39m keywords_to_dict()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# get the users tweets\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43muser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m followers, following \u001b[38;5;241m=\u001b[39m try_get_user_following(user)\n\u001b[0;32m     32\u001b[0m followers_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\user.py:170\u001b[0m, in \u001b[0;36mUser.get_tweets\u001b[1;34m(self, tweet_type, count)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_tweets\u001b[39m(\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    133\u001b[0m     tweet_type: Literal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReplies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedia\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLikes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    134\u001b[0m     count: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m    135\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result[Tweet]:\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    Retrieves the user's tweets.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m    ...\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_user_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:2065\u001b[0m, in \u001b[0;36mClient.get_user_tweets\u001b[1;34m(self, user_id, tweet_type, count, cursor)\u001b[0m\n\u001b[0;32m   2054\u001b[0m params \u001b[38;5;241m=\u001b[39m flatten_params({\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables,\n\u001b[0;32m   2056\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: FEATURES\n\u001b[0;32m   2057\u001b[0m })\n\u001b[0;32m   2058\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTweets\u001b[39m\u001b[38;5;124m'\u001b[39m: Endpoint\u001b[38;5;241m.\u001b[39mUSER_TWEETS,\n\u001b[0;32m   2060\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReplies\u001b[39m\u001b[38;5;124m'\u001b[39m: Endpoint\u001b[38;5;241m.\u001b[39mUSER_TWEETS_AND_REPLIES,\n\u001b[0;32m   2061\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedia\u001b[39m\u001b[38;5;124m'\u001b[39m: Endpoint\u001b[38;5;241m.\u001b[39mUSER_MEDIA,\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLikes\u001b[39m\u001b[38;5;124m'\u001b[39m: Endpoint\u001b[38;5;241m.\u001b[39mUSER_LIKES,\n\u001b[0;32m   2063\u001b[0m }[tweet_type]\n\u001b[1;32m-> 2065\u001b[0m response, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_headers\u001b[49m\n\u001b[0;32m   2069\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2071\u001b[0m instructions_ \u001b[38;5;241m=\u001b[39m find_dict(response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instructions_:\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:167\u001b[0m, in \u001b[0;36mBaseClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m Any, httpx\u001b[38;5;241m.\u001b[39mResponse]:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\Lib\\site-packages\\twikit\\client.py:148\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, url, auto_unlock, raise_exception, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequest(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m status_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n",
      "\u001b[1;31mUnauthorized\u001b[0m: status: 401, message: \"{\"errors\":[{\"message\":\"Could not authenticate you\",\"code\":32}]}\n\""
     ]
    }
   ],
   "source": [
    "save_path_list = []\n",
    "keywords, keywords_ids = keywords_to_dict()\n",
    "# all the file paths for all the structures in the end \n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\user_structure_1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\post_structure__1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\picture_strucutre__1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\keyword_structure__1st_round_new_drugs.json')\n",
    "save_path_list.append(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\relation_structure__1st_round_new_drugs.json')\n",
    "\n",
    "\n",
    "\n",
    "# user_struct = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_1st_round_new_drugs.json')\n",
    "\n",
    "# save_data_structures_to_path(save_path_list)\n",
    "keyname_list = [keywords[key] for key in range(2, 26)]\n",
    "\n",
    "\n",
    "counter = 1\n",
    "for keyname in keyname_list:\n",
    "    print(keyname)\n",
    "    change_client(counter)\n",
    "    tweet1 = client.search_tweet(keyname,'Latest',10)\n",
    "    client_tweets = client.search_tweet(keyname,'Top', 10)\n",
    "    tweet1 = [twt for twt in tweet1]\n",
    "    client_tweets = [twt for twt in client_tweets]\n",
    "    client_tweets.extend(tweet1)\n",
    "    user_list = []\n",
    "    for tweet in client_tweets:\n",
    "        user = tweet.user\n",
    "        user_list.append(user)\n",
    "    # user_list = retry_on_rate_limit_error(client.search_user, keyname, 20)\n",
    "    pic_id_num_starter = read_txt_file('picture_id_num.txt')\n",
    "    pic_id_num_starter = creating_structures_for_twitter(user_list, int(pic_id_num_starter), save_path_list)\n",
    "    write_txt_file('picture_id_num.txt', str(pic_id_num_starter))\n",
    "    print('')\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counter = 0 \n",
    "# for struct in user_struct.values():\n",
    "#     current_struct = open_json(r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_2nd_round_new_drugs.json')\n",
    "\n",
    "#     print(keywords)\n",
    "#     follower_following_list = []\n",
    "#     change_client(counter)\n",
    "#     try:\n",
    "#         follower_following_list.extend([client.get_user_by_id(follower.replace('user_', '')) for count, follower in enumerate(struct['followers']) if count <=10])\n",
    "#         follower_following_list.extend([client.get_user_by_id(followee.replace('user_','')) for count, followee in enumerate(struct['followees']) if count <=10])\n",
    "#     except TooManyRequests:\n",
    "#         print('sleep')\n",
    "#         time.sleep(120)\n",
    "#         print('exit')\n",
    "\n",
    "    # pic_id_num_starter = read_txt_file('picture_id_num.txt')\n",
    "    # tweet1 = client.search_tweet(keywords,'Latest',20)\n",
    "    # client_tweets = client.search_tweet(keywords,'Top', 20)\n",
    "    # tweet1 = [twt for twt in tweet1]\n",
    "    # client_tweets = [twt for twt in client_tweets]\n",
    "    # client_tweets.extend(tweet1)\n",
    "    # user_list = []\n",
    "    # for tweet in client_tweets:\n",
    "    #     user = tweet.user\n",
    "    #     user_list.append(user)\n",
    "        \n",
    "\n",
    "    # pic_id_num_starter = creating_structures_for_twitter(follower_following_list, int(pic_id_num_starter), save_path_list)\n",
    "    # write_txt_file('picture_id_num.txt', str(pic_id_num_starter))\n",
    "    # print('sleep')\n",
    "    # time.sleep(60)\n",
    "    # print('')\n",
    "    # counter +=1\n",
    "    # if len(current_struct) > 150:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\user_structure_1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\user_structure_1st_round_new_drugs.json']\n",
    "\n",
    "post_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\post_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\post_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\post_structure__1st_round_new_drugs.json']\n",
    "                            \n",
    "picture_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\picture_strucutre__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\picture_strucutre__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\picture_strucutre__1st_round_new_drugs.json']\n",
    "\n",
    "relation_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure__1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\relation_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\relation_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\relation_structure__1st_round_new_drugs.json']\n",
    "\n",
    "keyword_struct_1st_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure_1st round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\keyword_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\keyword_structure__1st_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_4th_pull\\keyword_structure__1st_round_new_drugs.json']\n",
    "\n",
    "all_paths = [user_struct_1st_round, post_struct_1st_round, picture_struct_1st_round, relation_struct_1st_round, keyword_struct_1st_round]\n",
    "for i,path_list in enumerate(all_paths):\n",
    "    for path in path_list:\n",
    "        if os.path.exists(path):\n",
    "            pass\n",
    "        else:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_struct_2nd_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\user_structure_2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\user_structure_2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\user_structure_2nd_round_new_drugs.json']\n",
    "\n",
    "post_struct_2nd_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\post_structure__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\post_structure__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\post_structure__2nd_round_new_drugs.json']\n",
    "\n",
    "picture_struct_2nd_round =  [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\picture_strucutre__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\picture_strucutre__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\picture_strucutre__2nd_round_new_drugs.json']\n",
    "\n",
    "relation_struct_2nd_round =  [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\relation_structure__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\relation_structure__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\relation_structure__2nd_round_new_drugs.json']\n",
    "\n",
    "keyword_struct_2nd_round = [r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures\\keyword_structure__2nd_round.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_2nd_pull\\keyword_structure__2nd_round_new_drugs.json',\n",
    "                         r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\data_structures_3rd_pull\\keyword_structure__2nd_round_new_drugs.json']\n",
    "\n",
    "all_2nd_paths = [user_struct_2nd_round, post_struct_2nd_round, picture_struct_2nd_round, relation_struct_2nd_round, keywrod_struct_2nd_round]\n",
    "\n",
    "for path_list in all_2nd_paths:\n",
    "    for path in path_list:\n",
    "        if os.path.exists(path):\n",
    "            pass\n",
    "        else:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_struct(path_list):\n",
    "    struct_list = []\n",
    "    for path in path_list:\n",
    "        struct = open_json(path)\n",
    "        struct_list.append(struct)\n",
    "    \n",
    "    final_struct = {}\n",
    "\n",
    "    for struct in struct_list:\n",
    "        for user in struct:\n",
    "            if user in final_struct.keys():\n",
    "                continue \n",
    "            final_struct[user] = struct[user]\n",
    "    return final_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_final_struct(path_list):\n",
    "    list_struct = []\n",
    "    for path in path_list:\n",
    "        struct = open_json(path)\n",
    "        list_struct.append(struct)\n",
    "    \n",
    "    final_list = []\n",
    "    for struct in list_struct:\n",
    "        for relation in struct:\n",
    "            if any(relation == info for info in relation):\n",
    "                continue \n",
    "            final_list.append(relation)\n",
    "\n",
    "    return final_list\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_final_struct(path_list):\n",
    "    struct_list = []\n",
    "    for path in path_list:\n",
    "        struct = open_json(path)\n",
    "        struct_list.append(struct)\n",
    "    \n",
    "    final_struct = {}\n",
    "\n",
    "    # go into the first list, then go into the dictionary and then each key \n",
    "    for struct in struct_list:\n",
    "        for key in struct:\n",
    "\n",
    "            # if the key in the final structure add ids if they are not in there \n",
    "            if key in final_struct.keys():\n",
    "                ids_list = final_struct[key]['ids']\n",
    "                current_ids_list = struct[key]['ids']\n",
    "\n",
    "                for cur_ids in current_ids_list:\n",
    "                    if cur_ids in ids_list:\n",
    "                        continue\n",
    "        \n",
    "                    ids_list.append(cur_ids)\n",
    "                final_struct[key]['ids'] = ids_list\n",
    "                \n",
    "                continue \n",
    "\n",
    "            # if not in dictionary add it and get rid of all ids that are duplicates. \n",
    "            final_ids = set()\n",
    "            for ids in struct[key]['ids']:\n",
    "                if ids in final_ids:\n",
    "                    continue\n",
    "                final_ids.add(ids)\n",
    "\n",
    "            struct[key]['ids'] = list(final_ids)\n",
    "            final_struct[key] = struct[key]\n",
    "    return final_struct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data_list = [final_struct(user_struct_1st_round),\n",
    " final_struct(user_struct_2nd_round),\n",
    "final_struct(post_struct_1st_round),\n",
    "final_struct(post_struct_2nd_round),\n",
    "final_struct(picture_struct_1st_round),\n",
    " final_struct(picture_struct_2nd_round),\n",
    " relation_final_struct(relation_struct_1st_round),\n",
    " relation_final_struct(relation_struct_2nd_round),\n",
    " keyword_final_struct(keyword_struct_1st_round),\n",
    "keyword_final_struct(keyword_struct_2nd_round)]\n",
    "final_struct_list = ['user_1st_round_final_struct', 'user_2nd_round_final_struct', 'post_1st_round_final_struct', 'post_2nd_round_final_struct',\n",
    "                      'picture_1st_round_final_struct', 'picture_2nd_round_final_struct', 'relation_1st_round_final_struct', 'relation_2nd_round_final_struct', \n",
    "                     'keyword_1st_round_final_struct', 'keyword_2nd_round_final_struct']\n",
    "\n",
    "for i, struct in enumerate(final_data_list):\n",
    "    path = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\final_data_structures'\n",
    "    path_file = final_struct_list[i] + '.json'\n",
    "    final_path = os.path.join(path,path_file)\n",
    "    save_json(struct, final_path)\n",
    " \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
