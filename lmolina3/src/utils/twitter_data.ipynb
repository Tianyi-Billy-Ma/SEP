{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client\n",
    "from account_info import USERNAME, EMAIL, PASSWORD\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "\n",
    "# Initialize client\n",
    "client = Client('en-US')\n",
    "\n",
    "client.login(\n",
    "    auth_info_1=USERNAME ,\n",
    "    auth_info_2=EMAIL,\n",
    "    password=PASSWORD\n",
    ")\n",
    "\n",
    "# save cookies in order to pull data without getting banned\n",
    "client.save_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the cookies that were saved\n",
    "client.load_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import TooManyRequests\n",
    "\n",
    "def download_image(URL, save_as):\n",
    "    '''Function uses image URL and saves the image onto the desired path and file type'''\n",
    "    urllib.request.urlretrieve(URL,save_as)\n",
    "\n",
    "def delete_jpg(file):\n",
    "    '''This file takes in the file path and deletes it.'''\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "    else:\n",
    "        print('File not found')\n",
    "\n",
    "# this saves the json file on the described directory \n",
    "def save_json(file, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(file, f,indent=4)\n",
    "\n",
    "\n",
    "def retry_on_rate_limit_error(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except TooManyRequests as e:\n",
    "            print(\"Rate limit exceed trying again in 60 sec\")\n",
    "            time.sleep(60)\n",
    "            retries += 1\n",
    "    raise Exception(\"Max tries reached\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file path to excel sheet\n",
    "file_path = r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\"\n",
    "\n",
    "def keywords_to_dict(file_path =r\"C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\codes.xlsx\" ):\n",
    "    '''This function takes in the excel sheet that has keywords and its ids split up into columns.\n",
    "    Then it creates two dictionaries, keyname and keyname_id. The values are numbered 0 to the \n",
    "    length of the columns. Once created they return these dictionaries.'''\n",
    "\n",
    "    # reads in excel sheet into pandas data frame\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # this turns the street name and its code name into list \n",
    "    street_name_list = df['Street Name'].astype(str).values.tolist()\n",
    "    street_name_code_list = df['Street Name Code'].astype(str).values.tolist()\n",
    "\n",
    "    # this dictionary contains the keyname in the keys and keyname id in values\n",
    "    keyname = {}\n",
    "    keyname_id = {}\n",
    "    for i in range(len(street_name_list)):\n",
    "        keyname[i] = street_name_list[i]\n",
    "        keyname_id[i] = street_name_code_list[i]\n",
    "        \n",
    "    return keyname, keyname_id\n",
    "\n",
    "# print(json.dumps(keyname, indent =4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "users = {}\n",
    "\n",
    "# goes through all the key names and searches them\n",
    "for search_key in keyname.values():\n",
    "    user_keyword = retry_on_rate_limit_error(client.search_user,search_key, 5)\n",
    "    for user in user_keyword:\n",
    "\n",
    "        try: \n",
    "            image_directoary = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\profile_images'\n",
    "            image_file_name = 'user_profile_pic_' + str(user.id) + '.jpg'\n",
    "            image_save_path = os.path.join(image_directoary, image_file_name)\n",
    "            download_image(user.profile_image_url, image_save_path)\n",
    "\n",
    "            keywords = []\n",
    "\n",
    "            keyname, keyname_id = keywords_to_dict()\n",
    "\n",
    "            # get the users tweets\n",
    "            tweets = user.get_tweets('tweets')\n",
    "\n",
    "            # gets users followers\n",
    "            followers = user.get_followers(user.id)\n",
    "\n",
    "            # gets users following\n",
    "            following = user.get_following(user.id)\n",
    "\n",
    "            # empty list for the followers\n",
    "            followers_list = []\n",
    "            following_list = []\n",
    "\n",
    "            # go through the users tweets, description, name to find keywords\n",
    "            for keys, keyname in keyname.items():\n",
    "                if keyname in user.name:\n",
    "                    keywords.append(keyname_id[keys])\n",
    "                elif keyname in user.screen_name:\n",
    "                    keywords.append(keyname_id[keys])\n",
    "                elif keyname in user.description:\n",
    "                    keywords.append(keyname_id[keys])\n",
    "                for tweet in tweets:\n",
    "                    if keyname in tweet.text:\n",
    "                        keywords.append(keyname_id[keys])\n",
    "\n",
    "                # go through the users followers description, tweet, and names to \n",
    "                # see if it finds any keywords. If it does it adds them to the follower_list\n",
    "                for follower_user in followers:\n",
    "                    if keyname in follower_user.name:\n",
    "                        followers_list.append('user_' +  user.id)\n",
    "                    elif keyname in follower_user.screen_name:\n",
    "                        followers_list.append('user_' + user.id)\n",
    "                    elif keyname in follower_user.description:\n",
    "                        followers_list.append('user_' + user.id)\n",
    "                    for tweet in tweets:\n",
    "                        if keyname in tweet.text:\n",
    "                            followers_list.append('user_' + user.id)\n",
    "\n",
    "                # Goes thorugh the users following description, tweet, and names to \n",
    "                # see if it finds any keywords. If it does it adds them to the following_list\n",
    "                for following_user in following:\n",
    "                    if keyname in following_user.name:\n",
    "                        following_list.append('user_' +  user.id)\n",
    "                    elif keyname in following_user.screen_name:\n",
    "                        following_list.append('user_' + user.id)\n",
    "                    elif keyname in following_user.description:\n",
    "                        following_list.append('user_' + user.id)\n",
    "                    for tweet in tweets:\n",
    "                        if keyname in tweet.text:\n",
    "                            following_list.append('user_' + user.id)\n",
    "\n",
    "            # Structure to store the data\n",
    "            User_Structure = {\n",
    "            \"username\": user.screen_name,\n",
    "            \"user_id\": user.id, \n",
    "            \"followers\": [follower for follower in followers_list], #['user_' + str(follower_id) for follower_id in client.get_followers_ids(user.id,user.screen_name,30)],\n",
    "            \"followees\": [following for following in following_list], #['user_' + str(followees_id) for followees_id in client.get_friends_ids(user.id,user.screen_name,30)],\n",
    "            \"profile_pic\": image_save_path,\n",
    "            \"profile_text\": user.description,\n",
    "            \"posts\": ['tweet_' + tweet.id for tweet in tweets]  ,\n",
    "            \"keywords\": keywords\n",
    "            }\n",
    "\n",
    "            # save the user to the dictionary\n",
    "            name = 'user_'+ user.id\n",
    "            if name not in users:\n",
    "                users[name] = User_Structure\n",
    "        except TooManyRequests as e:\n",
    "            print('Rate limit exceeced')\n",
    "            break\n",
    "\n",
    "save_path = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\data\\data_for_twitter\\user_structure.json'\n",
    "save_json(users,save_path)\n",
    "print(json.dumps(users, indent =4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
