{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, MLP\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd = os.path.join(cwd, 'data')\n",
    "\n",
    "dataset = Planetoid(root=cwd, name='Cora')\n",
    "dataset1 = Planetoid(root=cwd, name='CiteSeer')\n",
    "dataset2 = Planetoid(root = cwd, name= 'PubMed')\n",
    "\n",
    "datasets = [dataset, dataset1, dataset2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         # Pre-process normalization to avoid CPU communication/graph breaks:\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(2):\n",
    "            mlp = MLP([num_features, hidden_channels, hidden_channels])\n",
    "            self.convs.append(GINConv(nn=mlp, train_eps=False))\n",
    "            num_features = hidden_channels\n",
    "\n",
    "        self.mlp = MLP([hidden_channels, hidden_channels, num_classes], norm=None, dropout=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        return self.mlp(x) \n",
    "\n",
    "    \n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout= 0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, num_classes, heads=heads, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, optimizer, data, train_mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if model == GCN:\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "    else: \n",
    "        out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(logits[mask], data.y[mask])\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    correct = pred.eq(data.y[mask]).sum().item()\n",
    "    accuracy = correct / mask.sum().item()\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "def split_indices(data, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8):\n",
    "    indices = np.arange(data.num_nodes)\n",
    "    train_size = int(train_ratio * data.num_nodes)\n",
    "    val_size = int(val_ratio * data.num_nodes)\n",
    "    test_size = int(test_ratio * data.num_nodes)\n",
    "    \n",
    "    train_indices, temp_indices = sklearn_train_test_split(indices, train_size=train_size, random_state=42)\n",
    "    val_indices, test_indices = sklearn_train_test_split(temp_indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_indices] = True\n",
    "    val_mask[val_indices] = True\n",
    "    test_mask[test_indices] = True\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(model_class, num_features, num_classes, data, num_runs=5, epochs=300):\n",
    "    for dataset in datasets:\n",
    "        accuracies = []\n",
    "        for _ in range(num_runs):\n",
    "            print(f\"Run {_ + 1}/{num_runs} on dataset {dataset.name}\")\n",
    "            train_mask, val_mask, test_mask = split_indices(data)\n",
    "            \n",
    "            if model_class == GCN:\n",
    "                model = model_class(num_features, hidden_channels=256, num_classes=num_classes).to(device)\n",
    "            model = model_class(num_features, hidden_channels=256, num_classes=num_classes).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            \n",
    "            best_val_acc = 0.0\n",
    "            best_model_state = None\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train(model, optimizer, data, train_mask)\n",
    "                val_acc, val_loss = evaluate(model, data, val_mask)\n",
    "                test_acc, test_loss = evaluate(model, data, test_mask)\n",
    "\n",
    "                if epoch % 200 == 0:\n",
    "                \n",
    "                    print(f\"Epoch: {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = model.state_dict()\n",
    "            \n",
    "            if best_model_state:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            test_acc, _ = evaluate(model, data, test_mask)\n",
    "            accuracies.append(test_acc)\n",
    "        \n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies)\n",
    "        \n",
    "        print(f\"\\nMean Accuracy over {num_runs} runs: {mean_acc:.4f}, Std Deviation: {std_acc:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "\n",
      "<class '__main__.GCN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9518 | Val Acc: 0.4926 | Test Acc: 0.5125\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8287\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9632 | Val Acc: 0.4338 | Test Acc: 0.4534\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8269\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9439 | Val Acc: 0.3603 | Test Acc: 0.3869\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8319\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9501 | Val Acc: 0.4228 | Test Acc: 0.4460\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8278\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9504 | Val Acc: 0.3897 | Test Acc: 0.4021\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8283\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8279, Std Deviation: 0.0014\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9549 | Val Acc: 0.4301 | Test Acc: 0.4488\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.8015 | Test Acc: 0.8301\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9492 | Val Acc: 0.4265 | Test Acc: 0.4363\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8283\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9486 | Val Acc: 0.5257 | Test Acc: 0.5212\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8246\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9567 | Val Acc: 0.4118 | Test Acc: 0.4363\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8296\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9542 | Val Acc: 0.4412 | Test Acc: 0.4792\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8287\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8276, Std Deviation: 0.0019\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9517 | Val Acc: 0.3897 | Test Acc: 0.4252\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8315\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9519 | Val Acc: 0.4007 | Test Acc: 0.4132\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7868 | Test Acc: 0.8287\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9582 | Val Acc: 0.4853 | Test Acc: 0.4963\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8273\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9386 | Val Acc: 0.4265 | Test Acc: 0.4497\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8296\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9698 | Val Acc: 0.4706 | Test Acc: 0.4880\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8287\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8284, Std Deviation: 0.0021\n",
      "\n",
      "\n",
      "<class '__main__.GAT'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9842 | Val Acc: 0.4118 | Test Acc: 0.4552\n",
      "Epoch: 201/300 | Train Loss: 0.5242 | Val Acc: 0.7904 | Test Acc: 0.8167\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0285 | Val Acc: 0.4779 | Test Acc: 0.4866\n",
      "Epoch: 201/300 | Train Loss: 0.6075 | Val Acc: 0.8015 | Test Acc: 0.8259\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0785 | Val Acc: 0.4890 | Test Acc: 0.5309\n",
      "Epoch: 201/300 | Train Loss: 0.5859 | Val Acc: 0.7941 | Test Acc: 0.8306\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9805 | Val Acc: 0.4191 | Test Acc: 0.4617\n",
      "Epoch: 201/300 | Train Loss: 0.6478 | Val Acc: 0.7978 | Test Acc: 0.8269\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9988 | Val Acc: 0.4706 | Test Acc: 0.4820\n",
      "Epoch: 201/300 | Train Loss: 0.6298 | Val Acc: 0.7978 | Test Acc: 0.8315\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8293, Std Deviation: 0.0021\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9833 | Val Acc: 0.5257 | Test Acc: 0.5360\n",
      "Epoch: 201/300 | Train Loss: 0.6437 | Val Acc: 0.8015 | Test Acc: 0.8329\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0358 | Val Acc: 0.3787 | Test Acc: 0.4151\n",
      "Epoch: 201/300 | Train Loss: 0.6286 | Val Acc: 0.7831 | Test Acc: 0.8273\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0141 | Val Acc: 0.5588 | Test Acc: 0.5406\n",
      "Epoch: 201/300 | Train Loss: 0.5317 | Val Acc: 0.8015 | Test Acc: 0.8269\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0175 | Val Acc: 0.4890 | Test Acc: 0.5018\n",
      "Epoch: 201/300 | Train Loss: 0.6012 | Val Acc: 0.7978 | Test Acc: 0.8218\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0285 | Val Acc: 0.6176 | Test Acc: 0.6464\n",
      "Epoch: 201/300 | Train Loss: 0.6730 | Val Acc: 0.7978 | Test Acc: 0.8246\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8302, Std Deviation: 0.0059\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0408 | Val Acc: 0.4743 | Test Acc: 0.5005\n",
      "Epoch: 201/300 | Train Loss: 0.6537 | Val Acc: 0.8051 | Test Acc: 0.8310\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9876 | Val Acc: 0.6287 | Test Acc: 0.6219\n",
      "Epoch: 201/300 | Train Loss: 0.5884 | Val Acc: 0.8015 | Test Acc: 0.8407\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0058 | Val Acc: 0.3934 | Test Acc: 0.4100\n",
      "Epoch: 201/300 | Train Loss: 0.7000 | Val Acc: 0.8125 | Test Acc: 0.8343\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9942 | Val Acc: 0.4375 | Test Acc: 0.4243\n",
      "Epoch: 201/300 | Train Loss: 0.5379 | Val Acc: 0.8051 | Test Acc: 0.8283\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9914 | Val Acc: 0.4596 | Test Acc: 0.4825\n",
      "Epoch: 201/300 | Train Loss: 0.5478 | Val Acc: 0.8051 | Test Acc: 0.8389\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8308, Std Deviation: 0.0022\n",
      "\n",
      "\n",
      "<class '__main__.GIN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9550 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7059 | Test Acc: 0.7299\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9574 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6838 | Test Acc: 0.7396\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9339 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7390 | Test Acc: 0.7525\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9481 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7022 | Test Acc: 0.7401\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9680 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0012 | Val Acc: 0.7647 | Test Acc: 0.7447\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7439, Std Deviation: 0.0131\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9468 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7279 | Test Acc: 0.7318\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9382 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7169 | Test Acc: 0.7507\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9537 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7390 | Test Acc: 0.7516\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9365 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0002 | Val Acc: 0.7243 | Test Acc: 0.7304\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9573 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7096 | Test Acc: 0.7295\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7393, Std Deviation: 0.0082\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9460 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7353 | Test Acc: 0.7622\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9456 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7132 | Test Acc: 0.7313\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9543 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7279 | Test Acc: 0.7705\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9425 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7426 | Test Acc: 0.7525\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9488 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7353 | Test Acc: 0.7525\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7530, Std Deviation: 0.0150\n",
      "\n",
      "Dataset: CiteSeer\n",
      "\n",
      "<class '__main__.GCN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.7898 | Val Acc: 0.6527 | Test Acc: 0.6881\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6407 | Test Acc: 0.6802\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.7928 | Val Acc: 0.6347 | Test Acc: 0.6753\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6317 | Test Acc: 0.6779\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.7936 | Val Acc: 0.6707 | Test Acc: 0.6855\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6317 | Test Acc: 0.6764\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.7844 | Val Acc: 0.6557 | Test Acc: 0.6941\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6377 | Test Acc: 0.6798\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.7905 | Val Acc: 0.6407 | Test Acc: 0.6862\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6347 | Test Acc: 0.6772\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.6780, Std Deviation: 0.0018\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.7995 | Val Acc: 0.6377 | Test Acc: 0.6479\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6347 | Test Acc: 0.6787\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.7932 | Val Acc: 0.6587 | Test Acc: 0.6903\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6407 | Test Acc: 0.6783\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.7959 | Val Acc: 0.6647 | Test Acc: 0.6644\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6287 | Test Acc: 0.6813\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.7965 | Val Acc: 0.6317 | Test Acc: 0.6712\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6287 | Test Acc: 0.6798\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.8162 | Val Acc: 0.6557 | Test Acc: 0.6990\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6257 | Test Acc: 0.6738\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.6788, Std Deviation: 0.0016\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.7999 | Val Acc: 0.6317 | Test Acc: 0.6809\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6317 | Test Acc: 0.6738\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.8003 | Val Acc: 0.6796 | Test Acc: 0.6866\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6287 | Test Acc: 0.6738\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.7930 | Val Acc: 0.6527 | Test Acc: 0.6787\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6347 | Test Acc: 0.6806\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.7903 | Val Acc: 0.6497 | Test Acc: 0.6930\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6287 | Test Acc: 0.6749\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.8038 | Val Acc: 0.6647 | Test Acc: 0.6956\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.6347 | Test Acc: 0.6802\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.6773, Std Deviation: 0.0029\n",
      "\n",
      "\n",
      "<class '__main__.GAT'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.8734 | Val Acc: 0.5030 | Test Acc: 0.5423\n",
      "Epoch: 201/300 | Train Loss: 0.7394 | Val Acc: 0.6377 | Test Acc: 0.6663\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.8792 | Val Acc: 0.5030 | Test Acc: 0.5400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m             run_experiment(model_class, num_features, num_classes, data)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 15\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 19\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(model_class, num_features, num_classes, data, num_runs, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train(model, optimizer, data, train_mask)\n\u001b[0;32m     18\u001b[0m val_acc, val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, data, val_mask)\n\u001b[1;32m---> 19\u001b[0m test_acc, test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 17\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data, mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(model, data, mask):\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 17\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits[mask], data\u001b[38;5;241m.\u001b[39my[mask])\n\u001b[0;32m     19\u001b[0m     pred \u001b[38;5;241m=\u001b[39m logits[mask]\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[53], line 55\u001b[0m, in \u001b[0;36mGAT.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m---> 55\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     56\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m     57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x, edge_index)\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:280\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatic graphs not supported in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGATConv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 280\u001b[0m     x_src \u001b[38;5;241m=\u001b[39m x_dst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, H, C)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# If the module is initialized as bipartite, transform source\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# and destination node features separately:\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_src \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_dst \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    print(f'Dataset: {dataset.name}')\n",
    "    data = dataset[0]  # Assuming dataset is a tuple where [0] is the data object\n",
    "\n",
    "    global device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_features = data.num_features\n",
    "    num_classes = dataset.num_classes\n",
    "\n",
    "    for model_class in [GCN, GAT, GIN]:\n",
    "        print('')\n",
    "        print(f'{model_class}')\n",
    "        run_experiment(model_class, num_features, num_classes, data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
