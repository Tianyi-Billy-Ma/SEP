{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, MLP\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd = os.path.join(cwd, 'data')\n",
    "\n",
    "dataset = Planetoid(root=cwd, name='Cora')\n",
    "dataset1 = Planetoid(root=cwd, name='CiteSeer')\n",
    "dataset2 = Planetoid(root = cwd, name= 'PubMed')\n",
    "\n",
    "datasets = [dataset, dataset1, dataset2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         # Pre-process normalization to avoid CPU communication/graph breaks:\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(2):\n",
    "            mlp = MLP([num_features, hidden_channels, hidden_channels])\n",
    "            self.convs.append(GINConv(nn=mlp, train_eps=False))\n",
    "            num_features = hidden_channels\n",
    "\n",
    "        self.mlp = MLP([hidden_channels, hidden_channels, num_classes], norm=None, dropout=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        return self.mlp(x) \n",
    "\n",
    "    \n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout= 0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, num_classes, heads=heads, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, optimizer, data, train_mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if model == GCN:\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "    else: \n",
    "        out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(logits[mask], data.y[mask])\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    correct = pred.eq(data.y[mask]).sum().item()\n",
    "    accuracy = correct / mask.sum().item()\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "def split_indices(data, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8):\n",
    "    indices = np.arange(data.num_nodes)\n",
    "    train_size = int(train_ratio * data.num_nodes)\n",
    "    val_size = int(val_ratio * data.num_nodes)\n",
    "    test_size = int(test_ratio * data.num_nodes)\n",
    "    \n",
    "    train_indices, temp_indices = train_test_split(indices, train_size=train_size, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_indices] = True\n",
    "    val_mask[val_indices] = True\n",
    "    test_mask[test_indices] = True\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mask(mask, reduction_ratio):\n",
    "    num_true = mask.sum().item()\n",
    "    num_to_keep = int(num_true * (reduction_ratio))\n",
    "    \n",
    "    true_indices = mask.nonzero(as_tuple=True)[0].numpy()\n",
    "    np.random.shuffle(true_indices)\n",
    "    \n",
    "    selected_indices = true_indices[:num_to_keep]\n",
    "    \n",
    "    new_mask = torch.zeros_like(mask)\n",
    "    new_mask[selected_indices] = True\n",
    "    \n",
    "    return new_mask\n",
    "\n",
    "def adjust_masks(data, train_reduction=0.1, val_reduction=0.1, test_reduction=0.8):\n",
    "    train_mask = data.train_mask\n",
    "    val_mask = data.val_mask\n",
    "    test_mask = data.test_mask\n",
    "    \n",
    "    train_mask = reduce_mask(train_mask, train_reduction)\n",
    "    val_mask = reduce_mask(val_mask, val_reduction)\n",
    "    test_mask = reduce_mask(test_mask, test_reduction)\n",
    "    \n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(model_class, num_features, num_classes, data, num_runs=5, epochs=300):\n",
    "    for dataset in datasets:\n",
    "        accuracies = []\n",
    "        for _ in range(num_runs):\n",
    "            print(f\"Run {_ + 1}/{num_runs} on dataset {dataset.name}\")\n",
    "            train_mask, val_mask, test_mask = split_indices(data)\n",
    "            \n",
    "            if model_class == GCN:\n",
    "                model = model_class(num_features, hidden_channels=256, num_classes=num_classes).to(device)\n",
    "            model = model_class(num_features, hidden_channels=256, num_classes=num_classes).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            \n",
    "            best_val_acc = 0.0\n",
    "            best_model_state = None\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train(model, optimizer, data, train_mask)\n",
    "                val_acc, val_loss = evaluate(model, data, val_mask)\n",
    "                test_acc, test_loss = evaluate(model, data, test_mask)\n",
    "\n",
    "                if epoch % 200 == 0:\n",
    "                \n",
    "                    print(f\"Epoch: {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = model.state_dict()\n",
    "            \n",
    "            if best_model_state:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            test_acc, _ = evaluate(model, data, test_mask)\n",
    "            accuracies.append(test_acc)\n",
    "        \n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies)\n",
    "        \n",
    "        print(f\"\\nMean Accuracy over {num_runs} runs: {mean_acc:.4f}, Std Deviation: {std_acc:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "\n",
      "<class '__main__.GCN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9586 | Val Acc: 0.4191 | Test Acc: 0.4501\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8273\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9685 | Val Acc: 0.4485 | Test Acc: 0.4783\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8283\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9507 | Val Acc: 0.4559 | Test Acc: 0.4751\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8269\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9513 | Val Acc: 0.4926 | Test Acc: 0.4834\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8269\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9336 | Val Acc: 0.4706 | Test Acc: 0.4894\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8269\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8260, Std Deviation: 0.0005\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9395 | Val Acc: 0.3750 | Test Acc: 0.4044\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8292\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9483 | Val Acc: 0.5294 | Test Acc: 0.5346\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8246\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9471 | Val Acc: 0.4228 | Test Acc: 0.4409\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8278\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9410 | Val Acc: 0.4890 | Test Acc: 0.4945\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8310\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9546 | Val Acc: 0.3860 | Test Acc: 0.4317\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8301\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8283, Std Deviation: 0.0020\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9503 | Val Acc: 0.5147 | Test Acc: 0.5305\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8310\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9402 | Val Acc: 0.3824 | Test Acc: 0.4132\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8269\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9628 | Val Acc: 0.4412 | Test Acc: 0.4668\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8324\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9355 | Val Acc: 0.3824 | Test Acc: 0.4104\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8269\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9501 | Val Acc: 0.4265 | Test Acc: 0.4631\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8283\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8283, Std Deviation: 0.0018\n",
      "\n",
      "\n",
      "<class '__main__.GAT'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9873 | Val Acc: 0.4743 | Test Acc: 0.4760\n",
      "Epoch: 201/300 | Train Loss: 0.5517 | Val Acc: 0.8199 | Test Acc: 0.8287\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0218 | Val Acc: 0.4632 | Test Acc: 0.4621\n",
      "Epoch: 201/300 | Train Loss: 0.6108 | Val Acc: 0.7978 | Test Acc: 0.8333\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0278 | Val Acc: 0.5404 | Test Acc: 0.5254\n",
      "Epoch: 201/300 | Train Loss: 0.6490 | Val Acc: 0.8015 | Test Acc: 0.8347\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9653 | Val Acc: 0.5772 | Test Acc: 0.5457\n",
      "Epoch: 201/300 | Train Loss: 0.6270 | Val Acc: 0.8088 | Test Acc: 0.8338\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0241 | Val Acc: 0.5368 | Test Acc: 0.5651\n",
      "Epoch: 201/300 | Train Loss: 0.6412 | Val Acc: 0.8088 | Test Acc: 0.8283\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8270, Std Deviation: 0.0032\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0345 | Val Acc: 0.5257 | Test Acc: 0.5499\n",
      "Epoch: 201/300 | Train Loss: 0.7147 | Val Acc: 0.8088 | Test Acc: 0.8232\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0338 | Val Acc: 0.5919 | Test Acc: 0.6136\n",
      "Epoch: 201/300 | Train Loss: 0.6861 | Val Acc: 0.7904 | Test Acc: 0.8319\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9942 | Val Acc: 0.6544 | Test Acc: 0.6588\n",
      "Epoch: 201/300 | Train Loss: 0.6282 | Val Acc: 0.8272 | Test Acc: 0.8296\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9682 | Val Acc: 0.6397 | Test Acc: 0.6450\n",
      "Epoch: 201/300 | Train Loss: 0.5856 | Val Acc: 0.8015 | Test Acc: 0.8301\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9878 | Val Acc: 0.4853 | Test Acc: 0.4931\n",
      "Epoch: 201/300 | Train Loss: 0.5946 | Val Acc: 0.7941 | Test Acc: 0.8301\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8267, Std Deviation: 0.0040\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0008 | Val Acc: 0.4118 | Test Acc: 0.4201\n",
      "Epoch: 201/300 | Train Loss: 0.6651 | Val Acc: 0.8015 | Test Acc: 0.8255\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0111 | Val Acc: 0.4228 | Test Acc: 0.4298\n",
      "Epoch: 201/300 | Train Loss: 0.6654 | Val Acc: 0.8199 | Test Acc: 0.8255\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0220 | Val Acc: 0.5147 | Test Acc: 0.5125\n",
      "Epoch: 201/300 | Train Loss: 0.5206 | Val Acc: 0.8199 | Test Acc: 0.8403\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9865 | Val Acc: 0.6176 | Test Acc: 0.6468\n",
      "Epoch: 201/300 | Train Loss: 0.5962 | Val Acc: 0.8235 | Test Acc: 0.8296\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0038 | Val Acc: 0.5809 | Test Acc: 0.5771\n",
      "Epoch: 201/300 | Train Loss: 0.5757 | Val Acc: 0.7941 | Test Acc: 0.8278\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8288, Std Deviation: 0.0023\n",
      "\n",
      "\n",
      "<class '__main__.GIN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9609 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6985 | Test Acc: 0.7387\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9575 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7316 | Test Acc: 0.7438\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9541 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7316 | Test Acc: 0.7627\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9512 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7500 | Test Acc: 0.7285\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9545 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7243 | Test Acc: 0.7442\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7460, Std Deviation: 0.0141\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9523 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6949 | Test Acc: 0.7318\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9425 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7279 | Test Acc: 0.7548\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9620 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7426 | Test Acc: 0.7488\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9557 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7500 | Test Acc: 0.7678\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9498 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6912 | Test Acc: 0.7285\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7478, Std Deviation: 0.0144\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9713 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0003 | Val Acc: 0.7132 | Test Acc: 0.7322\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9300 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7426 | Test Acc: 0.7613\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9706 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6875 | Test Acc: 0.7318\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9516 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7279 | Test Acc: 0.7567\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9549 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7316 | Test Acc: 0.7659\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7577, Std Deviation: 0.0053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    print(f'Dataset: {dataset.name}')\n",
    "    data = dataset[0]  # Assuming dataset is a tuple where [0] is the data object\n",
    "\n",
    "    global device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_features = data.num_features\n",
    "    num_classes = dataset.num_classes\n",
    "\n",
    "    for model_class in [GCN, GAT, GIN]:\n",
    "        print('')\n",
    "        print(f'{model_class}')\n",
    "        run_experiment(model_class, num_features, num_classes, data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
     ]
    }
   ],
   "source": [
    "data = datasets[2][0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  ..., False, False, False]) torch.Size([19717]) 60\n",
      "tensor([False, False, False,  ..., False, False, False]) torch.Size([19717]) 500\n",
      "tensor([False, False, False,  ...,  True,  True,  True]) torch.Size([19717]) 1000\n"
     ]
    }
   ],
   "source": [
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "print(train_mask.data, train_mask.shape, train_mask.sum().item())\n",
    "print(val_mask.data, val_mask.shape, val_mask.sum().item())\n",
    "print(test_mask.data, test_mask.shape, test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False,  ...,  True, False,  True]) torch.Size([19717]) 1971\n",
      "tensor([False, False, False,  ..., False, False, False]) torch.Size([19717]) 1973\n",
      "tensor([ True,  True,  True,  ..., False,  True, False]) torch.Size([19717]) 15773\n"
     ]
    }
   ],
   "source": [
    "train_mask, val_mask, test_mask = split_indices(data)\n",
    "\n",
    "print(train_mask.data, train_mask.shape, train_mask.sum().item())\n",
    "print(val_mask.data, val_mask.shape, val_mask.sum().item())\n",
    "print(test_mask.data, test_mask.shape, test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable:[0 0 0 ... 3 3 3]\n",
      "labels inside encode_oneshot [[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "lable after encode oneshot [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t1.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 11)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 13)\t1.0\n",
      "  (14, 14)\t1.0\n",
      "  (15, 15)\t1.0\n",
      "  (16, 16)\t1.0\n",
      "  (17, 17)\t1.0\n",
      "  (18, 18)\t1.0\n",
      "  (19, 19)\t1.0\n",
      "  (20, 20)\t1.0\n",
      "  (21, 21)\t1.0\n",
      "  (22, 22)\t1.0\n",
      "  (23, 23)\t1.0\n",
      "  (24, 24)\t1.0\n",
      "  :\t:\n",
      "  (13304, 13304)\t1.0\n",
      "  (13305, 13305)\t1.0\n",
      "  (13306, 13306)\t1.0\n",
      "  (13307, 13307)\t1.0\n",
      "  (13308, 13308)\t1.0\n",
      "  (13309, 13309)\t1.0\n",
      "  (13310, 13310)\t1.0\n",
      "  (13311, 13311)\t1.0\n",
      "  (13312, 13312)\t1.0\n",
      "  (13313, 13313)\t1.0\n",
      "  (13314, 13314)\t1.0\n",
      "  (13315, 13315)\t1.0\n",
      "  (13316, 13316)\t1.0\n",
      "  (13317, 13317)\t1.0\n",
      "  (13318, 13318)\t1.0\n",
      "  (13319, 13319)\t1.0\n",
      "  (13320, 13320)\t1.0\n",
      "  (13321, 13321)\t1.0\n",
      "  (13322, 13322)\t1.0\n",
      "  (13323, 13323)\t1.0\n",
      "  (13324, 13324)\t1.0\n",
      "  (13325, 13325)\t1.0\n",
      "  (13326, 13326)\t1.0\n",
      "  (13327, 13327)\t1.0\n",
      "  (13328, 13328)\t1.0   (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t1.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 11)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 13)\t1.0\n",
      "  (14, 14)\t1.0\n",
      "  (15, 15)\t1.0\n",
      "  (16, 16)\t1.0\n",
      "  (17, 17)\t1.0\n",
      "  (18, 18)\t1.0\n",
      "  (19, 19)\t1.0\n",
      "  (20, 20)\t1.0\n",
      "  (21, 21)\t1.0\n",
      "  (22, 22)\t1.0\n",
      "  (23, 23)\t1.0\n",
      "  (24, 24)\t1.0\n",
      "  :\t:\n",
      "  (13304, 13304)\t1.0\n",
      "  (13305, 13305)\t1.0\n",
      "  (13306, 13306)\t1.0\n",
      "  (13307, 13307)\t1.0\n",
      "  (13308, 13308)\t1.0\n",
      "  (13309, 13309)\t1.0\n",
      "  (13310, 13310)\t1.0\n",
      "  (13311, 13311)\t1.0\n",
      "  (13312, 13312)\t1.0\n",
      "  (13313, 13313)\t1.0\n",
      "  (13314, 13314)\t1.0\n",
      "  (13315, 13315)\t1.0\n",
      "  (13316, 13316)\t1.0\n",
      "  (13317, 13317)\t1.0\n",
      "  (13318, 13318)\t1.0\n",
      "  (13319, 13319)\t1.0\n",
      "  (13320, 13320)\t1.0\n",
      "  (13321, 13321)\t1.0\n",
      "  (13322, 13322)\t1.0\n",
      "  (13323, 13323)\t1.0\n",
      "  (13324, 13324)\t1.0\n",
      "  (13325, 13325)\t1.0\n",
      "  (13326, 13326)\t1.0\n",
      "  (13327, 13327)\t1.0\n",
      "  (13328, 13328)\t1.0   (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t1.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 11)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 13)\t1.0\n",
      "  (14, 14)\t1.0\n",
      "  (15, 15)\t1.0\n",
      "  (16, 16)\t1.0\n",
      "  (17, 17)\t1.0\n",
      "  (18, 18)\t1.0\n",
      "  (19, 19)\t1.0\n",
      "  (20, 20)\t1.0\n",
      "  (21, 21)\t1.0\n",
      "  (22, 22)\t1.0\n",
      "  (23, 23)\t1.0\n",
      "  (24, 24)\t1.0\n",
      "  :\t:\n",
      "  (35865, 35865)\t1.0\n",
      "  (35866, 35866)\t1.0\n",
      "  (35867, 35867)\t1.0\n",
      "  (35868, 35868)\t1.0\n",
      "  (35869, 35869)\t1.0\n",
      "  (35870, 35870)\t1.0\n",
      "  (35871, 35871)\t1.0\n",
      "  (35872, 35872)\t1.0\n",
      "  (35873, 35873)\t1.0\n",
      "  (35874, 35874)\t1.0\n",
      "  (35875, 35875)\t1.0\n",
      "  (35876, 35876)\t1.0\n",
      "  (35877, 35877)\t1.0\n",
      "  (35878, 35878)\t1.0\n",
      "  (35879, 35879)\t1.0\n",
      "  (35880, 35880)\t1.0\n",
      "  (35881, 35881)\t1.0\n",
      "  (35882, 35882)\t1.0\n",
      "  (35883, 35883)\t1.0\n",
      "  (35884, 35884)\t1.0\n",
      "  (35885, 35885)\t1.0\n",
      "  (35886, 35886)\t1.0\n",
      "  (35887, 35887)\t1.0\n",
      "  (35888, 35888)\t1.0\n",
      "  (35889, 35889)\t1.0\n",
      "\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "\n",
      "  (0, 0)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 62)\tTrue\n",
      "  (3, 3)\tTrue\n",
      "  (3, 105)\tTrue\n",
      "  (3, 220)\tTrue\n",
      "  (4, 4)\tTrue\n",
      "  (4, 91)\tTrue\n",
      "  (5, 5)\tTrue\n",
      "  (6, 6)\tTrue\n",
      "  (6, 12)\tTrue\n",
      "  (7, 7)\tTrue\n",
      "  (8, 8)\tTrue\n",
      "  (9, 9)\tTrue\n",
      "  (10, 10)\tTrue\n",
      "  (11, 11)\tTrue\n",
      "  (11, 219)\tTrue\n",
      "  (12, 6)\tTrue\n",
      "  (12, 12)\tTrue\n",
      "  (12, 92)\tTrue\n",
      "  (13, 13)\tTrue\n",
      "  (14, 14)\tTrue\n",
      "  (14, 106)\tTrue\n",
      "  (14, 221)\tTrue\n",
      "  :\t:\n",
      "  (6552, 6552)\tTrue\n",
      "  (6553, 3315)\tTrue\n",
      "  (6553, 6553)\tTrue\n",
      "  (6554, 6554)\tTrue\n",
      "  (6555, 3322)\tTrue\n",
      "  (6555, 3364)\tTrue\n",
      "  (6555, 6555)\tTrue\n",
      "  (6556, 6556)\tTrue\n",
      "  (6557, 6557)\tTrue\n",
      "  (6558, 6558)\tTrue\n",
      "  (6559, 6559)\tTrue\n",
      "  (6560, 6560)\tTrue\n",
      "  (6561, 2995)\tTrue\n",
      "  (6561, 3008)\tTrue\n",
      "  (6561, 3039)\tTrue\n",
      "  (6561, 3183)\tTrue\n",
      "  (6561, 3201)\tTrue\n",
      "  (6561, 3309)\tTrue\n",
      "  (6561, 6561)\tTrue\n",
      "  (6562, 3340)\tTrue\n",
      "  (6562, 3366)\tTrue\n",
      "  (6562, 6562)\tTrue\n",
      "  (6563, 4586)\tTrue\n",
      "  (6563, 6497)\tTrue\n",
      "  (6563, 6563)\tTrue\n",
      "  (0, 0)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 62)\tTrue\n",
      "  (3, 3)\tTrue\n",
      "  (3, 105)\tTrue\n",
      "  (3, 220)\tTrue\n",
      "  (4, 4)\tTrue\n",
      "  (4, 91)\tTrue\n",
      "  (5, 5)\tTrue\n",
      "  (6, 6)\tTrue\n",
      "  (6, 12)\tTrue\n",
      "  (7, 7)\tTrue\n",
      "  (8, 8)\tTrue\n",
      "  (9, 9)\tTrue\n",
      "  (10, 10)\tTrue\n",
      "  (11, 11)\tTrue\n",
      "  (11, 219)\tTrue\n",
      "  (12, 6)\tTrue\n",
      "  (12, 12)\tTrue\n",
      "  (12, 92)\tTrue\n",
      "  (13, 13)\tTrue\n",
      "  (14, 14)\tTrue\n",
      "  (14, 106)\tTrue\n",
      "  (14, 221)\tTrue\n",
      "  :\t:\n",
      "  (6552, 6552)\tTrue\n",
      "  (6553, 3315)\tTrue\n",
      "  (6553, 6553)\tTrue\n",
      "  (6554, 6554)\tTrue\n",
      "  (6555, 3322)\tTrue\n",
      "  (6555, 3364)\tTrue\n",
      "  (6555, 6555)\tTrue\n",
      "  (6556, 6556)\tTrue\n",
      "  (6557, 6557)\tTrue\n",
      "  (6558, 6558)\tTrue\n",
      "  (6559, 6559)\tTrue\n",
      "  (6560, 6560)\tTrue\n",
      "  (6561, 2995)\tTrue\n",
      "  (6561, 3008)\tTrue\n",
      "  (6561, 3039)\tTrue\n",
      "  (6561, 3183)\tTrue\n",
      "  (6561, 3201)\tTrue\n",
      "  (6561, 3309)\tTrue\n",
      "  (6561, 6561)\tTrue\n",
      "  (6562, 3340)\tTrue\n",
      "  (6562, 3366)\tTrue\n",
      "  (6562, 6562)\tTrue\n",
      "  (6563, 4586)\tTrue\n",
      "  (6563, 6497)\tTrue\n",
      "  (6563, 6563)\tTrue\n",
      "  (0, 0)\tTrue\n",
      "  (0, 124)\tTrue\n",
      "  (0, 168)\tTrue\n",
      "  (0, 547)\tTrue\n",
      "  (0, 1698)\tTrue\n",
      "  (0, 2728)\tTrue\n",
      "  (0, 2935)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (1, 7)\tTrue\n",
      "  (1, 38)\tTrue\n",
      "  (1, 39)\tTrue\n",
      "  (1, 1690)\tTrue\n",
      "  (1, 1851)\tTrue\n",
      "  (1, 2091)\tTrue\n",
      "  (1, 2160)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 11)\tTrue\n",
      "  (2, 15)\tTrue\n",
      "  (2, 26)\tTrue\n",
      "  (2, 45)\tTrue\n",
      "  (2, 62)\tTrue\n",
      "  (2, 113)\tTrue\n",
      "  (2, 118)\tTrue\n",
      "  (2, 254)\tTrue\n",
      "  (2, 302)\tTrue\n",
      "  :\t:\n",
      "  (6563, 3334)\tTrue\n",
      "  (6563, 3349)\tTrue\n",
      "  (6563, 3354)\tTrue\n",
      "  (6563, 3355)\tTrue\n",
      "  (6563, 3360)\tTrue\n",
      "  (6563, 3363)\tTrue\n",
      "  (6563, 3366)\tTrue\n",
      "  (6563, 3656)\tTrue\n",
      "  (6563, 4377)\tTrue\n",
      "  (6563, 4700)\tTrue\n",
      "  (6563, 5396)\tTrue\n",
      "  (6563, 5891)\tTrue\n",
      "  (6563, 5951)\tTrue\n",
      "  (6563, 6524)\tTrue\n",
      "  (6563, 6531)\tTrue\n",
      "  (6563, 6535)\tTrue\n",
      "  (6563, 6536)\tTrue\n",
      "  (6563, 6537)\tTrue\n",
      "  (6563, 6541)\tTrue\n",
      "  (6563, 6542)\tTrue\n",
      "  (6563, 6543)\tTrue\n",
      "  (6563, 6547)\tTrue\n",
      "  (6563, 6560)\tTrue\n",
      "  (6563, 6562)\tTrue\n",
      "  (6563, 6563)\tTrue\n",
      "[tensor([2424, 3095, 3608,  667, 1313, 2000, 2414, 1454, 1844, 2588, 2737, 1330,\n",
      "         361, 2037, 4735, 1989, 6151, 2926, 2721, 5353, 2686, 2845, 5504, 4380,\n",
      "        2676, 1711,   76, 5565, 2333,  117,  990, 2887, 2420,  582,  878,  347,\n",
      "        1158, 2654, 3211, 3773, 1268, 1254, 1789, 4390, 1843, 5128, 1816, 2893,\n",
      "        6160, 2766, 1014, 2670,  961, 2538, 2827, 1334, 2304, 1227, 5537, 5260,\n",
      "         458,  789, 2032, 2035, 2703, 2671, 2346, 3039, 3882, 2562, 4093,   58,\n",
      "        2075, 2497, 1004, 2750, 5957, 4321, 2940, 1660]), tensor([2729,  824,  242,  591, 2809, 2607, 1297, 2026, 1700, 4080, 1673, 2929,\n",
      "         498, 1881, 2406, 4101, 2638, 5745, 1506, 5873, 2483,  541, 2754, 5355,\n",
      "        2826,   41, 4070, 4680, 1682, 5676, 4353, 2717, 5376,  697,  111, 2437,\n",
      "         690, 2827, 2677, 1293,  519, 2363, 6272, 2909, 2801, 2947, 1366, 5925,\n",
      "        1333, 1417, 2567, 2396,  116,  809, 2035, 1113, 6343, 4796, 1949,  223,\n",
      "        3244, 4458,  203, 6155,  516, 4969, 2538, 2566, 1502, 2907,  119, 3840,\n",
      "        2652, 3252, 2815, 1055, 2114, 2835,  980, 2642, 2890, 2771, 1246, 3312,\n",
      "        1939, 5095, 2611, 3875, 3535, 2953, 5224, 2651, 1354, 1363, 1952, 4195,\n",
      "        1266, 1418, 1003, 1065, 1386, 2472,  572, 2120, 1020, 5868, 5612, 2235,\n",
      "        2269, 2339, 1459, 2038, 1115, 5114, 2258,  674, 2401, 2859, 3755, 4515,\n",
      "        2083, 6503,  707, 2092, 2854,  737,  322,  664, 5540, 5480, 1283, 2872,\n",
      "        2429, 1133, 1762, 1392,  346, 5881, 2845, 2882, 1777, 5973, 2101, 2659,\n",
      "        3706,  576, 1073,  943, 2692, 3558, 6052, 2571, 2637, 6319, 2294, 2764,\n",
      "         139, 1868, 2589, 2951]), tensor([2823, 2499, 1989, 2739, 4454,  176, 4192, 2614, 2870, 4911, 2247, 2702,\n",
      "        2121, 1904, 2967, 5000,  271, 2641,   65, 5926, 2888, 1803, 5376, 2835,\n",
      "        2176, 2660, 3655, 2949, 6312, 2995, 4053, 2635, 1185, 1829, 1084, 2551,\n",
      "        1099, 3531, 1603, 5745, 2622, 2555, 1944, 1022, 3046, 1074, 6303,  151,\n",
      "         138, 6487,  924, 3316, 3816, 3888, 5225, 5259, 1665, 2474, 3565, 2845,\n",
      "        2683, 2968, 1693, 3344,  479, 1312, 2447, 1831,  790, 5262, 2417, 1666,\n",
      "        2884, 1351, 2530, 6174, 2493, 1524, 1097,   60,  756, 3184, 1167, 2685,\n",
      "         595, 2955, 1825, 1210, 6036, 1688, 2781,  708, 3342, 1707,  787, 4298,\n",
      "        1385, 2553, 2473, 1023, 1594,  341, 2507, 2187, 2290, 2752, 6045, 6026,\n",
      "          55, 5706, 2501, 1716,  583, 2631, 2219, 2913, 4769, 2681, 2367, 2568,\n",
      "        5740, 2804, 1871, 2603, 2090, 2295, 3234, 2618, 2689, 5501, 1983, 3407,\n",
      "        2405, 2451, 1423, 2398, 1193, 2413, 2132, 4387, 1684, 4422, 1203,  492,\n",
      "        5621,  938, 2572, 1955, 2795,  187, 1369, 2035, 1744,  118, 3977, 2522,\n",
      "        2715, 2815, 5260,  665, 2463, 6124, 1548, 1297, 1024, 4299, 2880, 1593,\n",
      "        4142,  227, 2607,  961,  256, 4212, 1291, 1630, 2640,  866, 2107, 2627,\n",
      "        2258,  422, 1119, 5728, 2810, 2686, 4063, 1968, 1810, 2122,   38,  865,\n",
      "        6007, 5174,  354, 2894, 1559,  353,  257, 2971, 2654, 5796, 1518, 3012,\n",
      "        3138, 2421, 4610, 2160,  452, 1838, 2532, 3114,  234, 2301, 5033, 1246,\n",
      "        2380, 2877, 2152, 1168,  394,  730,  797, 2113, 1034, 5164, 1260, 2171,\n",
      "         247,   34,  220, 3039, 4493, 5293, 1196, 1352, 1396, 2946, 2771, 6092])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "import torch as th\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data_folder = \"data/\"\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "\n",
    "    # reshapes the numpy array to one column and whatever rows is required. \n",
    "    labels = labels.reshape(-1, 1)\n",
    "    print(f'labels inside encode_oneshot {labels}')\n",
    "\n",
    "    # tranfromss the labels or categorical array into a matrix of 0 and 1 that encodes where the data is presenst\n",
    "    # this is used to pass in models for understanding where the catgories are. \n",
    "    enc = OneHotEncoder()\n",
    "\n",
    "    #This method is used to fit the encoder to the data, learning the unique categories for each feature that will\n",
    "    #  be transformed during the encoding process.\n",
    "    enc.fit(labels)\n",
    "\n",
    "    # converst categoriacal data into a binary matrix. \n",
    "    labels_onehot = enc.transform(labels).toarray()\n",
    "\n",
    "    # returns this oneshot binary matrix. \n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "\n",
    "    # gets a matrix plugged in and sums up all the rows of each the matrix and stores them in a tensor which gest turned into an array. \n",
    "    rowsum = np.array(features.sum(1))\n",
    "\n",
    "\n",
    "    r_inv = np.power(rowsum, -1).flatten() # this performs an element-wise inverse on rowsum and then flatten the results to a 1-d array\n",
    "    r_inv[np.isinf(r_inv)] = 0. # checks if any of the values are infinity prompting them to be equal to zero. \n",
    "    r_mat_inv = sp.diags(r_inv) # construct a digonal sparse matrix using the array of r_inv\n",
    "    features = r_mat_inv.dot(features) # this multiples the new digonal matrix by the original features matrix. \n",
    "    if isinstance(features, np.ndarray):\n",
    "        return features # if features was a numpy array it returns the new matrix \n",
    "    else:\n",
    "        return features.todense() # if features matrix is not a numpy array it turns it into a dense matrix\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "\n",
    "\n",
    "    # the adj is a matrix that tells us which nodes are connecte dwith each other. \n",
    "    adj = sp.coo_matrix(adj) # this turns the adj matrix into a coo matrix which is used to save memory and better fro computation \n",
    "    # only saves the none zero objects in the matrix. \n",
    "    print(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    # \n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = th.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = th.from_numpy(sparse_mx.data)\n",
    "    shape = th.Size(sparse_mx.shape)\n",
    "    return th.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "\n",
    "def process_data_in_pyg(neigs):\n",
    "    d = defaultdict(dict)\n",
    "    metapaths = []\n",
    "    for mp_i, nei1 in enumerate(neigs):\n",
    "        dst_array_concat = np.concatenate(nei1)\n",
    "        src_array_concat = []\n",
    "        for src_id, dst_array in enumerate(nei1):\n",
    "            src_array_concat.extend([src_id] * len(dst_array))\n",
    "        src_array_concat = np.array(src_array_concat)\n",
    "        src_name = f\"target\"\n",
    "        dst_name = f\"dst_{mp_i}\"\n",
    "        relation = f\"relation_{mp_i}\"\n",
    "        d[(src_name, relation + \"-->\", dst_name)][\"edge_index\"] = th.LongTensor(np.vstack([src_array_concat, dst_array_concat]))\n",
    "        metapaths.append((src_name, relation + \"-->\", dst_name))\n",
    "        d[(dst_name, \"<--\" + relation, src_name)][\"edge_index\"] = th.LongTensor(np.vstack([dst_array_concat, src_array_concat]))\n",
    "        metapaths.append((dst_name, \"<--\" + relation, src_name))\n",
    "    g = HeteroData(d)\n",
    "    return g, metapaths\n",
    "\n",
    "\n",
    "\n",
    "def load_aminer(ratio, type_num):\n",
    "    # The order of node types: 0 p 1 a 2 \\r\n",
    "\n",
    "    # creates the path to aminer dataset \n",
    "    path = data_folder + \"aminer/\"\n",
    "\n",
    "    #loads the labels.npy into a np array \n",
    "    label = np.load(path + \"labels.npy\").astype('int32')\n",
    "    print(f'lable:{label}')\n",
    "\n",
    "\n",
    "    label = encode_onehot(label)\n",
    "    print(f'lable after encode oneshot {label}')\n",
    "    # load object that are seralized inside a numpy array \n",
    "    # these objects are from the authors. \n",
    "    nei_a = np.load(path + \"nei_a.npy\", allow_pickle=True)\\\n",
    "\n",
    "    # load object that are searilized inside a numpy array \n",
    "    # this is for relationships. \n",
    "    nei_r = np.load(path + \"nei_r.npy\", allow_pickle=True)\n",
    "\n",
    "    # Because none of P, A or R has features, we assign one-hot encodings to all of them.\n",
    "    # the fatures are added to the papers, authors and relationships \n",
    "    # the type-num is \"type_num\": [6564, 13329, 35890],\n",
    "    # so paper -> 6564, author -> 13329 -> relationships -> 35890\n",
    "    # spicy.sparse.eye create a identity matrix of size type_num \n",
    "    feat_p = sp.eye(type_num[0]) #return a tuple of the indices connected witht he value for example (2,2) 1, means there is a 1 in row 2, column2, this goes down to the size of the array. \n",
    "\n",
    "    feat_a = sp.eye(type_num[1])\n",
    "    feat_r = sp.eye(type_num[2])\n",
    "\n",
    "    #loads sparse matrix that have been stored in paper-author-paper, paper-relationship-paper, paper-\n",
    "    pap = sp.load_npz(path + \"pap.npz\")\n",
    "    prp = sp.load_npz(path + \"prp.npz\")\n",
    "    pos = sp.load_npz(path + \"pos.npz\")\n",
    "\n",
    "    # contains the indices of the nodes for the train, test, val\n",
    "    # .npy is a way to store numpy arrays into files\n",
    "    train = [np.load(path + \"train_\" + str(i) + \".npy\") for i in ratio]\n",
    "    test = [np.load(path + \"test_\" + str(i) + \".npy\") for i in ratio]\n",
    "    val = [np.load(path + \"val_\" + str(i) + \".npy\") for i in ratio]\n",
    "\n",
    "\n",
    "    label = th.FloatTensor(label) # make label a float tensor \n",
    "    nei_a = [th.LongTensor(i) for i in nei_a] # make neighboring area a tensor instead of a numpy array\n",
    "    nei_r = [th.LongTensor(i) for i in nei_r] # make neighboring area of relationships a float tensor intstead of a numpy array. \n",
    "\n",
    "    # go into the preporccess features. \n",
    "    print(\"\")\n",
    "    print(feat_a, feat_a, feat_r)\n",
    "\n",
    "    # passes the feat_a p, r sparse matrices with are idneitity matrices. there are tuples designenating row and column with a value next to it \n",
    "    # representing the value that is in the matrix. \n",
    "    feat_p = th.FloatTensor(preprocess_features(feat_p)) # this turns the matrix that is returned int a float tensor \n",
    "    print('')\n",
    "    print(feat_p)\n",
    "    feat_a = th.FloatTensor(preprocess_features(feat_a))\n",
    "    feat_r = th.FloatTensor(preprocess_features(feat_r))\n",
    "\n",
    "    print('')\n",
    "    print(pap)\n",
    "    pap = sparse_mx_to_torch_sparse_tensor(normalize_adj(pap))\n",
    "    prp = sparse_mx_to_torch_sparse_tensor(normalize_adj(prp))\n",
    "    pos = sparse_mx_to_torch_sparse_tensor(pos)\n",
    "    train = [th.LongTensor(i) for i in train]\n",
    "    val = [th.LongTensor(i) for i in val]\n",
    "    test = [th.LongTensor(i) for i in test]\n",
    "    print(train)\n",
    "    return [nei_a, nei_r], [feat_p, feat_a, feat_r], [pap, prp], pos, label, train, val, test\n",
    "\n",
    "def load_data(ratio=[20,40,60], type_num=[6564, 13329, 35890]):\n",
    "    data = load_aminer(ratio, type_num)\n",
    "    g, metapaths = process_data_in_pyg(data[0])\n",
    "    return data, g, metapaths\n",
    "\n",
    "\n",
    "data, g, metapaths = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2368, 2138, 5156, 3491, 2714, 6142,  488, 6265, 2353, 4638, 5611, 2293,\n",
      "        2356, 2688, 3396, 6465, 1085, 6375, 5318, 1407, 5205, 4700, 1697, 1913,\n",
      "         993, 1450,  370, 3744, 1906, 4568, 2407,   82, 5625, 2398, 4886,  793,\n",
      "        4599, 1141, 4467,  485, 5496, 2393, 6158, 1453, 4327,   97, 1211,  607,\n",
      "        1259, 5593,  814, 2971, 6206, 1223, 4948, 1839, 4755, 3838, 2901, 1411,\n",
      "        3423, 5619, 5056, 2908, 2675, 6281,  994, 3573, 4653,  557, 5775, 1562,\n",
      "         646, 3245, 1143, 1156, 4832, 6038, 3346, 2009, 4786, 5483, 6300, 6459,\n",
      "         621, 3055,   65, 4078,  445, 6000,  641, 6262, 5322,  379, 1728, 1640,\n",
      "        4464,  602,  909, 1663, 1599, 1852, 1833, 5684, 6388, 4768, 4843, 2399,\n",
      "         693, 4873, 3494, 4677, 3725, 4290, 5874, 5860, 1078, 3182, 5598, 2886,\n",
      "        1519, 2119, 6517, 3577, 4108,  567, 4872, 3407,  952, 6165, 2389, 1419,\n",
      "        5913, 5965, 5060, 2254, 4064, 5552,  729,  321, 2352, 4060,  713, 1521,\n",
      "        2877, 1651, 3467, 2283, 3324,  348, 1297, 2619, 4215, 5253, 4192, 1646,\n",
      "        6425, 5521, 5492, 4243, 5798, 3139, 3779, 3376, 1664, 1332,  830, 6156,\n",
      "        5130, 3197, 3911, 4595, 4992, 4358, 6018, 1449, 1071, 3403,  593, 1510,\n",
      "        3824, 1284,   43, 5187, 3028, 1693, 1855, 5890, 5327, 5438, 1115, 3481,\n",
      "        4572, 1113, 6163, 3569, 5214, 4097, 4607, 1354, 6146, 5164, 1292, 6110,\n",
      "        6411, 1558, 5181,  855, 1507, 1128, 3115, 1936, 2130, 5866, 1759,  466,\n",
      "        5627, 3976,  622, 4283, 4866,  572, 5900, 1090, 3134, 5376, 5934, 5307,\n",
      "         270, 6134, 1066, 5870, 2531,   94, 6408, 3966, 5525, 5382,  423, 1695,\n",
      "         215, 3858,  665, 3077, 6549, 5618, 1097, 1723, 1735, 2117,   51, 1045,\n",
      "         931, 5428,  777, 3891, 3289, 5185,  104, 3335, 1345,  408, 3029, 1371,\n",
      "         410, 4525, 1950, 4391, 6201, 4349, 4704, 1344, 1742, 1545, 5824,  753,\n",
      "        6008, 1837, 1500, 5652,  477, 4258, 3236, 2549, 5544,  207, 6308, 1007,\n",
      "        5112, 2808, 1000, 2825, 6405, 2818, 3475, 5706, 4414, 5137, 3748, 5014,\n",
      "        3749,  847, 5042, 5803, 1755,  411, 6349, 2767, 1341,  668, 1420, 6452,\n",
      "        3162, 5281, 6221, 3010, 1315, 5473, 2182, 4346,  978, 4411,   84, 4810,\n",
      "        5608, 2692, 6108, 4626, 6316, 2831, 1610, 1504, 4302, 1948, 2147,  157,\n",
      "        1219, 5838, 4148, 4674, 5804, 2836, 4885, 3876, 1820, 1076,  853, 4443,\n",
      "        4889, 4929, 5015, 6385, 2334, 5359,  857, 5516, 1830, 2773,  310, 3626,\n",
      "        2830, 5597, 4829, 6225, 1098, 4033, 3212, 3922, 2110,  171, 4341, 4655,\n",
      "        4669, 5616, 4463, 6512, 6443, 4094, 4712, 1481, 3751, 2040,  656, 3199,\n",
      "          50, 3451,  730, 5843,  195, 2963, 3469, 4452, 4432,   40, 3114, 4773,\n",
      "         373, 5848, 2153, 5719, 5448, 4484, 4099, 1471, 5341, 4615, 6093, 1886,\n",
      "        3572,   17, 4405, 3642,  543, 3585, 4536, 4693, 6034, 2335, 2585, 3253,\n",
      "        3477, 4365,  605, 3959, 5868, 5982, 5734, 5789, 1831, 4427, 6103, 4660,\n",
      "        6253, 5573, 1247, 2292, 5192, 4330, 6313, 2355, 4498, 3516, 3416,  864,\n",
      "         524, 3405, 4890, 5191, 3554,  163, 6009, 3937, 3001, 3233, 1355, 6094,\n",
      "        1391, 5733, 1025, 2476, 6046, 1992, 3201, 1118, 3484, 4800, 2804,  405,\n",
      "        3894, 3122, 2852, 3936, 1348, 5065, 3932, 5198, 4909, 6353, 2617, 1442,\n",
      "        1485, 4101,  245, 3005, 5629, 3351, 5019, 3061,  984, 6137, 5425, 1800,\n",
      "        3716, 5727, 3627, 2486, 1882, 6329, 6111, 1938,   29, 5712, 6273, 4191,\n",
      "        3209, 2017, 1304, 1681, 4024, 3874, 3482, 1815, 4030,  671,  775, 2047,\n",
      "        2051, 3556, 5100, 1658, 1094, 1183, 1232, 4281, 5563, 4812, 4982, 3214,\n",
      "        2768, 2923, 1632, 5710, 2733, 2518, 3719, 3440, 2807, 1968,  376, 2902,\n",
      "        2690, 1570,  912, 4324, 2552,  653, 4831, 5614,   47, 3843, 5274, 2196,\n",
      "        1718, 6215, 6457, 5257, 2493, 2879, 1148, 5202, 4165, 1535, 1743, 3306,\n",
      "        5343, 1764, 5877, 3510, 4589, 3165, 2039, 4996,  174,  134, 5213, 4756,\n",
      "        1655,  407, 1753, 5950, 2221, 4470, 2053, 3791,  906, 2443, 4207, 2229,\n",
      "        4691,  314, 1791, 6170,   19, 4561, 4345, 2977, 4127, 5650, 6502, 4362,\n",
      "        1772,  419, 1705, 3865, 6213, 3371, 6161, 2494, 1818, 4662, 6541, 2563,\n",
      "        4403, 1456, 2948, 4190, 2264, 2793, 3528, 2980, 1846, 2055, 4703, 3782,\n",
      "        2647, 1999,  212, 6051,  448, 1931, 5364, 3951, 6242, 4350, 5446, 3480,\n",
      "        1832, 2573, 5853,  734, 3240,  108,  998, 3379, 5596, 4076, 4724, 4750,\n",
      "        6277, 2621, 2672, 4651, 1010, 2911, 4169,   13, 2932, 3988, 3200, 3427,\n",
      "         629,  893, 1351, 4537, 2165, 4262, 2260, 3108, 4716, 5275, 2882, 3713,\n",
      "         329, 1412,  956, 6483, 6106,  739, 3996, 4124, 4881,  391,  406,   62,\n",
      "        3798, 1684, 4688, 3875, 4083, 4911, 1267, 5186, 2094, 2822, 3384, 6535,\n",
      "        5990, 5305, 2594, 3568, 6421, 5517, 2802, 4475, 4381,  404, 5831, 4900,\n",
      "        6073,  133, 4303, 4780, 3003, 1910, 2431, 1730, 4898,  783, 4347, 6246,\n",
      "          39, 3859, 3262, 4801, 4121, 2101,  838, 1081, 5883, 2866,  401, 3130,\n",
      "        1418, 4128, 3907, 1293, 1246, 1029, 1438, 4085, 3050, 4656, 6010, 6259,\n",
      "        5760,  917, 5781, 1312, 4998, 2453, 1722,  371, 3192, 1998, 2914, 4461,\n",
      "        3814, 5201, 5894, 4271, 3574, 4825, 2514, 6048, 6120, 4878,  663, 3047,\n",
      "        5711, 2550, 4462, 5818, 4955, 4641, 2440, 6419, 3658, 3883, 3141,  116,\n",
      "        2481, 2599, 6442,  619, 5402, 4116, 2170, 4182, 2491, 6031, 4608, 5769,\n",
      "        4680, 1039, 1494, 5339, 1969, 1673, 6230, 3497, 2839, 5035, 2088, 2870,\n",
      "        6377, 4728, 5280, 6219, 4624, 4880, 1009,  834, 1002, 6148, 5980, 6444,\n",
      "        5833, 2235, 2777, 5385, 3473,  589,  973, 4433, 2832,  612, 4310, 3656,\n",
      "        6075, 5663, 4285, 3303, 2374, 2548, 4852, 6268,  434,   20,  936, 1549,\n",
      "         549, 4113, 3570, 3132, 1185, 3919,  561,  872, 1860, 2448,  578, 1606,\n",
      "        4809, 5484,  255,  439, 3647, 4388, 2116, 4552,   56, 3424, 5183,  842,\n",
      "         449, 3279, 3203, 2234, 6431, 5996, 3452, 2127,  496,  426, 1044, 5888,\n",
      "        4134, 2941, 6026, 4298, 4891, 1946, 2680, 4308,   21, 5290, 4223, 2388,\n",
      "        4550, 5634, 4429, 2306, 6181, 5839, 4112,  259, 2758, 3342,  887,   10,\n",
      "        3186, 4449, 3489, 1708,  179, 1195, 6059, 4609, 2798, 1958, 6005, 2432,\n",
      "        5784, 3632, 2208, 3248, 2230, 1649, 6492, 5777,  679, 3923,  740, 3783,\n",
      "        3685, 3136, 5296, 2584, 4204, 5610, 4894, 4175, 3098, 3317, 5720, 2128,\n",
      "         334, 6440, 5603, 3105, 3070, 2074, 2018, 3394, 3701, 3260, 3544, 2903,\n",
      "        1433,  714, 1252, 3735, 3849, 6555, 2553, 3487, 3339, 1383,  576, 6252,\n",
      "        6128, 2747, 2107, 3372, 3140, 2921, 1214, 2762, 3281,  354,  716, 4054,\n",
      "        3759, 4930,  545, 1944, 6065, 1191, 2906, 6030,    4, 4382, 1709, 4567,\n",
      "         888, 4035, 1555, 6274, 1530, 6491, 5158,  623, 2290, 4293, 2382, 4130,\n",
      "        2937, 4739, 4847, 5493]), tensor([6010, 5713, 4522, 5849, 3099, 5361, 6114,  384, 1818, 5102, 5152, 5566,\n",
      "        4385, 5424, 3539, 6339, 2281,  661,  794, 4645, 1555,  492, 4497,  398,\n",
      "        1093, 4025, 2077, 3501,  678, 6299,  267, 5756, 1729, 2464, 3301, 4298,\n",
      "         557, 2233, 4433, 4610, 1390, 1927, 4758, 6231,  782,  736, 5455, 2203,\n",
      "         869, 5853,  859, 2796, 3564,  159, 6463, 1277, 1479, 6453, 5602, 5452,\n",
      "         620,  424, 1771, 3292, 3407, 3273, 3372, 4357, 1538, 1983, 3138,  985,\n",
      "        3929, 1576, 5720, 4085, 4275,  232, 4761,   42, 5103, 5790, 3268, 5583,\n",
      "        3874, 3661, 4174, 5949, 5979, 4841, 3791, 1750, 1665, 5323, 2894, 2924,\n",
      "         776, 4168, 6542, 2635,  539, 1234, 4322, 3007, 3754,  406, 6347,  790,\n",
      "        2902, 3388, 2465,  103, 2773, 5826, 2293,  374, 1552, 4379, 6451, 5091,\n",
      "        1346, 3294,   43, 1660, 2090,  583, 4211, 2930, 5631,  670, 1104, 6273,\n",
      "        1385, 3383,  709, 5691, 3238, 2424, 4495,  696, 5354, 4467, 2785, 6391,\n",
      "        5699, 3977, 1712, 6142, 4601, 5504, 1102, 3946,  847, 2518, 2774, 5531,\n",
      "        1853, 4302, 5569, 1963, 2298, 3387,   83,  656, 2989, 1732,  409, 6062,\n",
      "         387, 5257, 2430, 6286,  596, 3925, 5806, 2608, 5678, 3038, 1351, 4435,\n",
      "        1171, 3171, 1965, 5371, 1412,  702, 2534, 1068, 4253, 2006, 5311, 5656,\n",
      "        2317, 3811, 2270, 1405, 2688, 6270,  395, 6037, 2558, 3419, 2360, 4000,\n",
      "        4729, 6443, 2612, 1103, 2322, 4162, 4354,  693, 5372, 2825, 1173, 2145,\n",
      "        3014, 3611,  694, 4937, 4160, 3327, 4378, 2563, 2274, 2857, 5541, 3881,\n",
      "        5131, 5010, 4309,  392,  538, 3852, 3845, 3743, 6357, 6472, 1130, 4902,\n",
      "         673, 5004, 3853, 5628, 2917, 4447, 6325,  605,  918, 5469, 6390, 2920,\n",
      "         283, 6497, 3843, 6023,  336,  925, 3058, 2402, 3008,  370, 6462, 1035,\n",
      "        5173, 2875, 4362, 3379, 6258, 4501, 6237, 2167, 5892, 2542,  623, 5052,\n",
      "         408, 3325, 5664, 4575, 4473, 1858,  247, 2373,  806, 2987, 3402, 3233,\n",
      "        1540, 1942, 1242, 5153, 6562,  338, 2564, 3149, 5862, 5847,  204, 5711,\n",
      "         219,  569, 5302, 1875, 3450, 3029, 1695,  117, 2054, 6482,  300, 5499,\n",
      "        2222, 2116, 6474, 5054, 3091, 5615, 1024, 1822, 5050, 2488, 4137,  382,\n",
      "        5773, 3924, 3987, 1039, 5690, 4833,  514, 2398, 3061, 6188, 6139,  170,\n",
      "        4048,  791, 2583, 5459, 6379, 4264, 2911, 4866, 2668,  221,  632, 3016,\n",
      "        6138, 6523, 5435, 4563,  934, 4030, 3285, 5820, 1596, 5697, 5193,  573,\n",
      "         724, 3469, 2445, 4636, 6056, 1683,  468, 4945,  150, 6204,  651, 4540,\n",
      "        4003, 3355, 6251, 2676, 4439, 1235, 4712, 1458, 1772, 3711, 5975,  585,\n",
      "        1793, 1929,  588, 5399, 3528, 2288, 2720, 3377, 1566, 5801, 4073, 5521,\n",
      "         720, 2124, 1192, 3188, 4093, 4176, 1163, 4542,  584, 1760,  353, 2064,\n",
      "        4940, 4619, 1442, 3214, 4816, 4254, 6076, 3890, 1361, 5197, 5492, 3349,\n",
      "        5401, 2470, 2185, 1726,  706, 1106, 5780, 1505, 5877, 3763, 3608, 1918,\n",
      "         321,  900, 2787, 4769, 5066, 5462, 1533,  771, 4151, 2889, 1915, 3687,\n",
      "        2071, 1291, 1321, 2427, 5475, 4957, 2194,  581, 1009, 1897, 2044, 2502,\n",
      "        1746, 3928, 1624, 3122, 1391, 1178,  754, 4189, 1503, 2906, 6513, 6461,\n",
      "        1096,  428,  282, 1641, 4927, 5807,  177, 2282, 3487,  993,  701, 1216,\n",
      "         926, 2850, 5252, 2543, 2104, 4414, 5722, 3019, 4678, 3733,  207, 1987,\n",
      "        2526,  233, 2332, 4157,  936, 1840, 2205, 1644, 4557, 4008, 5212, 2610,\n",
      "        1127, 5174, 4226, 5772, 6557,  862, 3919, 3728, 4135, 3860, 1630, 3792,\n",
      "        5089, 5649, 5266, 3993, 4388, 6480, 2371,  453, 4436, 6493,  848, 4066,\n",
      "        6143, 5633, 1369,  953, 4217, 1633, 1274, 5727, 1400, 3199, 5092, 4306,\n",
      "        1273, 3272, 1251, 1254, 2024,   95, 1924,  715, 1407, 6377, 5654, 1994,\n",
      "        4830, 4581, 1769, 4690, 6445, 4548, 1742, 3361, 2575, 2017, 4692, 3565,\n",
      "        4213, 5937, 1262, 3553,    9,  210, 6279,  852, 4288, 6094, 2681, 6144,\n",
      "        5638,  988,  293,  153, 2955, 6047, 4784, 5768, 2707, 5669, 3429, 6063,\n",
      "         954, 6302,   52, 1000, 4031, 3696,  617, 5988, 5616, 5770, 5787, 5962,\n",
      "        6287,  629, 3727, 5552, 2588, 3862, 3749, 1357,  963, 4656, 2622, 3332,\n",
      "        5712, 5042, 6075, 3067, 2212, 4149, 3073,   38, 6402, 1425, 2719, 6090,\n",
      "          39, 6180, 6388,  352, 1041, 3340, 3232, 1194, 3787, 4115, 4897, 4187,\n",
      "        2277, 1894, 1951,  861, 3909, 5111, 3810, 2531, 2690, 1891, 6034, 1196,\n",
      "        6045, 5353, 6386, 5443, 6120, 4389, 4425, 3957, 3066, 1626, 1381, 2343,\n",
      "        4060, 1885, 4204, 4909, 1005, 1696, 1091, 5149, 3385, 5412, 4287, 3120,\n",
      "        3665,  360, 1585, 3796, 2384, 1398,  967, 4765, 5719, 2623, 5618, 2246,\n",
      "          27, 5192,  529, 3013, 4715, 2574, 1223, 6510, 1708, 6410,  606, 4094,\n",
      "        6514, 5805, 6507, 1261, 2390, 3119, 4403,  858,  996, 4560, 3115, 3079,\n",
      "        3347, 2378, 6362, 3074,  851, 3808,  866, 2192, 6018, 1944, 4454, 5377,\n",
      "        6007, 2207, 3778,  345, 4624, 1488, 2146, 5918, 4027, 4393, 2268, 4954,\n",
      "        2450, 3775, 2230, 6011,    6, 5318,  894,  933, 5432, 5920, 6101,   63,\n",
      "         246, 2766,  163, 1218, 5375, 2358, 1565, 4002, 3780, 6292, 3675, 2484,\n",
      "        6496, 6084, 5960, 4627, 3878, 5799, 2791, 3767, 5582, 5216, 1854,  775,\n",
      "        3328, 1542, 6547, 4188, 3446, 5863,   35, 4892, 1382,  403, 3821, 3053,\n",
      "        2191, 1449, 2820, 4689, 3098, 2705, 5893, 3812, 2693,  659, 5800, 3415,\n",
      "        4864,   12,   44, 2223, 6485, 2819, 3152, 3187, 4751, 1367,  765, 1973,\n",
      "        2824,   46, 2591, 1667, 6210, 5710, 6413,  793,  788, 5846,  315,  865,\n",
      "        5447, 5420, 1079, 2457,  385,  648,   82, 2597,  912, 3425, 5278, 3229,\n",
      "        2284, 5704, 4117, 6385, 1270, 2163, 5839, 3078, 2813, 1514,  501, 1355,\n",
      "        4462, 1959, 6190,  128, 4175,  798, 2008, 1462, 3395,  505, 2113, 3075,\n",
      "        3823, 4124, 1743, 5388, 1526,  813, 4396,  805, 1773, 5177, 6490, 2052,\n",
      "         718,  289, 2595, 5187, 3705, 1429,  279, 2533, 1561,   79, 5150,   94,\n",
      "        5130, 3945, 6535, 5635, 2808, 3484, 1539,  950, 4018, 3885, 1513, 4807,\n",
      "        4330, 5422, 1011, 1548, 5068, 3044,  881, 2602, 4308, 5429,  496, 2420,\n",
      "        5743, 2291, 1674, 5804, 4550, 4064, 1768, 4041,  687, 3896,  308, 5124,\n",
      "        1832, 6140, 6151, 2899, 4118, 5927, 3626, 1207, 4110, 2940,  618,  273,\n",
      "        5813, 4485, 3107, 6353, 5109, 3492, 2341, 2330, 4603, 2039, 4100, 4358,\n",
      "        2297, 4347, 5878, 4076, 5200, 4092, 2798, 2001, 5185, 2789, 1157, 1681,\n",
      "        1796, 1805, 2994, 1993, 2986, 1304, 3903, 3145, 1867, 1909, 1145, 1781,\n",
      "        5074, 2189, 6015,  332, 5284,  945,  404,  475, 3400, 2667, 1036, 6437,\n",
      "        2122, 1547, 2409, 2214,  503, 2744, 2672, 4413, 3417,   55, 3783, 4829,\n",
      "        5922, 5162, 5752, 6073, 4883, 2060, 2166, 4044, 4285, 3751, 2922, 1089,\n",
      "        4541, 3259, 3864, 1148]), tensor([2786, 3653, 1055, 4853, 4901, 1942,  693, 2371, 2740, 1766, 4261, 5871,\n",
      "        5389, 1771, 2904, 3058, 4849, 4981,  698, 4599, 2769,  236, 2504, 5581,\n",
      "        3472, 4663, 5163, 6538,  954, 2276, 3091, 1100, 3417,  651, 6283, 1879,\n",
      "         363, 4308, 5057, 6179, 3626, 3274, 6009, 5847, 4949, 5018, 4613,   16,\n",
      "        2816, 3893, 6451, 1412, 6307, 5218, 3010, 3761,  154, 2363, 1275, 5372,\n",
      "        4897, 1122, 4094, 5440, 1448, 1718, 2428, 4593, 2446, 4935,   11, 3630,\n",
      "        5645, 1839, 3113,  219, 6164, 1277,  297,  159, 2312, 3951, 6333, 5549,\n",
      "        1668, 1397, 1384, 1776, 1468,  590, 3658, 5487, 2651,  674, 4115, 1230,\n",
      "         910, 5539, 5994, 5977, 5507, 4814, 5947, 3348, 6463, 4406, 2517, 1717,\n",
      "         771, 2407, 1481, 6228, 3793, 1962, 1840, 2604, 1775, 5388, 3938, 4902,\n",
      "        2115,  802, 4239, 3773, 1702, 5957,  393, 2550, 3571, 5492, 6161, 4485,\n",
      "        4963,  625, 6290, 5982, 2634, 2859, 5829,  930, 3266, 2630, 5757, 5517,\n",
      "        5112, 4296, 5398, 1455, 1247, 3443, 2514, 4715,  644, 6278, 2600, 6370,\n",
      "        6141, 6420, 4864,  379, 5265, 1614, 4273, 1605, 3500, 4584, 1375, 5776,\n",
      "         919, 1161, 4777, 3663, 5553, 1256, 3579, 1679, 2548, 6337, 1843,   32,\n",
      "        5881, 2972,  725, 5719, 2706, 2326, 5958,  965, 6437, 5801, 6504, 4025,\n",
      "        4297, 2338,   12,  860, 2063, 2947, 4437,  705, 6466,  926, 2981,  348,\n",
      "         962, 5478, 1453, 1733, 5144, 3660, 2013, 3065, 3404,   72, 3548, 2953,\n",
      "        5196, 1035, 5006, 1176, 5903, 6522, 4947, 3719, 1892, 1454, 1847, 4639,\n",
      "         317, 4851, 1929, 4060, 1361, 3779, 1166, 1374, 1607, 4986, 1424, 3795,\n",
      "        4400, 3969, 3801, 4102,  714, 6017,  840,  736, 1057,  921, 5557, 5165,\n",
      "        3007, 2502, 1295, 1163, 2935, 5408,  263, 6315,  125, 3933, 2552, 3014,\n",
      "        6514, 3741, 3096,  201, 4204, 5708, 6364, 5950, 1510, 5875, 1915,  826,\n",
      "        2279, 4287, 4834, 5343, 3203, 1764, 4603, 5545, 2142, 5541, 2198, 5368,\n",
      "        3911, 5020, 1221, 1849, 3281,  287, 3676, 5960, 1021,  489, 1681, 4830,\n",
      "        6079, 2281, 5233,  464, 4798, 5434, 6349, 1288, 6479, 5766, 4469, 1067,\n",
      "         702, 2934, 3380, 3140, 3207, 6428, 1600, 2589, 6497, 2346,   64, 5291,\n",
      "        3620, 5587, 2183, 1232, 2747, 1077, 1633,  535, 3133, 3806,  260, 4270,\n",
      "        3413, 6521, 3312, 2941, 5462, 2085, 2544, 3313, 5384, 4209, 3909, 5274,\n",
      "        5153, 3467, 5402, 2869,  799, 5378, 5494, 2006,  281, 6255, 2826, 1602,\n",
      "        2633, 5344, 4786, 1184,  470, 4906, 6185, 3634, 2729,  389, 6404,   20,\n",
      "        5185,  442, 2021, 3143, 6080,  237, 1667,  724, 2523, 4576, 2123,  443,\n",
      "         197, 5257, 2476, 4954, 5620, 6406, 3508, 5363, 1438, 2437, 3965, 6166,\n",
      "        1379, 5971, 1757, 3387, 6304, 2468, 2912, 5949, 5182, 5827, 4258, 5476,\n",
      "        5880,  195, 6474,  946, 3304, 2911, 2844,  862, 4286, 3102, 2527, 5119,\n",
      "        1520, 6407, 1573,   99, 1123, 1149, 2099,  759, 2054,  146, 4505,  496,\n",
      "        2817,  315, 1924, 4945, 4089, 4247, 2466, 4623, 6207, 5150, 2927, 5715,\n",
      "        4138, 4642, 3192, 6097, 3466, 1012, 2841, 2659,  994,  855, 3386, 3444,\n",
      "        3474,   29, 6383,  549, 1073, 4987, 6154, 6286, 4855,  692,  477, 2133,\n",
      "        2982,   14, 1301,  541, 6125, 3006, 2488, 3572, 1656, 1692, 4235, 2965,\n",
      "        3128, 3112, 3367, 1941, 2382, 1765, 4739, 1837, 5961, 2199, 3122, 4561,\n",
      "        5142, 2644, 6209,  542, 2408, 2318, 4771, 4597, 5724, 1677, 1341, 4101,\n",
      "        4013, 1125, 2218, 1901, 3260, 1228, 1813, 4108, 1078, 2569, 4393, 6336,\n",
      "        1439, 1036, 2261, 2916, 3385, 1856,  198, 6224, 1349, 1982, 3415, 5483,\n",
      "        5468,  568, 4728, 2274, 1897, 1422, 5187, 1014, 1712, 6269, 4214, 2638,\n",
      "        1615, 3406, 1209, 4455, 5983,  700, 5373, 4004, 1814, 5162, 1188, 2311,\n",
      "        2672, 1977, 4778, 3959,  589,  150,  141, 1491, 5887,  766, 3691, 4280,\n",
      "        3958,  828, 3769, 4316, 5145,  444, 2485, 5536, 3021, 3321, 5964, 4586,\n",
      "        1883, 2710, 5771, 6015, 6251, 3130, 3651,  397, 6167, 5685, 2369, 3794,\n",
      "        3690, 3396,  616, 3427,  561, 1456, 1253, 5686,  243, 1906, 4525, 6150,\n",
      "        5846, 1213, 2095, 1299, 2584, 6270,   44, 3341, 3668, 4805, 4503, 6263,\n",
      "        3036, 1313,  555, 2920, 2263, 6241, 5848, 4665, 4552, 6429, 2249, 2656,\n",
      "        5012, 1272, 2303, 6057, 6430, 5789, 2179, 6405, 1571,  336,  515, 3364,\n",
      "        1285,  543, 1898, 4577, 5152, 3315, 4845, 2698, 4850, 2891, 3681,  540,\n",
      "        5857,  856, 5295, 3562, 5425, 3693, 1570, 6035, 2699, 4321,   98, 2854,\n",
      "        1841, 5938,  687, 3550,  465,  466,  514, 4139,  769, 4288,  602,  551,\n",
      "        1415, 1174, 2296, 2406,  148, 5902,  283, 1311, 5054,  791, 1094, 6323,\n",
      "        5504, 5919, 4635, 2419, 3912, 4170, 3684, 4606, 2126, 4428, 3609, 2141,\n",
      "        6081, 1052, 3988, 5406, 1134, 2461, 2723,   90, 5664, 6320,   78, 4680,\n",
      "        3904, 4701, 1009, 1326, 6199, 1705, 4993, 6112, 4373, 1806, 5790, 5751,\n",
      "          33, 4389, 6431, 3258, 4399,  206, 1511, 5361, 4234,  631, 5997, 4517,\n",
      "         889,  868, 2718, 3305, 5414, 3092, 3762,  777, 4602, 1191,  897, 5335,\n",
      "        2368, 2325,  820, 2096, 4179, 3735,  137, 2923, 3819, 3233,  746, 3730,\n",
      "        5691, 5362, 4837,  497, 2047, 6226, 4918, 3881, 2266, 6044, 2166, 5913,\n",
      "        5979, 5758,  136, 2896, 4419, 1080, 3640, 2713, 6149, 4541, 3702, 4617,\n",
      "        6424, 5216, 5495, 2779,  404, 1414, 6122, 2893, 3776,  883, 2725,  988,\n",
      "        2440, 3768, 5770, 1934, 1778, 4643, 6409, 5269, 5127, 4692, 2573, 4479,\n",
      "        5014, 4225, 4402, 2456,  825,  264, 3069, 4036, 1500, 4420, 3860, 1992,\n",
      "        4359, 3319, 1674, 4678, 4346, 3960, 2168,  432, 1537, 4059, 5350, 4215,\n",
      "         717, 4383, 5448, 4506, 4388,  240, 4431, 1136, 3732, 6503, 5634, 3583,\n",
      "         914, 1047, 4351, 1039, 6458, 5809, 4486, 5245, 2843, 5001, 5466, 1091,\n",
      "        2068, 1137, 1142,  999,  117, 6350, 1637, 5631, 6559, 2901, 4594, 1523,\n",
      "        2046, 1788, 2985, 5569, 4965, 2491, 4352, 5183, 1028, 5459, 3884, 5544,\n",
      "        5754, 5746, 3737, 6120,  634, 3744, 6373, 6136, 6082, 2528, 1005,  351,\n",
      "        4519, 5547,  593, 1179, 5297, 3005,  878, 5175,  182, 3276, 1584, 2560,\n",
      "        5497, 6376, 3923, 3247, 2805, 3088,  511, 2104, 3090,  481, 5660, 3469,\n",
      "        1515,  978, 1358, 3452,  104, 3071,  381, 5657, 6257,  456, 4572, 4441,\n",
      "        4219, 1611, 4116, 6169, 5186, 1479,   96,  931, 1528,  875, 5317, 2987,\n",
      "        2186, 2944, 3646, 4703,  913, 1598,  421, 3688, 3189, 5865, 3652, 1371,\n",
      "        5404, 3330, 4858, 3836, 1882, 3345, 5966, 6347, 3569, 1155, 2789,   15,\n",
      "        2598, 4266,  983, 5375, 2565, 4174, 2602, 2471, 4662, 3542, 6165, 2050,\n",
      "        6321, 3382, 1505, 2351, 2587, 5143, 3682, 6163, 4758, 2997, 1768, 4891,\n",
      "         391, 2304, 4776,  977, 4180, 6056, 5387,  180, 5542, 1891, 3715, 1150,\n",
      "        1863, 2954, 6003, 1696])]\n"
     ]
    }
   ],
   "source": [
    "print(data[7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  (target, relation_0-->, dst_0)={ edge_index=[2, 18007] },\n",
      "  (dst_0, <--relation_0, target)={ edge_index=[2, 18007] },\n",
      "  (target, relation_1-->, dst_1)={ edge_index=[2, 58831] },\n",
      "  (dst_1, <--relation_1, target)={ edge_index=[2, 58831] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('target', 'relation_0-->', 'dst_0'), ('dst_0', '<--relation_0', 'target'), ('target', 'relation_1-->', 'dst_1'), ('dst_1', '<--relation_1', 'target')]\n"
     ]
    }
   ],
   "source": [
    "print(metapaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
