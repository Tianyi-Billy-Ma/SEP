{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, MLP\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os \n",
    "\n",
    "cwd = os.getcwd()\n",
    "cwd = os.path.join(cwd, 'data')\n",
    "\n",
    "dataset = Planetoid(root=cwd, name='Cora')\n",
    "dataset1 = Planetoid(root=cwd, name='CiteSeer')\n",
    "dataset2 = Planetoid(root = cwd, name= 'PubMed')\n",
    "\n",
    "datasets = [dataset, dataset1, dataset2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class GCN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super().__init__()\n",
    "#         # Pre-process normalization to avoid CPU communication/graph breaks:\n",
    "#         self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "#         self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = F.dropout(x, p=0.5, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GIN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(2):\n",
    "            mlp = MLP([num_features, hidden_channels, hidden_channels])\n",
    "            self.convs.append(GINConv(nn=mlp, train_eps=False))\n",
    "            num_features = hidden_channels\n",
    "\n",
    "        self.mlp = MLP([hidden_channels, hidden_channels, num_classes], norm=None, dropout=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        return self.mlp(x) \n",
    "\n",
    "    \n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_channels, heads=heads, dropout= 0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, num_classes, heads=heads, dropout=0.6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, optimizer, data, train_mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if model == GCN:\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "    else: \n",
    "        out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(logits[mask], data.y[mask])\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    correct = pred.eq(data.y[mask]).sum().item()\n",
    "    accuracy = correct / mask.sum().item()\n",
    "    return accuracy, loss.item()\n",
    "\n",
    "def split_indices(data, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8):\n",
    "    indices = np.arange(data.num_nodes)\n",
    "    train_size = int(train_ratio * data.num_nodes)\n",
    "    val_size = int(val_ratio * data.num_nodes)\n",
    "    test_size = int(test_ratio * data.num_nodes)\n",
    "    \n",
    "    train_indices, temp_indices = train_test_split(indices, train_size=train_size, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=test_size, random_state=42)\n",
    "    \n",
    "    train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_indices] = True\n",
    "    val_mask[val_indices] = True\n",
    "    test_mask[test_indices] = True\n",
    "    \n",
    "    return train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mask(mask, reduction_ratio):\n",
    "    num_true = mask.sum().item()\n",
    "    num_to_keep = int(num_true * (reduction_ratio))\n",
    "    \n",
    "    true_indices = mask.nonzero(as_tuple=True)[0].numpy()\n",
    "    np.random.shuffle(true_indices)\n",
    "    \n",
    "    selected_indices = true_indices[:num_to_keep]\n",
    "    \n",
    "    new_mask = torch.zeros_like(mask)\n",
    "    new_mask[selected_indices] = True\n",
    "    \n",
    "    return new_mask\n",
    "\n",
    "def adjust_masks(data, train_reduction=0.1, val_reduction=0.1, test_reduction=0.8):\n",
    "    train_mask = data.train_mask\n",
    "    val_mask = data.val_mask\n",
    "    test_mask = data.test_mask\n",
    "    \n",
    "    train_mask = reduce_mask(train_mask, train_reduction)\n",
    "    val_mask = reduce_mask(val_mask, val_reduction)\n",
    "    test_mask = reduce_mask(test_mask, test_reduction)\n",
    "    \n",
    "    return train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiment(model_class, num_features, num_classes, data, num_runs=5, epochs=300):\n",
    "    for dataset in datasets:\n",
    "        accuracies = []\n",
    "        for _ in range(num_runs):\n",
    "            print(f\"Run {_ + 1}/{num_runs} on dataset {dataset.name}\")\n",
    "            train_mask, val_mask, test_mask = split_indices(data)\n",
    "            \n",
    "            if model_class == GCN:\n",
    "                model = model_class(num_features, hidden_channels=256, num_classes=num_classes).to(device)\n",
    "            model = model_class(num_features, hidden_channels=256, num_classes=num_classes).to(device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "            \n",
    "            best_val_acc = 0.0\n",
    "            best_model_state = None\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                train_loss = train(model, optimizer, data, train_mask)\n",
    "                val_acc, val_loss = evaluate(model, data, val_mask)\n",
    "                test_acc, test_loss = evaluate(model, data, test_mask)\n",
    "\n",
    "                if epoch % 200 == 0:\n",
    "                \n",
    "                    print(f\"Epoch: {epoch + 1}/{epochs} | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "                \n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_model_state = model.state_dict()\n",
    "            \n",
    "            if best_model_state:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            test_acc, _ = evaluate(model, data, test_mask)\n",
    "            accuracies.append(test_acc)\n",
    "        \n",
    "        mean_acc = np.mean(accuracies)\n",
    "        std_acc = np.std(accuracies)\n",
    "        \n",
    "        print(f\"\\nMean Accuracy over {num_runs} runs: {mean_acc:.4f}, Std Deviation: {std_acc:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora\n",
      "\n",
      "<class '__main__.GCN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9586 | Val Acc: 0.4191 | Test Acc: 0.4501\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8273\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9685 | Val Acc: 0.4485 | Test Acc: 0.4783\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8283\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9507 | Val Acc: 0.4559 | Test Acc: 0.4751\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8269\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9513 | Val Acc: 0.4926 | Test Acc: 0.4834\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8269\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9336 | Val Acc: 0.4706 | Test Acc: 0.4894\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8269\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8260, Std Deviation: 0.0005\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9395 | Val Acc: 0.3750 | Test Acc: 0.4044\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7904 | Test Acc: 0.8292\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9483 | Val Acc: 0.5294 | Test Acc: 0.5346\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8246\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9471 | Val Acc: 0.4228 | Test Acc: 0.4409\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8278\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9410 | Val Acc: 0.4890 | Test Acc: 0.4945\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8310\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9546 | Val Acc: 0.3860 | Test Acc: 0.4317\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8301\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8283, Std Deviation: 0.0020\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9503 | Val Acc: 0.5147 | Test Acc: 0.5305\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8310\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9402 | Val Acc: 0.3824 | Test Acc: 0.4132\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8269\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9628 | Val Acc: 0.4412 | Test Acc: 0.4668\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8324\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9355 | Val Acc: 0.3824 | Test Acc: 0.4104\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7978 | Test Acc: 0.8269\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9501 | Val Acc: 0.4265 | Test Acc: 0.4631\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7941 | Test Acc: 0.8283\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8283, Std Deviation: 0.0018\n",
      "\n",
      "\n",
      "<class '__main__.GAT'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9873 | Val Acc: 0.4743 | Test Acc: 0.4760\n",
      "Epoch: 201/300 | Train Loss: 0.5517 | Val Acc: 0.8199 | Test Acc: 0.8287\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0218 | Val Acc: 0.4632 | Test Acc: 0.4621\n",
      "Epoch: 201/300 | Train Loss: 0.6108 | Val Acc: 0.7978 | Test Acc: 0.8333\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0278 | Val Acc: 0.5404 | Test Acc: 0.5254\n",
      "Epoch: 201/300 | Train Loss: 0.6490 | Val Acc: 0.8015 | Test Acc: 0.8347\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9653 | Val Acc: 0.5772 | Test Acc: 0.5457\n",
      "Epoch: 201/300 | Train Loss: 0.6270 | Val Acc: 0.8088 | Test Acc: 0.8338\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 2.0241 | Val Acc: 0.5368 | Test Acc: 0.5651\n",
      "Epoch: 201/300 | Train Loss: 0.6412 | Val Acc: 0.8088 | Test Acc: 0.8283\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8270, Std Deviation: 0.0032\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0345 | Val Acc: 0.5257 | Test Acc: 0.5499\n",
      "Epoch: 201/300 | Train Loss: 0.7147 | Val Acc: 0.8088 | Test Acc: 0.8232\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 2.0338 | Val Acc: 0.5919 | Test Acc: 0.6136\n",
      "Epoch: 201/300 | Train Loss: 0.6861 | Val Acc: 0.7904 | Test Acc: 0.8319\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9942 | Val Acc: 0.6544 | Test Acc: 0.6588\n",
      "Epoch: 201/300 | Train Loss: 0.6282 | Val Acc: 0.8272 | Test Acc: 0.8296\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9682 | Val Acc: 0.6397 | Test Acc: 0.6450\n",
      "Epoch: 201/300 | Train Loss: 0.5856 | Val Acc: 0.8015 | Test Acc: 0.8301\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9878 | Val Acc: 0.4853 | Test Acc: 0.4931\n",
      "Epoch: 201/300 | Train Loss: 0.5946 | Val Acc: 0.7941 | Test Acc: 0.8301\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8267, Std Deviation: 0.0040\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0008 | Val Acc: 0.4118 | Test Acc: 0.4201\n",
      "Epoch: 201/300 | Train Loss: 0.6651 | Val Acc: 0.8015 | Test Acc: 0.8255\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0111 | Val Acc: 0.4228 | Test Acc: 0.4298\n",
      "Epoch: 201/300 | Train Loss: 0.6654 | Val Acc: 0.8199 | Test Acc: 0.8255\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0220 | Val Acc: 0.5147 | Test Acc: 0.5125\n",
      "Epoch: 201/300 | Train Loss: 0.5206 | Val Acc: 0.8199 | Test Acc: 0.8403\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9865 | Val Acc: 0.6176 | Test Acc: 0.6468\n",
      "Epoch: 201/300 | Train Loss: 0.5962 | Val Acc: 0.8235 | Test Acc: 0.8296\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 2.0038 | Val Acc: 0.5809 | Test Acc: 0.5771\n",
      "Epoch: 201/300 | Train Loss: 0.5757 | Val Acc: 0.7941 | Test Acc: 0.8278\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.8288, Std Deviation: 0.0023\n",
      "\n",
      "\n",
      "<class '__main__.GIN'>\n",
      "Run 1/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9609 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6985 | Test Acc: 0.7387\n",
      "Run 2/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9575 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7316 | Test Acc: 0.7438\n",
      "Run 3/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9541 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7316 | Test Acc: 0.7627\n",
      "Run 4/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9512 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7500 | Test Acc: 0.7285\n",
      "Run 5/5 on dataset Cora\n",
      "Epoch: 1/300 | Train Loss: 1.9545 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7243 | Test Acc: 0.7442\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7460, Std Deviation: 0.0141\n",
      "\n",
      "Run 1/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9523 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6949 | Test Acc: 0.7318\n",
      "Run 2/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9425 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7279 | Test Acc: 0.7548\n",
      "Run 3/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9620 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7426 | Test Acc: 0.7488\n",
      "Run 4/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9557 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7500 | Test Acc: 0.7678\n",
      "Run 5/5 on dataset CiteSeer\n",
      "Epoch: 1/300 | Train Loss: 1.9498 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6912 | Test Acc: 0.7285\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7478, Std Deviation: 0.0144\n",
      "\n",
      "Run 1/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9713 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0003 | Val Acc: 0.7132 | Test Acc: 0.7322\n",
      "Run 2/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9300 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7426 | Test Acc: 0.7613\n",
      "Run 3/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9706 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.6875 | Test Acc: 0.7318\n",
      "Run 4/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9516 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0001 | Val Acc: 0.7279 | Test Acc: 0.7567\n",
      "Run 5/5 on dataset PubMed\n",
      "Epoch: 1/300 | Train Loss: 1.9549 | Val Acc: 0.2904 | Test Acc: 0.3084\n",
      "Epoch: 201/300 | Train Loss: 0.0000 | Val Acc: 0.7316 | Test Acc: 0.7659\n",
      "\n",
      "Mean Accuracy over 5 runs: 0.7577, Std Deviation: 0.0053\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    print(f'Dataset: {dataset.name}')\n",
    "    data = dataset[0]  # Assuming dataset is a tuple where [0] is the data object\n",
    "\n",
    "    global device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    num_features = data.num_features\n",
    "    num_classes = dataset.num_classes\n",
    "\n",
    "    for model_class in [GCN, GAT, GIN]:\n",
    "        print('')\n",
    "        print(f'{model_class}')\n",
    "        run_experiment(model_class, num_features, num_classes, data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
     ]
    }
   ],
   "source": [
    "data = datasets[2][0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True,  True,  True,  ..., False, False, False]) torch.Size([19717]) 60\n",
      "tensor([False, False, False,  ..., False, False, False]) torch.Size([19717]) 500\n",
      "tensor([False, False, False,  ...,  True,  True,  True]) torch.Size([19717]) 1000\n"
     ]
    }
   ],
   "source": [
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "print(train_mask.data, train_mask.shape, train_mask.sum().item())\n",
    "print(val_mask.data, val_mask.shape, val_mask.sum().item())\n",
    "print(test_mask.data, test_mask.shape, test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False,  ...,  True, False,  True]) torch.Size([19717]) 1971\n",
      "tensor([False, False, False,  ..., False, False, False]) torch.Size([19717]) 1973\n",
      "tensor([ True,  True,  True,  ..., False,  True, False]) torch.Size([19717]) 15773\n"
     ]
    }
   ],
   "source": [
    "train_mask, val_mask, test_mask = split_indices(data)\n",
    "\n",
    "print(train_mask.data, train_mask.shape, train_mask.sum().item())\n",
    "print(val_mask.data, val_mask.shape, val_mask.sum().item())\n",
    "print(test_mask.data, test_mask.shape, test_mask.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lable:[0 0 0 ... 3 3 3]\n",
      "labels inside encode_oneshot [[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "lable after encode oneshot [[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t1.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 11)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 13)\t1.0\n",
      "  (14, 14)\t1.0\n",
      "  (15, 15)\t1.0\n",
      "  (16, 16)\t1.0\n",
      "  (17, 17)\t1.0\n",
      "  (18, 18)\t1.0\n",
      "  (19, 19)\t1.0\n",
      "  (20, 20)\t1.0\n",
      "  (21, 21)\t1.0\n",
      "  (22, 22)\t1.0\n",
      "  (23, 23)\t1.0\n",
      "  (24, 24)\t1.0\n",
      "  :\t:\n",
      "  (13304, 13304)\t1.0\n",
      "  (13305, 13305)\t1.0\n",
      "  (13306, 13306)\t1.0\n",
      "  (13307, 13307)\t1.0\n",
      "  (13308, 13308)\t1.0\n",
      "  (13309, 13309)\t1.0\n",
      "  (13310, 13310)\t1.0\n",
      "  (13311, 13311)\t1.0\n",
      "  (13312, 13312)\t1.0\n",
      "  (13313, 13313)\t1.0\n",
      "  (13314, 13314)\t1.0\n",
      "  (13315, 13315)\t1.0\n",
      "  (13316, 13316)\t1.0\n",
      "  (13317, 13317)\t1.0\n",
      "  (13318, 13318)\t1.0\n",
      "  (13319, 13319)\t1.0\n",
      "  (13320, 13320)\t1.0\n",
      "  (13321, 13321)\t1.0\n",
      "  (13322, 13322)\t1.0\n",
      "  (13323, 13323)\t1.0\n",
      "  (13324, 13324)\t1.0\n",
      "  (13325, 13325)\t1.0\n",
      "  (13326, 13326)\t1.0\n",
      "  (13327, 13327)\t1.0\n",
      "  (13328, 13328)\t1.0   (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t1.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 11)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 13)\t1.0\n",
      "  (14, 14)\t1.0\n",
      "  (15, 15)\t1.0\n",
      "  (16, 16)\t1.0\n",
      "  (17, 17)\t1.0\n",
      "  (18, 18)\t1.0\n",
      "  (19, 19)\t1.0\n",
      "  (20, 20)\t1.0\n",
      "  (21, 21)\t1.0\n",
      "  (22, 22)\t1.0\n",
      "  (23, 23)\t1.0\n",
      "  (24, 24)\t1.0\n",
      "  :\t:\n",
      "  (13304, 13304)\t1.0\n",
      "  (13305, 13305)\t1.0\n",
      "  (13306, 13306)\t1.0\n",
      "  (13307, 13307)\t1.0\n",
      "  (13308, 13308)\t1.0\n",
      "  (13309, 13309)\t1.0\n",
      "  (13310, 13310)\t1.0\n",
      "  (13311, 13311)\t1.0\n",
      "  (13312, 13312)\t1.0\n",
      "  (13313, 13313)\t1.0\n",
      "  (13314, 13314)\t1.0\n",
      "  (13315, 13315)\t1.0\n",
      "  (13316, 13316)\t1.0\n",
      "  (13317, 13317)\t1.0\n",
      "  (13318, 13318)\t1.0\n",
      "  (13319, 13319)\t1.0\n",
      "  (13320, 13320)\t1.0\n",
      "  (13321, 13321)\t1.0\n",
      "  (13322, 13322)\t1.0\n",
      "  (13323, 13323)\t1.0\n",
      "  (13324, 13324)\t1.0\n",
      "  (13325, 13325)\t1.0\n",
      "  (13326, 13326)\t1.0\n",
      "  (13327, 13327)\t1.0\n",
      "  (13328, 13328)\t1.0   (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 2)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 4)\t1.0\n",
      "  (5, 5)\t1.0\n",
      "  (6, 6)\t1.0\n",
      "  (7, 7)\t1.0\n",
      "  (8, 8)\t1.0\n",
      "  (9, 9)\t1.0\n",
      "  (10, 10)\t1.0\n",
      "  (11, 11)\t1.0\n",
      "  (12, 12)\t1.0\n",
      "  (13, 13)\t1.0\n",
      "  (14, 14)\t1.0\n",
      "  (15, 15)\t1.0\n",
      "  (16, 16)\t1.0\n",
      "  (17, 17)\t1.0\n",
      "  (18, 18)\t1.0\n",
      "  (19, 19)\t1.0\n",
      "  (20, 20)\t1.0\n",
      "  (21, 21)\t1.0\n",
      "  (22, 22)\t1.0\n",
      "  (23, 23)\t1.0\n",
      "  (24, 24)\t1.0\n",
      "  :\t:\n",
      "  (35865, 35865)\t1.0\n",
      "  (35866, 35866)\t1.0\n",
      "  (35867, 35867)\t1.0\n",
      "  (35868, 35868)\t1.0\n",
      "  (35869, 35869)\t1.0\n",
      "  (35870, 35870)\t1.0\n",
      "  (35871, 35871)\t1.0\n",
      "  (35872, 35872)\t1.0\n",
      "  (35873, 35873)\t1.0\n",
      "  (35874, 35874)\t1.0\n",
      "  (35875, 35875)\t1.0\n",
      "  (35876, 35876)\t1.0\n",
      "  (35877, 35877)\t1.0\n",
      "  (35878, 35878)\t1.0\n",
      "  (35879, 35879)\t1.0\n",
      "  (35880, 35880)\t1.0\n",
      "  (35881, 35881)\t1.0\n",
      "  (35882, 35882)\t1.0\n",
      "  (35883, 35883)\t1.0\n",
      "  (35884, 35884)\t1.0\n",
      "  (35885, 35885)\t1.0\n",
      "  (35886, 35886)\t1.0\n",
      "  (35887, 35887)\t1.0\n",
      "  (35888, 35888)\t1.0\n",
      "  (35889, 35889)\t1.0\n",
      "\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "\n",
      "  (0, 0)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 62)\tTrue\n",
      "  (3, 3)\tTrue\n",
      "  (3, 105)\tTrue\n",
      "  (3, 220)\tTrue\n",
      "  (4, 4)\tTrue\n",
      "  (4, 91)\tTrue\n",
      "  (5, 5)\tTrue\n",
      "  (6, 6)\tTrue\n",
      "  (6, 12)\tTrue\n",
      "  (7, 7)\tTrue\n",
      "  (8, 8)\tTrue\n",
      "  (9, 9)\tTrue\n",
      "  (10, 10)\tTrue\n",
      "  (11, 11)\tTrue\n",
      "  (11, 219)\tTrue\n",
      "  (12, 6)\tTrue\n",
      "  (12, 12)\tTrue\n",
      "  (12, 92)\tTrue\n",
      "  (13, 13)\tTrue\n",
      "  (14, 14)\tTrue\n",
      "  (14, 106)\tTrue\n",
      "  (14, 221)\tTrue\n",
      "  :\t:\n",
      "  (6552, 6552)\tTrue\n",
      "  (6553, 3315)\tTrue\n",
      "  (6553, 6553)\tTrue\n",
      "  (6554, 6554)\tTrue\n",
      "  (6555, 3322)\tTrue\n",
      "  (6555, 3364)\tTrue\n",
      "  (6555, 6555)\tTrue\n",
      "  (6556, 6556)\tTrue\n",
      "  (6557, 6557)\tTrue\n",
      "  (6558, 6558)\tTrue\n",
      "  (6559, 6559)\tTrue\n",
      "  (6560, 6560)\tTrue\n",
      "  (6561, 2995)\tTrue\n",
      "  (6561, 3008)\tTrue\n",
      "  (6561, 3039)\tTrue\n",
      "  (6561, 3183)\tTrue\n",
      "  (6561, 3201)\tTrue\n",
      "  (6561, 3309)\tTrue\n",
      "  (6561, 6561)\tTrue\n",
      "  (6562, 3340)\tTrue\n",
      "  (6562, 3366)\tTrue\n",
      "  (6562, 6562)\tTrue\n",
      "  (6563, 4586)\tTrue\n",
      "  (6563, 6497)\tTrue\n",
      "  (6563, 6563)\tTrue\n",
      "  (0, 0)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 62)\tTrue\n",
      "  (3, 3)\tTrue\n",
      "  (3, 105)\tTrue\n",
      "  (3, 220)\tTrue\n",
      "  (4, 4)\tTrue\n",
      "  (4, 91)\tTrue\n",
      "  (5, 5)\tTrue\n",
      "  (6, 6)\tTrue\n",
      "  (6, 12)\tTrue\n",
      "  (7, 7)\tTrue\n",
      "  (8, 8)\tTrue\n",
      "  (9, 9)\tTrue\n",
      "  (10, 10)\tTrue\n",
      "  (11, 11)\tTrue\n",
      "  (11, 219)\tTrue\n",
      "  (12, 6)\tTrue\n",
      "  (12, 12)\tTrue\n",
      "  (12, 92)\tTrue\n",
      "  (13, 13)\tTrue\n",
      "  (14, 14)\tTrue\n",
      "  (14, 106)\tTrue\n",
      "  (14, 221)\tTrue\n",
      "  :\t:\n",
      "  (6552, 6552)\tTrue\n",
      "  (6553, 3315)\tTrue\n",
      "  (6553, 6553)\tTrue\n",
      "  (6554, 6554)\tTrue\n",
      "  (6555, 3322)\tTrue\n",
      "  (6555, 3364)\tTrue\n",
      "  (6555, 6555)\tTrue\n",
      "  (6556, 6556)\tTrue\n",
      "  (6557, 6557)\tTrue\n",
      "  (6558, 6558)\tTrue\n",
      "  (6559, 6559)\tTrue\n",
      "  (6560, 6560)\tTrue\n",
      "  (6561, 2995)\tTrue\n",
      "  (6561, 3008)\tTrue\n",
      "  (6561, 3039)\tTrue\n",
      "  (6561, 3183)\tTrue\n",
      "  (6561, 3201)\tTrue\n",
      "  (6561, 3309)\tTrue\n",
      "  (6561, 6561)\tTrue\n",
      "  (6562, 3340)\tTrue\n",
      "  (6562, 3366)\tTrue\n",
      "  (6562, 6562)\tTrue\n",
      "  (6563, 4586)\tTrue\n",
      "  (6563, 6497)\tTrue\n",
      "  (6563, 6563)\tTrue\n",
      "  (0, 0)\tTrue\n",
      "  (0, 124)\tTrue\n",
      "  (0, 168)\tTrue\n",
      "  (0, 547)\tTrue\n",
      "  (0, 1698)\tTrue\n",
      "  (0, 2728)\tTrue\n",
      "  (0, 2935)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (1, 7)\tTrue\n",
      "  (1, 38)\tTrue\n",
      "  (1, 39)\tTrue\n",
      "  (1, 1690)\tTrue\n",
      "  (1, 1851)\tTrue\n",
      "  (1, 2091)\tTrue\n",
      "  (1, 2160)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 11)\tTrue\n",
      "  (2, 15)\tTrue\n",
      "  (2, 26)\tTrue\n",
      "  (2, 45)\tTrue\n",
      "  (2, 62)\tTrue\n",
      "  (2, 113)\tTrue\n",
      "  (2, 118)\tTrue\n",
      "  (2, 254)\tTrue\n",
      "  (2, 302)\tTrue\n",
      "  :\t:\n",
      "  (6563, 3334)\tTrue\n",
      "  (6563, 3349)\tTrue\n",
      "  (6563, 3354)\tTrue\n",
      "  (6563, 3355)\tTrue\n",
      "  (6563, 3360)\tTrue\n",
      "  (6563, 3363)\tTrue\n",
      "  (6563, 3366)\tTrue\n",
      "  (6563, 3656)\tTrue\n",
      "  (6563, 4377)\tTrue\n",
      "  (6563, 4700)\tTrue\n",
      "  (6563, 5396)\tTrue\n",
      "  (6563, 5891)\tTrue\n",
      "  (6563, 5951)\tTrue\n",
      "  (6563, 6524)\tTrue\n",
      "  (6563, 6531)\tTrue\n",
      "  (6563, 6535)\tTrue\n",
      "  (6563, 6536)\tTrue\n",
      "  (6563, 6537)\tTrue\n",
      "  (6563, 6541)\tTrue\n",
      "  (6563, 6542)\tTrue\n",
      "  (6563, 6543)\tTrue\n",
      "  (6563, 6547)\tTrue\n",
      "  (6563, 6560)\tTrue\n",
      "  (6563, 6562)\tTrue\n",
      "  (6563, 6563)\tTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16822\\AppData\\Local\\Temp\\ipykernel_30184\\4135217362.py:71: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:620.)\n",
      "  return th.sparse.FloatTensor(indices, values, shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2424, 3095, 3608,  667, 1313, 2000, 2414, 1454, 1844, 2588, 2737, 1330,\n",
      "         361, 2037, 4735, 1989, 6151, 2926, 2721, 5353, 2686, 2845, 5504, 4380,\n",
      "        2676, 1711,   76, 5565, 2333,  117,  990, 2887, 2420,  582,  878,  347,\n",
      "        1158, 2654, 3211, 3773, 1268, 1254, 1789, 4390, 1843, 5128, 1816, 2893,\n",
      "        6160, 2766, 1014, 2670,  961, 2538, 2827, 1334, 2304, 1227, 5537, 5260,\n",
      "         458,  789, 2032, 2035, 2703, 2671, 2346, 3039, 3882, 2562, 4093,   58,\n",
      "        2075, 2497, 1004, 2750, 5957, 4321, 2940, 1660]), tensor([2729,  824,  242,  591, 2809, 2607, 1297, 2026, 1700, 4080, 1673, 2929,\n",
      "         498, 1881, 2406, 4101, 2638, 5745, 1506, 5873, 2483,  541, 2754, 5355,\n",
      "        2826,   41, 4070, 4680, 1682, 5676, 4353, 2717, 5376,  697,  111, 2437,\n",
      "         690, 2827, 2677, 1293,  519, 2363, 6272, 2909, 2801, 2947, 1366, 5925,\n",
      "        1333, 1417, 2567, 2396,  116,  809, 2035, 1113, 6343, 4796, 1949,  223,\n",
      "        3244, 4458,  203, 6155,  516, 4969, 2538, 2566, 1502, 2907,  119, 3840,\n",
      "        2652, 3252, 2815, 1055, 2114, 2835,  980, 2642, 2890, 2771, 1246, 3312,\n",
      "        1939, 5095, 2611, 3875, 3535, 2953, 5224, 2651, 1354, 1363, 1952, 4195,\n",
      "        1266, 1418, 1003, 1065, 1386, 2472,  572, 2120, 1020, 5868, 5612, 2235,\n",
      "        2269, 2339, 1459, 2038, 1115, 5114, 2258,  674, 2401, 2859, 3755, 4515,\n",
      "        2083, 6503,  707, 2092, 2854,  737,  322,  664, 5540, 5480, 1283, 2872,\n",
      "        2429, 1133, 1762, 1392,  346, 5881, 2845, 2882, 1777, 5973, 2101, 2659,\n",
      "        3706,  576, 1073,  943, 2692, 3558, 6052, 2571, 2637, 6319, 2294, 2764,\n",
      "         139, 1868, 2589, 2951]), tensor([2823, 2499, 1989, 2739, 4454,  176, 4192, 2614, 2870, 4911, 2247, 2702,\n",
      "        2121, 1904, 2967, 5000,  271, 2641,   65, 5926, 2888, 1803, 5376, 2835,\n",
      "        2176, 2660, 3655, 2949, 6312, 2995, 4053, 2635, 1185, 1829, 1084, 2551,\n",
      "        1099, 3531, 1603, 5745, 2622, 2555, 1944, 1022, 3046, 1074, 6303,  151,\n",
      "         138, 6487,  924, 3316, 3816, 3888, 5225, 5259, 1665, 2474, 3565, 2845,\n",
      "        2683, 2968, 1693, 3344,  479, 1312, 2447, 1831,  790, 5262, 2417, 1666,\n",
      "        2884, 1351, 2530, 6174, 2493, 1524, 1097,   60,  756, 3184, 1167, 2685,\n",
      "         595, 2955, 1825, 1210, 6036, 1688, 2781,  708, 3342, 1707,  787, 4298,\n",
      "        1385, 2553, 2473, 1023, 1594,  341, 2507, 2187, 2290, 2752, 6045, 6026,\n",
      "          55, 5706, 2501, 1716,  583, 2631, 2219, 2913, 4769, 2681, 2367, 2568,\n",
      "        5740, 2804, 1871, 2603, 2090, 2295, 3234, 2618, 2689, 5501, 1983, 3407,\n",
      "        2405, 2451, 1423, 2398, 1193, 2413, 2132, 4387, 1684, 4422, 1203,  492,\n",
      "        5621,  938, 2572, 1955, 2795,  187, 1369, 2035, 1744,  118, 3977, 2522,\n",
      "        2715, 2815, 5260,  665, 2463, 6124, 1548, 1297, 1024, 4299, 2880, 1593,\n",
      "        4142,  227, 2607,  961,  256, 4212, 1291, 1630, 2640,  866, 2107, 2627,\n",
      "        2258,  422, 1119, 5728, 2810, 2686, 4063, 1968, 1810, 2122,   38,  865,\n",
      "        6007, 5174,  354, 2894, 1559,  353,  257, 2971, 2654, 5796, 1518, 3012,\n",
      "        3138, 2421, 4610, 2160,  452, 1838, 2532, 3114,  234, 2301, 5033, 1246,\n",
      "        2380, 2877, 2152, 1168,  394,  730,  797, 2113, 1034, 5164, 1260, 2171,\n",
      "         247,   34,  220, 3039, 4493, 5293, 1196, 1352, 1396, 2946, 2771, 6092])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "import torch as th\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data_folder = \"data/\"\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "\n",
    "    # reshapes the numpy array to one column and whatever rows is required. \n",
    "    labels = labels.reshape(-1, 1)\n",
    "    print(f'labels inside encode_oneshot {labels}')\n",
    "\n",
    "    # tranfromss the labels or categorical array into a matrix of 0 and 1 that encodes where the data is presenst\n",
    "    # this is used to pass in models for understanding where the catgories are. \n",
    "    enc = OneHotEncoder()\n",
    "\n",
    "    #This method is used to fit the encoder to the data, learning the unique categories for each feature that will\n",
    "    #  be transformed during the encoding process.\n",
    "    enc.fit(labels)\n",
    "\n",
    "    # converst categoriacal data into a binary matrix. \n",
    "    labels_onehot = enc.transform(labels).toarray()\n",
    "\n",
    "    # returns this oneshot binary matrix. \n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "\n",
    "    # gets a matrix plugged in and sums up all the rows of each the matrix and stores them in a tensor which gest turned into an array. \n",
    "    rowsum = np.array(features.sum(1))\n",
    "\n",
    "\n",
    "    r_inv = np.power(rowsum, -1).flatten() # this performs an element-wise inverse on rowsum and then flatten the results to a 1-d array\n",
    "    r_inv[np.isinf(r_inv)] = 0. # checks if any of the values are infinity prompting them to be equal to zero. \n",
    "    r_mat_inv = sp.diags(r_inv) # construct a digonal sparse matrix using the array of r_inv\n",
    "    features = r_mat_inv.dot(features) # this multiples the new digonal matrix by the original features matrix. \n",
    "    if isinstance(features, np.ndarray):\n",
    "        return features # if features was a numpy array it returns the new matrix \n",
    "    else:\n",
    "        return features.todense() # if features matrix is not a numpy array it turns it into a dense matrix\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "\n",
    "\n",
    "    # the adj is a matrix that tells us which nodes are connecte dwith each other. \n",
    "    adj = sp.coo_matrix(adj) # this turns the adj matrix into a coo matrix which is used to save memory and better fro computation \n",
    "    # only saves the none zero objects in the matrix. \n",
    "    print(adj)\n",
    "    rowsum = np.array(adj.sum(1))# sums up each row in the adj matrix and makes them into a numpy array\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # elemnt-wise inverse square root from rowsum, then flattens it to a 1-d array\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0. # checks to see if any values infinity if so it turns it into zero \n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # creates a sparse diagonal matrix where the values of d_inv_sqrt are placed in the diagonal \n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # perfroms the symmetric normalization of the adjacency matrix. \n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = th.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = th.from_numpy(sparse_mx.data)\n",
    "    shape = th.Size(sparse_mx.shape)\n",
    "    return th.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "\n",
    "def process_data_in_pyg(neigs):\n",
    "    d = defaultdict(dict)\n",
    "    metapaths = []\n",
    "    for mp_i, nei1 in enumerate(neigs):\n",
    "        dst_array_concat = np.concatenate(nei1)\n",
    "        src_array_concat = []\n",
    "        for src_id, dst_array in enumerate(nei1):\n",
    "            src_array_concat.extend([src_id] * len(dst_array))\n",
    "        src_array_concat = np.array(src_array_concat)\n",
    "        src_name = f\"target\"\n",
    "        dst_name = f\"dst_{mp_i}\"\n",
    "        relation = f\"relation_{mp_i}\"\n",
    "        d[(src_name, relation + \"-->\", dst_name)][\"edge_index\"] = th.LongTensor(np.vstack([src_array_concat, dst_array_concat]))\n",
    "        metapaths.append((src_name, relation + \"-->\", dst_name))\n",
    "        d[(dst_name, \"<--\" + relation, src_name)][\"edge_index\"] = th.LongTensor(np.vstack([dst_array_concat, src_array_concat]))\n",
    "        metapaths.append((dst_name, \"<--\" + relation, src_name))\n",
    "    g = HeteroData(d)\n",
    "    return g, metapaths\n",
    "\n",
    "\n",
    "\n",
    "def load_aminer(ratio, type_num):\n",
    "    # The order of node types: 0 p 1 a 2 \\r\n",
    "\n",
    "    # creates the path to aminer dataset \n",
    "    path = data_folder + \"aminer/\"\n",
    "\n",
    "    #loads the labels.npy into a np array \n",
    "    label = np.load(path + \"labels.npy\").astype('int32')\n",
    "    print(f'lable:{label}')\n",
    "\n",
    "\n",
    "    label = encode_onehot(label)\n",
    "    print(f'lable after encode oneshot {label}')\n",
    "    # load object that are seralized inside a numpy array \n",
    "    # these objects are from the authors. \n",
    "    nei_a = np.load(path + \"nei_a.npy\", allow_pickle=True)\\\n",
    "\n",
    "    # load object that are searilized inside a numpy array \n",
    "    # this is for relationships. \n",
    "    nei_r = np.load(path + \"nei_r.npy\", allow_pickle=True)\n",
    "\n",
    "    # Because none of P, A or R has features, we assign one-hot encodings to all of them.\n",
    "    # the fatures are added to the papers, authors and relationships \n",
    "    # the type-num is \"type_num\": [6564, 13329, 35890],\n",
    "    # so paper -> 6564, author -> 13329 -> relationships -> 35890\n",
    "    # spicy.sparse.eye create a identity matrix of size type_num \n",
    "    feat_p = sp.eye(type_num[0]) #return a tuple of the indices connected witht he value for example (2,2) 1, means there is a 1 in row 2, column2, this goes down to the size of the array. \n",
    "\n",
    "    feat_a = sp.eye(type_num[1])\n",
    "    feat_r = sp.eye(type_num[2])\n",
    "\n",
    "    #loads sparse matrix that have been stored in paper-author-paper, paper-relationship-paper, paper-\n",
    "    pap = sp.load_npz(path + \"pap.npz\")\n",
    "    prp = sp.load_npz(path + \"prp.npz\")\n",
    "    pos = sp.load_npz(path + \"pos.npz\")\n",
    "\n",
    "    # contains the indices of the nodes for the train, test, val\n",
    "    # .npy is a way to store numpy arrays into files\n",
    "    train = [np.load(path + \"train_\" + str(i) + \".npy\") for i in ratio]\n",
    "    test = [np.load(path + \"test_\" + str(i) + \".npy\") for i in ratio]\n",
    "    val = [np.load(path + \"val_\" + str(i) + \".npy\") for i in ratio]\n",
    "\n",
    "\n",
    "    label = th.FloatTensor(label) # make label a float tensor \n",
    "    nei_a = [th.LongTensor(i) for i in nei_a] # make neighboring area a tensor instead of a numpy array\n",
    "    nei_r = [th.LongTensor(i) for i in nei_r] # make neighboring area of relationships a float tensor intstead of a numpy array. \n",
    "\n",
    "    # go into the preporccess features. \n",
    "    print(\"\")\n",
    "    print(feat_a, feat_a, feat_r)\n",
    "\n",
    "    # passes the feat_a p, r sparse matrices with are idneitity matrices. there are tuples designenating row and column with a value next to it \n",
    "    # representing the value that is in the matrix. \n",
    "    feat_p = th.FloatTensor(preprocess_features(feat_p)) # this turns the matrix that is returned int a float tensor \n",
    "    print('')\n",
    "    print(feat_p)\n",
    "    feat_a = th.FloatTensor(preprocess_features(feat_a))\n",
    "    feat_r = th.FloatTensor(preprocess_features(feat_r))\n",
    "\n",
    "    print('')\n",
    "    print(pap)\n",
    "    pap = sparse_mx_to_torch_sparse_tensor(normalize_adj(pap)) # perfroms a symetric normalization on the matrices. _\n",
    "    prp = sparse_mx_to_torch_sparse_tensor(normalize_adj(prp))\n",
    "    pos = sparse_mx_to_torch_sparse_tensor(pos)\n",
    "    train = [th.LongTensor(i) for i in train]\n",
    "    val = [th.LongTensor(i) for i in val]\n",
    "    test = [th.LongTensor(i) for i in test]\n",
    "    print(train)\n",
    "    return [nei_a, nei_r], [feat_p, feat_a, feat_r], [pap, prp], pos, label, train, val, test\n",
    "\n",
    "def load_data(ratio=[20,40,60], type_num=[6564, 13329, 35890]):\n",
    "    data = load_aminer(ratio, type_num)\n",
    "    g, metapaths = process_data_in_pyg(data[0])\n",
    "    return data, g, metapaths\n",
    "\n",
    "\n",
    "(nei_index, feats, mps, pos, label, idx_train, idx_val, idx_test), g, metapaths = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80]) torch.Size([160]) torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(idx_train[0].shape, idx_train[1].shape, idx_train[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]]),\n",
       " tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6564, 4])\n"
     ]
    }
   ],
   "source": [
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('target', 'relation_0-->', 'dst_0'), ('dst_0', '<--relation_0', 'target'), ('target', 'relation_1-->', 'dst_1'), ('dst_1', '<--relation_1', 'target')]\n"
     ]
    }
   ],
   "source": [
    "print(metapaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import AMiner\n",
    "\n",
    "path = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\metapath2vec\\Aminer'\n",
    "dataset = AMiner(path)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  author={\n",
      "    y=[246678],\n",
      "    y_index=[246678],\n",
      "    num_nodes=1693531,\n",
      "  },\n",
      "  venue={\n",
      "    y=[134],\n",
      "    y_index=[134],\n",
      "    num_nodes=3883,\n",
      "  },\n",
      "  paper={ num_nodes=3194405 },\n",
      "  (paper, written_by, author)={ edge_index=[2, 9323605] },\n",
      "  (author, writes, paper)={ edge_index=[2, 9323605] },\n",
      "  (paper, published_in, venue)={ edge_index=[2, 3194405] },\n",
      "  (venue, publishes, paper)={ edge_index=[2, 3194405] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
