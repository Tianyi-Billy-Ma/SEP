{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch_geometric.datasets import AMiner, Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, MLP\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\metapath2vec\\Aminer'\n",
    "dataset = AMiner(path)\n",
    "data = dataset[0]\n",
    "homo = data.to_homogeneous()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =r'C:\\Users\\16822\\Research Project SER\\SEP-NHANES\\lmolina3\\src\\utils\\GCN\\data'\n",
    "dataset = Planetoid(root= path, name='Cora')\n",
    "cora = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10556])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.train_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  author={\n",
      "    y=[246678],\n",
      "    y_index=[246678],\n",
      "    num_nodes=1693531,\n",
      "  },\n",
      "  venue={\n",
      "    y=[134],\n",
      "    y_index=[134],\n",
      "    num_nodes=3883,\n",
      "  },\n",
      "  paper={ num_nodes=3194405 },\n",
      "  (paper, written_by, author)={ edge_index=[2, 9323605] },\n",
      "  (author, writes, paper)={ edge_index=[2, 9323605] },\n",
      "  (paper, published_in, venue)={ edge_index=[2, 3194405] },\n",
      "  (venue, publishes, paper)={ edge_index=[2, 3194405] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo = data.to_homogeneous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 25036020], y=[3441217], y_index=[3441217], node_type=[4891819], edge_type=[25036020])\n"
     ]
    }
   ],
   "source": [
    "print(homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1697414, 1697415, 1697416,  ..., 1696679, 1696679, 1696679],\n",
       "        [      0,       1,       2,  ..., 4891816, 4891817, 4891818]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3441217])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo.y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 168866, 1327323,     870,  ...,      -1,      -1,      -1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo.y_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo.node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo.edge_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25036020]) torch.Size([2, 25036020])\n"
     ]
    }
   ],
   "source": [
    "print(homo.edge_type.shape, homo.edge_index.shape) \n",
    "# edge_type tells us the type of edge it is \n",
    "# edge_index tells us which nodes are connected with each other \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.randn(4891819, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 5,  ..., 7, 7, 7])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homo.y[homo.y != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0054,  0.6463, -0.0601,  ...,  0.3903,  1.7832,  0.4792],\n",
       "        [-1.1563, -1.0221, -0.1909,  ...,  0.9849, -0.4358, -0.0301],\n",
       "        [ 0.5020, -0.3755, -1.1954,  ...,  0.2668, -0.1184,  0.9632],\n",
       "        ...,\n",
       "        [-0.8231, -0.9942,  3.2841,  ..., -0.3404, -0.8767, -0.1286],\n",
       "        [ 1.4441,  0.2791, -0.1515,  ...,  0.2072,  1.1465,  0.7235],\n",
       "        [-0.8635,  1.2216, -0.6395,  ...,  0.2325,  0.1296,  0.0270]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit[homo.y_index][homo.y != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, GATConv, GINConv, MLP\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def train(model, optimizer, data, train_mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    if model == GCN:\n",
    "        out = model(data.x, data.edge_index, data.edge_weight)\n",
    "    else: \n",
    "        out = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    logits = model(data.x, data.edge_index)\n",
    "    loss = F.cross_entropy(logits[mask], data.y[mask])\n",
    "    pred = logits[mask].max(1)[1]\n",
    "    correct = pred.eq(data.y[mask]).sum().item()\n",
    "    accuracy = correct / mask.sum().item()\n",
    "    return accuracy, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "devcie = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\16822\\AppData\\Local\\Temp\\ipykernel_30484\\3511987151.py:71: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:620.)\n",
      "  return th.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import scipy.sparse as sp\n",
    "import torch as th\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "data_folder = \"data/\"\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "\n",
    "    # reshapes the numpy array to one column and whatever rows is required. \n",
    "    labels = labels.reshape(-1, 1)\n",
    "    # print(f'labels inside encode_oneshot {labels}')\n",
    "\n",
    "    # tranfromss the labels or categorical array into a matrix of 0 and 1 that encodes where the data is presenst\n",
    "    # this is used to pass in models for understanding where the catgories are. \n",
    "    enc = OneHotEncoder()\n",
    "\n",
    "    #This method is used to fit the encoder to the data, learning the unique categories for each feature that will\n",
    "    #  be transformed during the encoding process.\n",
    "    enc.fit(labels)\n",
    "\n",
    "    # converst categoriacal data into a binary matrix. \n",
    "    labels_onehot = enc.transform(labels).toarray()\n",
    "\n",
    "    # returns this oneshot binary matrix. \n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "\n",
    "    # gets a matrix plugged in and sums up all the rows of each the matrix and stores them in a tensor which gest turned into an array. \n",
    "    rowsum = np.array(features.sum(1))\n",
    "\n",
    "\n",
    "    r_inv = np.power(rowsum, -1).flatten() # this performs an element-wise inverse on rowsum and then flatten the results to a 1-d array\n",
    "    r_inv[np.isinf(r_inv)] = 0. # checks if any of the values are infinity prompting them to be equal to zero. \n",
    "    r_mat_inv = sp.diags(r_inv) # construct a digonal sparse matrix using the array of r_inv\n",
    "    features = r_mat_inv.dot(features) # this multiples the new digonal matrix by the original features matrix. \n",
    "    if isinstance(features, np.ndarray):\n",
    "        return features # if features was a numpy array it returns the new matrix \n",
    "    else:\n",
    "        return features.todense() # if features matrix is not a numpy array it turns it into a dense matrix\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "\n",
    "\n",
    "    # the adj is a matrix that tells us which nodes are connecte dwith each other. \n",
    "    adj = sp.coo_matrix(adj) # this turns the adj matrix into a coo matrix which is used to save memory and better fro computation \n",
    "    # only saves the none zero objects in the matrix. \n",
    "    # print(adj)\n",
    "    rowsum = np.array(adj.sum(1))# sums up each row in the adj matrix and makes them into a numpy array\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten() # elemnt-wise inverse square root from rowsum, then flattens it to a 1-d array\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0. # checks to see if any values infinity if so it turns it into zero \n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) # creates a sparse diagonal matrix where the values of d_inv_sqrt are placed in the diagonal \n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() # perfroms the symmetric normalization of the adjacency matrix. \n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = th.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = th.from_numpy(sparse_mx.data)\n",
    "    shape = th.Size(sparse_mx.shape)\n",
    "    return th.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "\n",
    "def process_data_in_pyg(neigs):\n",
    "    d = defaultdict(dict)\n",
    "    metapaths = []\n",
    "    for mp_i, nei1 in enumerate(neigs):\n",
    "        dst_array_concat = np.concatenate(nei1)\n",
    "        src_array_concat = []\n",
    "        for src_id, dst_array in enumerate(nei1):\n",
    "            src_array_concat.extend([src_id] * len(dst_array))\n",
    "        src_array_concat = np.array(src_array_concat)\n",
    "        src_name = f\"target\"\n",
    "        dst_name = f\"dst_{mp_i}\"\n",
    "        relation = f\"relation_{mp_i}\"\n",
    "        d[(src_name, relation + \"-->\", dst_name)][\"edge_index\"] = th.LongTensor(np.vstack([src_array_concat, dst_array_concat]))\n",
    "        metapaths.append((src_name, relation + \"-->\", dst_name))\n",
    "        d[(dst_name, \"<--\" + relation, src_name)][\"edge_index\"] = th.LongTensor(np.vstack([dst_array_concat, src_array_concat]))\n",
    "        metapaths.append((dst_name, \"<--\" + relation, src_name))\n",
    "    g = HeteroData(d)\n",
    "    return g, metapaths\n",
    "\n",
    "\n",
    "\n",
    "def load_aminer(ratio, type_num):\n",
    "    # The order of node types: 0 p 1 a 2 \\r\n",
    "\n",
    "    # creates the path to aminer dataset \n",
    "    path = data_folder + \"aminer/\"\n",
    "\n",
    "    #loads the labels.npy into a np array \n",
    "    label = np.load(path + \"labels.npy\").astype('int32')\n",
    "    # print(f'lable:{label}')\n",
    "\n",
    "\n",
    "    label = encode_onehot(label)\n",
    "    # print(f'lable after encode oneshot {label}')\n",
    "    # load object that are seralized inside a numpy array \n",
    "    # these objects are from the authors. \n",
    "    nei_a = np.load(path + \"nei_a.npy\", allow_pickle=True)\\\n",
    "\n",
    "    # load object that are searilized inside a numpy array \n",
    "    # this is for relationships. \n",
    "    nei_r = np.load(path + \"nei_r.npy\", allow_pickle=True)\n",
    "\n",
    "    # Because none of P, A or R has features, we assign one-hot encodings to all of them.\n",
    "    # the fatures are added to the papers, authors and relationships \n",
    "    # the type-num is \"type_num\": [6564, 13329, 35890],\n",
    "    # so paper -> 6564, author -> 13329 -> relationships -> 35890\n",
    "    # spicy.sparse.eye create a identity matrix of size type_num \n",
    "    feat_p = sp.eye(type_num[0]) #return a tuple of the indices connected witht he value for example (2,2) 1, means there is a 1 in row 2, column2, this goes down to the size of the array. \n",
    "\n",
    "    feat_a = sp.eye(type_num[1])\n",
    "    feat_r = sp.eye(type_num[2])\n",
    "\n",
    "    #loads sparse matrix that have been stored in paper-author-paper, paper-relationship-paper, paper-\n",
    "    pap = sp.load_npz(path + \"pap.npz\")\n",
    "    prp = sp.load_npz(path + \"prp.npz\")\n",
    "    pos = sp.load_npz(path + \"pos.npz\")\n",
    "\n",
    "    # contains the indices of the nodes for the train, test, val\n",
    "    # .npy is a way to store numpy arrays into files\n",
    "    train = [np.load(path + \"train_\" + str(i) + \".npy\") for i in ratio]\n",
    "    test = [np.load(path + \"test_\" + str(i) + \".npy\") for i in ratio]\n",
    "    val = [np.load(path + \"val_\" + str(i) + \".npy\") for i in ratio]\n",
    "\n",
    "\n",
    "    label = th.FloatTensor(label) # make label a float tensor \n",
    "    nei_a = [th.LongTensor(i) for i in nei_a] # make neighboring area a tensor instead of a numpy array\n",
    "    nei_r = [th.LongTensor(i) for i in nei_r] # make neighboring area of relationships a float tensor intstead of a numpy array. \n",
    "\n",
    "    # go into the preporccess features. \n",
    "    # print(\"\")\n",
    "    # print(feat_a, feat_a, feat_r)\n",
    "\n",
    "    # passes the feat_a p, r sparse matrices with are idneitity matrices. there are tuples designenating row and column with a value next to it \n",
    "    # representing the value that is in the matrix. \n",
    "    feat_p = th.FloatTensor(preprocess_features(feat_p)) # this turns the matrix that is returned int a float tensor \n",
    "    # print('')\n",
    "    # print(feat_p)\n",
    "    feat_a = th.FloatTensor(preprocess_features(feat_a))\n",
    "    feat_r = th.FloatTensor(preprocess_features(feat_r))\n",
    "\n",
    "    # print('')\n",
    "    # print(pap)\n",
    "    pap = sparse_mx_to_torch_sparse_tensor(normalize_adj(pap)) # perfroms a symetric normalization on the matrices. _\n",
    "    prp = sparse_mx_to_torch_sparse_tensor(normalize_adj(prp))\n",
    "    pos = sparse_mx_to_torch_sparse_tensor(pos)\n",
    "    train = [th.LongTensor(i) for i in train]\n",
    "    val = [th.LongTensor(i) for i in val]\n",
    "    test = [th.LongTensor(i) for i in test]\n",
    "    # print(train)\n",
    "    return [nei_a, nei_r], [feat_p, feat_a, feat_r], [pap, prp], pos, label, train, val, test\n",
    "\n",
    "def load_data(ratio=[20,40,60], type_num=[6564, 13329, 35890]):\n",
    "    data = load_aminer(ratio, type_num)\n",
    "    g, metapaths = process_data_in_pyg(data[0])\n",
    "    return data, g, metapaths\n",
    "\n",
    "\n",
    "(nei_index, feats, mps, pos, label, idx_train, idx_val, idx_test), g, metapaths = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,  124,  168,  ..., 6536, 6547, 6563],\n",
       "                       [   0,    0,    0,  ..., 6563, 6563, 6563]]),\n",
       "       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "       size=(6564, 6564), nnz=67730, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55783\n",
      "torch.Size([6564, 4])\n"
     ]
    }
   ],
   "source": [
    "a, b , r = feats\n",
    "sum = a.shape[0] + b.shape[0] + r.shape[0]\n",
    "print(sum)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6480])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.cat((torch.cat(idx_train), torch.cat(idx_test), torch.cat(idx_val)), dim = 0)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "def pad_feats(feat_matrix, target_matrix):\n",
    "    '''This function pads the feature matrix with zeros in order to allow for concatination'''\n",
    "    if feat_matrix.shape[1] < target_matrix[1]:\n",
    "        padding = torch.zeros(feat_matrix.shape[0], target_matrix[1] - feat_matrix.shape[1])\n",
    "        feat_matrix = torch.cat([feat_matrix, padding], dim=1)\n",
    "    return feat_matrix\n",
    "\n",
    "def homogenous_graph(nei_index, feats, mps, pos, label, idx_train, idx_val, idx_test):\n",
    "    '''Using the outputs of load_aminer given in load_data.py file from HGMAE. This construst a \n",
    "    homogenous graph.'''\n",
    "\n",
    "    # get features \n",
    "    feat_papers, feat_authors, feat_relationships = feats\n",
    "    target_shape = (max(feat_papers.shape[0], feat_authors.shape[0], feat_relationships.shape[0]), \n",
    "                    max(feat_papers.shape[1], feat_authors.shape[1], feat_relationships.shape[1]))\n",
    "    \n",
    "    # pad the features \n",
    "    feat_papers = pad_feats(feat_papers, target_shape)\n",
    "    feat_authors = pad_feats(feat_authors, target_shape)\n",
    "    feat_relationships = pad_feats(feat_relationships, target_shape)\n",
    "\n",
    "    # create features matrix \n",
    "    features = torch.cat([feat_papers, feat_authors, feat_relationships], dim =0)\n",
    "    # print(features)\n",
    "    # print(features.shape)\n",
    "\n",
    "    pap, prp = mps\n",
    "    node_papers, node_authors, node_relationships = feat_papers.shape[0], feat_authors.shape[0], feat_relationships.shape[0]\n",
    "\n",
    "    pap_edge_index = pap.coalesce().indices()\n",
    "    prp_edge_index = prp.coalesce().indices()\n",
    "    # print(pap)\n",
    "    # print('')\n",
    "    # print(pap_edge_index)\n",
    "    # print('')\n",
    "    # print(prp, prp_edge_index)\n",
    "\n",
    "    pap_edge_index += node_papers\n",
    "    prp_edge_index += node_papers + node_relationships\n",
    "\n",
    "    edge_index = torch.cat((pap_edge_index, prp_edge_index), dim = 1)\n",
    "\n",
    "    # print(edge_index.shape)\n",
    "\n",
    "    data = Data(x=features, edge_index=edge_index, y= label)\n",
    "    num_nodes = data.num_nodes\n",
    "    # print(num_nodes)\n",
    "\n",
    "    train_mask, val_mask, test_mask = torch.zeros(num_nodes, dtype= torch.bool), torch.zeros(num_nodes, dtype=torch.bool), torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    # print(train_mask.shape)\n",
    "    idx_train = torch.cat(idx_train)\n",
    "    idx_val = torch.cat(idx_val)\n",
    "    idx_test = torch.cat(idx_test)\n",
    "    # print(idx_test.shape[0] + idx_val.shape[0] + idx_train.shape[0])\n",
    "\n",
    "    train_mask[idx_train] = True\n",
    "    val_mask[idx_val] = True\n",
    "    test_mask[idx_test] = True \n",
    "\n",
    "    data.train_mask, data.val_mask, data.test_mask = train_mask, val_mask, test_mask\n",
    "\n",
    "    data.num_classes = data.y.shape[1]\n",
    "\n",
    "    return data\n",
    "\n",
    "    # print(train_mask)\n",
    "\n",
    "\n",
    "\n",
    "data = homogenous_graph(nei_index, feats, mps, pos, label, idx_train, idx_val, idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55783"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6564, 4])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35890"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [55783] at index 0 does not match the shape of the indexed tensor [6564, 4] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m301\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     val_acc, val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, data, data\u001b[38;5;241m.\u001b[39mval_mask)\n\u001b[0;32m     10\u001b[0m     test_acc, test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, data, data\u001b[38;5;241m.\u001b[39mtest_mask)\n",
      "Cell \u001b[1;32mIn[29], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, data, train_mask)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m     28\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index)\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(out[train_mask], \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     30\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [55783] at index 0 does not match the shape of the indexed tensor [6564, 4] at index 0"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GCN(data.num_features, hidden_channels=256, num_classes=data.num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(301):\n",
    "    train_loss = train(model, optimizer, data, data.train_mask)\n",
    "    val_acc, val_loss = evaluate(model, data, data.val_mask)\n",
    "    test_acc, test_loss = evaluate(model, data, data.test_mask)\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "    \n",
    "        print(f\"Epoch: {epoch + 1}/300 | Train Loss: {train_loss:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
