{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv \n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3749, 0.3713, 0.8901, 0.7779, 0.8807],\n",
      "        [0.8543, 0.8180, 0.9878, 0.1403, 0.3032],\n",
      "        [0.2311, 0.5651, 0.1418, 0.4426, 0.6663]])\n",
      "3\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# linear transformation\n",
    "\n",
    "the_in_feats = 5\n",
    "out_feats = 2\n",
    "nb_nodes = 3\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(the_in_feats, out_feats))) # xavier paramiter inizialation\n",
    "nn.init.xavier_uniform_(W.data, gain =1.414)\n",
    "\n",
    "input = torch.rand(nb_nodes, the_in_feats)\n",
    "# tensor of 3 nodes, with 5 features with a random value between 0 and 1\n",
    "\n",
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "\n",
    "print(input)\n",
    "print(N)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.4843],\n",
      "        [ 0.6496],\n",
      "        [-0.8846],\n",
      "        [ 0.6198]], requires_grad=True)\n",
      "torch.Size([4, 1])\n",
      "LeakyReLU(negative_slope=0.2)\n"
     ]
    }
   ],
   "source": [
    "# Attention Mechanism \n",
    "\n",
    "a = nn.Parameter(torch.zeros(size=(2*out_feats, 1)))\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "print(a) # allows for back propagation for the flag. \n",
    "print(a.shape)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(0.2)\n",
    "print(leakyrelu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5500,  0.0788,  2.5500,  0.0788],\n",
      "         [ 2.5500,  0.0788,  2.2703, -1.2548],\n",
      "         [ 2.5500,  0.0788,  1.4414,  0.5385]],\n",
      "\n",
      "        [[ 2.2703, -1.2548,  2.5500,  0.0788],\n",
      "         [ 2.2703, -1.2548,  2.2703, -1.2548],\n",
      "         [ 2.2703, -1.2548,  1.4414,  0.5385]],\n",
      "\n",
      "        [[ 1.4414,  0.5385,  2.5500,  0.0788],\n",
      "         [ 1.4414,  0.5385,  2.2703, -1.2548],\n",
      "         [ 1.4414,  0.5385,  1.4414,  0.5385]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a_input = torch.cat([h.repeat(1,N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_feats)\n",
    "print(a_input)\n",
    "print(a_input.shape) # 3-d tensor with 3 channles, 3 nodes each wtih 4 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1881, -1.3040, -0.9350],\n",
      "        [-1.2784, -1.3942, -1.0252],\n",
      "        [-0.7993, -0.9151, -0.5462]], grad_fn=<LeakyReluBackward0>)\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))\n",
    "print(e)\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
      "\n",
      "torch.Size([3, 3, 1])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a_input.shape, a.shape)\n",
    "print('')\n",
    "print(torch.matmul(a_input, a).shape)\n",
    "print('')\n",
    "print(torch.matmul(a_input, a).squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 1]])\n",
      "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Masked Attention\n",
    "\n",
    "adj = torch.randint(2, (3,3))\n",
    "print(adj)\n",
    "\n",
    "zero_vec = -9e15*torch.ones_like(e)\n",
    "print(zero_vec)\n",
    "print(zero_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 0],\n",
      "        [1, 0, 1],\n",
      "        [0, 0, 1]])\n",
      "\n",
      "tensor([[-1.1881, -1.3040, -0.9350],\n",
      "        [-1.2784, -1.3942, -1.0252],\n",
      "        [-0.7993, -0.9151, -0.5462]], grad_fn=<LeakyReluBackward0>)\n",
      "\n",
      "tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n",
      "\n",
      "tensor([[-1.1881e+00, -1.3040e+00, -9.0000e+15],\n",
      "        [-1.2784e+00, -9.0000e+15, -1.0252e+00],\n",
      "        [-9.0000e+15, -9.0000e+15, -5.4615e-01]], grad_fn=<WhereBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "print(adj)\n",
    "print('')\n",
    "print(e)\n",
    "print('')\n",
    "print(zero_vec)\n",
    "print('')\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5289, 0.4711, 0.0000],\n",
      "        [0.4371, 0.0000, 0.5629],\n",
      "        [0.0000, 0.0000, 1.0000]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "tensor([[ 2.4182, -0.5495],\n",
      "        [ 1.9259,  0.3376],\n",
      "        [ 1.4414,  0.5385]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime = torch.matmul(attention, h)\n",
    "print(attention)\n",
    "print('')\n",
    "print(h_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.4182, -0.5495],\n",
      "        [ 1.9259,  0.3376],\n",
      "        [ 1.4414,  0.5385]], grad_fn=<MmBackward0>)\n",
      "\n",
      "tensor([[ 2.5500,  0.0788],\n",
      "        [ 2.2703, -1.2548],\n",
      "        [ 1.4414,  0.5385]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# h_prime vs h\n",
    "\n",
    "print(h_prime)\n",
    "print('')\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer( nn.Module ):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout = 0.6, alpha= 0.2, concat = True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features # dataset.num_features\n",
    "        self.out_features = out_features # dataset.num_classes\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat \n",
    "\n",
    "        # create the weight matrix that will learn during the training process\n",
    "        # this creates a trainable paramter with a matrix of size in_features x out_features \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "\n",
    "        # set up weights of the layer to promote even propogation \n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        # learnable parameter that is used to see to compute the attention \n",
    "        # scores of nodes around the neighborhod. \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        # set up the activation function leakyRelu, similar to Relu, \n",
    "        # but allows for negatives which prevents the model from stoppping to learn\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "\n",
    "        # linear Transformation\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "        print(N)\n",
    "\n",
    "        # Attention Mechanism \n",
    "        a_input = torch.cat([h.repeat(1,N).view(N*N, -1), h.repeat(N,1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # Masked Attention \n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "\n",
    "\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import dataset\n",
    "\n",
    "dataset.transform = T.NormalizeFeatures()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10556) must match the size of tensor b (2708) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m, in \u001b[0;36mGAT.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     12\u001b[0m x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m---> 15\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(x)\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\16822\\miniconda3\\envs\\Machine_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 40\u001b[0m, in \u001b[0;36mGATLayer.forward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Masked Attention \u001b[39;00m\n\u001b[0;32m     39\u001b[0m zero_vec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9e15\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(e)\n\u001b[1;32m---> 40\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(attention, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10556) must match the size of tensor b (2708) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "\n",
    "        self.conv1 = GATLayer(dataset.num_features, self.hid)\n",
    "        self.conv2 = GATLayer(self.in_head *self.hid, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9436, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7244, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5033, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5200, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5288, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "\n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.in_head *self.hid, dataset.num_classes, concat = False, heads= self.out_head, dropout=0.6)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def validate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        val_loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask]).item()\n",
    "        pred = out[data.val_mask].max(1)[1]\n",
    "        correct = pred.eq(data.y[data.val_mask]).sum().item()\n",
    "        val_acc = correct / data.val_mask.sum().item()\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out[data.test_mask].max(1)[1]\n",
    "        correct = pred.eq(data.y[data.test_mask]).sum().item()\n",
    "        test_acc = correct / data.test_mask.sum().item()\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 1.9425, Val Loss: 1.9396, Val Acc: 0.3300\n",
      "Early stopping\n",
      "Test Accuracy: 0.8160\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "best_val_acc = 0\n",
    "patience = 100  # Early stopping patience\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train_loss = train(model, data, optimizer)\n",
    "    val_loss, val_acc = validate(model, data)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if epoch % 200 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Load the best model for testing\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_acc = test(model, data)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8180\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
